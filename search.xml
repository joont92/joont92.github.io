<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[nextstep 교육과정 및 멘토링 후기]]></title>
    <url>%2Flife%2Fnextstep-%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95-%ED%9B%84%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[nextstep 교욱과정 작년 3월, 그러니까 1년 반전 쯤… 이쪽 업계에서는 유명하신 박재성(자바지기)님이 하시는 교육과정을 들은적이 있었다. 교육과정명은 클린코드를 위한 TDD, 리팩토링 with Java 이었는데, 과정은 대충 이러했다. 매주 금요일 3시간의 수업을 진행한다(총 5주) 매주 금요일 오후 7시 반에 강남역에 모여서 수업을 진행한다. 수업 내용은 대부분 OOP, TDD, refactoring 에 대한 내용이었고, 가끔씩 재성님의 인생 철학(!)을 얘기해주시기도 하신다. 수업 내용은 아주 유익하고, 인생 철학 또한 아주 유익하다 ㅋㅋㅋ 수업과 별개로 과제를 진행한다 이 과제가 교육과정의 핵심이다. 프로그래밍 언어로 자동차 경주 게임, 로또, 사다리 게임 등을 직접 구현하는 과제이다(출력은 콘솔로 한다). 단순히 구현하는 것이 아니라, 교육과정 중에 배운 OOP, TDD, refactoring 을 열심히 사용(?)하여 구현해야 한다. 그리고 각 과제마다, 담당 코드 리뷰어들이 있다. (리뷰어는 이미 이 과정을 수료하신 분들로 구성되어 있다.) 하나의 과제는 여러 step 들로 구성되어 있으며, step 별 요구사항을 완료하여 PR 을 보내면 리뷰어가 리뷰를 해주고, approve 가 되면 다음 step 으로 넘어가게 되는 구조이다. 이렇게 총 5개(1개는 optional)의 과제를 완료하면 교육과정을 수료할 수 있게 된다. 어렵다 어려워 매번 서비스 메서드에 모든 로직을 다 떄려박던 나같은 트랜잭션 스크립터(?)에게 이 과제는 매우 어려웠었다. 지금까지 내가 해오던 코딩과는 너무 달랐기 때문이었다. 어떤 기준으로 객체를 설계해야할지, 객체끼리는 어떻게 메시지를 주고 받아야할지, 테스트는 제대로 짜고 있는지… 그야말로 혼란스러웠다. 몇시간동안 한줄도 못 짜고 머리를 싸매며 고통스러워 했었던적도 한두번이 아니었던 것 같다. 그래도 매일 고민하고, 리뷰어님들의 아주아주 정성스러운 피드백들을 받다보니 시간이 지날수록 조금씩 나아지는 내 모습을 발견할 수 있었다. 그저 빛… 결과적으로 5주 동안 필수 과제 4개를 전부 완료하고, 한 단계 더 성장한 모습으로 이 과정을 수료할 수 있었다. 이제는 실전 회사로 돌아왔다(교육과정 듣는다고 회사를 안간것은 아니었지만…). 회사 코드를 봤다. 트랜잭션 스크립트 패턴이 많고, 테스트도 많이 없다. 개선해야 할 부분이 많아 보인다. 하지만 코드 자체가 워낙 양이 많고 복잡하며, 다른 코드나 시스템끼리 복잡하게 의존되는 부분이 많아서 어디서부터 어떻게 시작해야할지 정하기가 힘들었다. 그래도 훈련소까지 갔다왔는데… 그냥 포기할수는 없었다. 기존에 존재하던 코드들에 대해 테스트를 작성하기 시작했고, 테스트 작성이 완료되고나니 조금씩 리팩토링 하는것이 가능했다. 그렇게 하나의 프로젝트를 전반적으로 리팩토링했다. 굉장히 뿌듯했다. 새로운 프로젝트를 들어갔을때에도, 클린코드에 대한 끈을 놓지 않으려고 했다. 항상 테스트를 작성했으며, 객체 설계에 대해 매번 고민하고 작성했다. 그렇게 1년 정도 실전에서 훈련한 결과, 확실히 코드 자체가 작년에 비해 많이 달라진것을 느끼고 있다. 아직 갈 길은 많이 멀었지만, 작년에 이 과정을 듣지 못했다면 아직까지 나는 얽히고 섥힌 코드들을 유지보수하느라 고통받고 있지 않았을까 싶다. nextstep 멘토링 이 교육에는 수업 이후에 멘토링을 해주는 과정이 추가적으로 포함되어 있다. 멘토링은 이 과정을 수료한 수강자에 한해서 모두 제공된다. 단순히 수업을 듣는다고 수료가 되는것이 아니라, 필수과제 4개의 과제를 모두 제출해 통과해야만 수료가 된다. 멘토링에서 해주는 내용은 아래와 같다. 이력서 검토 및 피드백 온라인, 오프라인 모의 면접 추천 어쩌다보니 나도 이번에 이직 시기가 되어서, 멘토링을 신청하게 되었다. 이력서 검토 및 피드백 일단 자유 포멧으로 이력서를 작성해 전달해주면, 이력서를 전체적으로 점검하고 피드백을 작성하여 전달해주신다. 이런식으로 전달해주신다. 매우 좋다!!! 전달받은 피드백을 적용해서 이력서를 다시 전달해드리면 또 추가적으로 피드백을 주시게 되고, 이런식으로 몇번 왔다갔다 하면서 이력서를 전체적으로 다듬어가는 과정이다. 이 과정에서 확실히 처음에 비해 이력서가 매우 나아지는 과정을 경험했다. 온라인 모의 면접 처음에는 그냥 화상 회의라고 하셔서 부담없이 들어갔는데… 온라인 모의 면접이었다😅 이력서를 기반으로 꼼꼼하게 질문해주시고, 이 질문이 어떤 의도였는지, 내 대답이 어땠으면 좋았을지에 대해 바로 바로 피드백 해주신다. 그리고 다음 오프라인 면접 날짜를 잡고, 그 날짜까지 추가로 더 준비해오면 좋을 것들에 대해 숙제(!)를 내주신다. 개인적으로 피드백 해주시는 내용들이 도움이 많이 되었고, 내 스스로 내가 한 일들에 대해 제대로 정리되어 있지 않았다는 사실을 깨달을 수 있었다. 오프라인 모의 면접 온라인 면접 후, 딱 1주일 뒤에 오프라인 모의 면접을 진행했다. 오프라인 모의 면접은 잠실역에 있는 우형 사무실에서 진행했다. 총 1시간 정도 진행되며, 온라인 면접처럼 바로 피드백을 주지 않고 실제 면접과 동일한 분위기로 모의 면접을 진행하였다. 그리고 오프라인인 만큼 내가 수행했던 프로젝트에 대해 화이트보드에 그려가며 설명하는 과정들이 많았다. 확실히 모의면접을 진행하고나니 자신감도 붙고, 앞으로 어떤 부분들을 더 준비해야할지 더 잘 알게 되었다. 추천 이후에는 추천이다. 이력서, 모의면접 과정을 다 진행하고 난 후에 통과가 되면 nextstep 쪽에서 연계된 기업들에 한해 추천을 해준다. 단순히 내 이력서만 전달해주시는 것이 아니라, 교육과정과 멘토링 진행할 때의 내 모습을 기반으로 직접 추천서를 작성해서 같이 전달해주신다. 그래서 결과는? 모의면접 진행이 끝난 후 실제 이력서를 내고 면접을 진행하였고, 최종적으로 원하던 기업에 합격하여 다음주에 입사하게 된다. 면접 과정에서 nextstep 에서 받았던 멘토링 과정이 도움이 많이 되었다. 평소에 갈증이 많은 개발자라면 nextstep 의 교육 과정을 들어보는 것을 추천한다.]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>nextstep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql and or 순서]]></title>
    <url>%2Fdb%2Fsql-and-or-%EC%88%9C%EC%84%9C%2F</url>
    <content type="text"><![CDATA[다들 알다시피 SQL 에서 소괄호 없이 AND OR 을 사용하면 AND -&gt; OR 의 순서로 처리된다 그렇다면 아래와 같은 쿼리는 어떻게 처리되는걸까? 123select count(*) from employees where year(hire_date) = '1998' or year(hire_date) = '1999' and gender = 'M'; 이 쿼리는 and 조건 먼저 실행되고 그다음 or 조건이 실행된다 이 쿼리는 아래의 두 쿼리를 합친것과 동일하다 12345select count(*) from employees where year(hire_date) = '1999' and gender = 'M';select count(*) from employees where year(hire_date) = '1998'; 결론은 그냥 알아보기 너무 힘드니까 입닥치고 괄호로 감싸주자]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>and</tag>
        <tag>or</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] 쓰레드 기본]]></title>
    <url>%2Fjava%2F%EC%93%B0%EB%A0%88%EB%93%9C-%EA%B8%B0%EB%B3%B8%2F</url>
    <content type="text"><![CDATA[기본적으로 서버 프로그램의 경우 많은 동시에 많은 사용자의 요청을 처리해야 하므로 멀티 쓰레드로 동작한다 우리가 매번 사용하는 톰캣 또한 사용자의 요청을 모두 쓰레드가 처리하는, 멀티 쓰레드 구조이다 그래므로 쓰레드에 대한 지식은 필수이다 라고 말했지만 나는 겉핥기밖에 모르는 것 같아서 다시 기초부터 공부하고 있고, 이를 정리하고자 한다 쓰레드에 대한 기본 지식은 구글에 검색하면 아주 아주 잘 설명된 글들이 많으니 그것을 참조하면 되고, 여기서는 자바에서 쓰레드를 사용하는 법에 대해 정리하겠다 쓰레드 구현 자바에서 쓰레드를 구현하는 방법은 Thread 클래스를 상속받는 방법과 Runnable 인터페이스를 구현하는 방법이 있다 자바에서는 상속이 비싼 행위이기 때문에 보통은 Runnable 인터페이스를 구현하여 쓰레드를 구현한다 123456789101112131415161718192021222324class MyThread1 extends Thread &#123; @Override public void run() &#123; /* 쓰레드에서 실행할 내용 */ &#125;&#125;class MyThread2 implements Runnable &#123; @Override public void run() &#123; /* 쓰레드에서 실행할 내용 */ &#125;&#125;class Main &#123; public static void main(String[] args) &#123; MyThread1 myThread1 = new MyThread1(); // Thread 상속 시 Thread myThread2 = new Thread(new MyThread2()); // Runnable 구현시 // 쓰레드 실행 myThread1.start(); myThread2.start(); &#125;&#125; Thread 클래스를 상속받은 경우 바로 인스턴스를 생성하면 되고, Runnable 인터페이스를 구현한 경우 Thread 클래스의 인자로 넘겨주면 된다 그리고 start() 메서드를 호출해서 쓰레드를 실행시키고 있음을 볼 수 있다 참고로 한번 사용한(start()가 이미 호출된) 쓰레드는 재사용 할 수 없다 두 번 이상 호출 시 IllegalThreadStateException이 발생한다 앞서 우리가 구현했던 run 메서드는 단순히 수행할 태스크만을 작성하는 부분이고, 실제로 쓰레드를 실행시키려면 위와 같이 start 메서드를 통해서 실행해야 한다 start 메서드 호출 시 쓰레드가 작업을 실행하는데 필요한 호출 스택을 생성하고 run 메서드를 실행하게 된다 !(thread call stack)[https://joont92.github.io/temp/thread-call-stack.jpg] 위는 메인 메서드에서 2개의 쓰레드를 실행시켰을 때의 모습이다 보다시피 각 쓰레드들은 모두 별개의 작업이고, 스케줄링의 대상이 된다 스케줄러가 정한 실행순서에 따라 각 쓰레드들을 돌면서 연산을 수행하게 될 것이고, 수행이 끝난 쓰레드들은 호출스택이 비워지면서 먼저 종료가 될 것이다 CPU는 기본적으로 쓰레드를 기반으로 스케줄링을 한다 확인 필요(+ JVM 쓰레드 스케줄러) 즉, 메인 메서드가 수행을 마쳤다 하더라도 쓰레드가 남아있다면 프로그램은 종료되지 않음을 뜻한다 실행중인 사용자 쓰레드가 하나도 없을 경우 프로그램은 종료된다 그리고 언급했다시피 각각의 쓰레드들은 각각 별개의 작업흐름이기 때문에 한 쓰레드에서 예외가 발생해도 다른 쓰레드의 실행에는 영향을 미치지 않게 된다 싱글 쓰레드와 멀티 쓰레드 방금 아주 간단하게 멀티 쓰레드를 구성해봤는데, 사실 단순히 CPU만 사용해서 계산하는 작업을 수행할 경우 위와 같이 멀티쓰레드로 작업을 수행하는것이 더 비효율적이다 쓰레드간 context switching 비용이 발생하기 때문이다 하지만 쓰레드 내에서 연산 이외의 작업을 수행할 경우(CPU 이외의 자원을 사용할 경우), 멀티 쓰레드 프로세스가 훨씬 효율적이다 예를 들면 파일이나 네트워크 I/O 작업등이 있게 되는데, 특정 쓰레드가 이러한 작업을 수행하고 있을 경우 CPU는 이를 기다리지 않고 다른 쓰레드의 작업을 수행하면 되기 때문이다 만약 싱글 쓰레드였다면 I/O 작업이 끝날 때 까지 CPU가 대기해야 했을 것이다 쓰레드 우선순위 쓰레드는 우선순위(priority)라는 속성(멤버변수)을 가지고 있고, 이 우선순위에 따라 스케줄러가 할당하는 시간이 달라진다 쓰레드의 우선순위가 같다면 CPU는 각 쓰레드에게 거의 같은 양의 시간을 할당하지만, 우선순위가 다르다면 CPU는 우선순위가 높은 쓰레드에게 더 많은 작업시간을 할당한다 즉 쓰레드가 수행하는 작업의 중요도에 따라 쓰레드의 우선순위를 다르게 지정하여 특정 쓰레드가 더 많은 작업시간을 갖도록 처리할 수 있다 (예를 들면 채팅을 처리하는 쓰레드는 쓰레드는 파일을 전송하는 쓰레드보다 우선순위가 높아야한다) 1234567891011121314151617Thread thread1 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 100; i++) &#123; System.out.print("1"); &#125;&#125;);Thread thread2 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 100; i++) &#123; System.out.print("2"); &#125;&#125;);thread1.setPriority(10);thread2.setPriority(1);thread1.start();thread2.start();// 출력 : 222222222222222222222222222222222222222222222222...........1111111111111111111111111111111111111............... 쓰레드가 가질 수 있는 우선순위의 범위는 1~10까지 이며, 숫자가 높을수록(작을수록) 우선순위가 높다 참고로 이 우선순위 값은 절대적인 것이 아니라 상대적인 값이다 값이 1정도 차이나는 경우에는 별 차이가 없지만, 2 이상 차이가 나면 실행시간에 많은 차이가 발생하게 된다 쓰레드의 우선순위는 따로 지정해주지 않으면 쓰레드를 생성한 쓰레드로부터 상속을 받게 된다(main 쓰레드는 우선순위가 5이다) 데몬 쓰레드 쓰레드는 사용자 쓰레드와 데몬 쓰레드 2종류가 있다 지금까지 언급했던 것은 전부 사용자 쓰레드이고, 데몬 쓰레드의 경우 일반 쓰레드의 작업을 돕는 보조적인 역할을 수행하는 쓰레드를 말한다 보조 역할을 하는 쓰레드이므로 일반 쓰레드가 모두 종료되면 데몬 쓰레드 또한 강제적으로 종료된다(더이상 필요없기 때문에) 데몬 쓰레드의 예로는 가비지 컬렉터, 자동저장, 화면 자동갱신 등이 있다 쓰레드를 데몬 쓰레드로 생성시키고 싶다면 아래와 같이 setDaemon() 메서드만 실행시켜주면 된다 1234Thread thread = new Thread(new MyThread());thread.setDaemon(true); // start 전에 해줘야함thread.start(); 이 외에도 데몬 쓰레드가 생성한 쓰레드도 자동으로 데몬 쓰레드가 된다 데몬 쓰레드는 기본적으로 무한루프와 조건문을 이용해서 작성된다 (예시에서 봤듯이 가비지 컬렉터, 자동저장 등은 계속해서 상태를 체크해야하는 작업이다) 12345678910111213141516class AutoSaveDaemonThread implements Runnable &#123; @Override public void run() &#123; while(true) &#123; try &#123; Thread.sleep(3 * 1000); &#125; catch(InterruptedException e) &#123; // do nothing &#125; if(autoSave) &#123; autoSave(); &#125; &#125; &#125;&#125; JVM은 기본적으로 가비지 컬렉션, 이벤트처리, 그래픽처리와 같이 프로그램이 실행되는데 필요한 보조작업을 수행하는 데몬 쓰레드들을 자동적으로 실행시킨다 쓰레드 실행제어(Welcome To Hell!) 앞서 쓰레드의 우선순위로 쓰레드간 실행을 제어하긴 했지만, 사실 이것만으로는 부족하다 효율적인 멀티쓰레드 프로그램을 만들기 위해서는 보다 정교한 스케줄링을 통해 주어진 자원을 여러 쓰레드가 낭비없이 잘 사용하도록 프로그래밍 해야한다 이를 위해 자바에서는 쓰레드를 컨트롤 할 수 있는 기능을 제공하는데, 이를 알기전에 먼저 쓰레드의 상태에 대해 알고 있어야한다 NEW 쓰레드가 생성되고 아직 start()가 호출되지 않은 상태 RUNNABLE start()를 호출했다고 바로 실행되는 것이 아니라, 큐의 구조로 된 실행 대기열에 저장되어 대기하다가 자기 차례가 되면 실행한다 RUNNING 실행대기열에 있던 쓰레드가 자기 차례가 되어 실행중인 상태 WAITING 실행중인 쓰레드가 특정 메서드(sleep(), wait(), join() 등)에 의해 일시정지 상태가 된 것을 말한다 지정된 일시정지 시간이 다 되거나 특정 메서드(notify(), interrupt()) 호출이 발생하면 다시 RUNNABLE 상태로 돌아가 실행대기열에 저장된다 BLOCKED ? TERMINATED 쓰레드의 작업이 종료된 상태를 말한다 작업이 종료된 쓰레드는 소멸되므로 사실상 쓰레드가 이 상태로 있는것은 아니다 아래는 쓰레드의 상태를 제어하는 주요 메서드들에 대한 설명이다 start 위에서 언급한 것처럼, 쓰레드를 실행시키는 메서드이다 바로 RUNNING 상태가 되지는 않고 RUNNABLE 상태로 되었다가 자기 차례가 되면 RUNNING 상태가 된다 join 지정한 쓰레드의 작업이 끝날 때 까지 기다리는 메서드이다 1234567891011121314Thread thread1 = new Thread(new MyThread());Thread thread2 = new Thread(new MyThread());try &#123; thread1.start(); thread1.join(); // 1 thread2.start(); thread2.join(); // 2&#125; catch(InterruptedException e) &#123; // do nothing&#125;System.out.println("job ended"); // 3 join() 메서드 실행 시, 기다리는 대상은 해당 쓰레드를 실행한 쓰레드이다 thread1, thread2를 실행한 쓰레드를 메인 쓰레드라고 가정한다면, main thread는 thread1 을 실행하고, thread1 의 작업이 끝날 때 까지 기다린다 메서드의 인자로 밀리초나 나노초를 줘서 특정 시간 동안만 기다리게도 설정할 수 있다 thread1의 작업이 끝난 뒤 mani thread는 thread2 를 실행하고, thread2 의 작업이 끝날 때 까지 기다린다 thread2의 작업이 끝나고 나면 main thread의 마지막 부분을 실행하고, main thread는 종료된다 보다시피 join 메서드는 한 쓰레드의 작업 중간에 다른 쓰레드의 작업이 필요할 경우 유용하게 사용할 수 있다 sleep 지정한 시간동안 작업을 일시정지 상태로 들어가는 메서드이다 static 메서드로 제공되며(인스턴스 메서드는 deprecated 되었다), 이 메서드를 호출한 쓰레드가 지정한 시간만큼 일시정시 상태가 된다 12345678Thread thread = new Thread(new MyThread());thread.start();try &#123; Thread.sleep(5000);&#125; catch(InterruptedException e) &#123; //&#125; 5000 millisecond 이기 때문에 5초간 WAITTING 상태가 된다 5초가 지난 후 다시 RUNNABLE 상태가 되어 실행대기열에 저장된다 yield RUNNABLE 상태로 들어가면서 다른 쓰레드에게 작업을 양보하는 메서드이다 이 또한 static 메서드로 제공되며, 이 메서드를 호출한 쓰레드가 RUNNABLE 상태로 들어가게 된다 다른 메서드들과의 차이점은 WAITING 상태로 들어가지 않고 바로 RUNNABLE 로 들어간다는 점이다 12345678910111213141516171819class MyThread implements Runnable &#123; @Override public void run() &#123; // 1000번 loop for(long i = 0; i &lt; 1000; i++) &#123; &#125; System.out.println("sub thread has been terminated"); &#125;&#125;class Main &#123; public void static main(String[] args) &#123; Thread thread = new Thread(new MyThread()); thread.start(); Thread.yield(); System.out.println("main thread has been terminated"); &#125;&#125; 원래라면 main thread 종료 문구 -&gt; sub thread 종료 문구 의 순서로 출력되어야 하지만, main thread 에서 yield 로 다른 쓰레드에게 실행을 양보함으로써 sub thread가 먼저 종료됨을 볼 수 있다 쓰레드의 실행을 양보하고 RUNNABLE 상태로 들어간 것이기 때문에, sub thread의 작업 시간이 CPU 스케줄링에서 쓰레드에 할당하는 작업 시간보다 길 경우, main thread 종료 문구가 먼저 출력될 것이다 WAITING 상태로 들어가는 join 과는 다르다 suspend, resume, stop 이 3개의 메서드는 쓰레드를 교착상태(dead-lock)에 빠트릴 수 있어 deprecated 되었다 그렇다고 사용 못하는 것은 아니고, 아래와 같이 작성하여 구현할 수 있다 12345678910111213141516171819202122232425262728293031323334353637383940414243class MyThread implements Runnable &#123; private boolean suspended = false; private boolean stopped = false; private Thread thread; public MyThread(String name) &#123; thread = new Thread(this, name); &#125; @Override public void run() &#123; while(!stopped) &#123; if(!suspended) &#123; try &#123; // &#125; catch(InterruptedException e) &#123; System.out.println(name + " interrupted"); &#125; &#125; else &#123; Thread.yield(); &#125; &#125; &#125; public void suspend() &#123; suspended = true; thread.interrupt(); &#125; public void resume() &#123; suspended = false; &#125; public void stop() &#123; stopped = true; thread.interrupt(); &#125; public void start() &#123; thread.start(); &#125;&#125; stopped, suspended flag 값으로 기존의 suspend(), stop() 메서드를 구현하였다 게다가 suspend 상태일때는 Thread.yield() 를 발생시켜 불필요한 while 문을 돌지 않도록 하였으며, stop()과 suspend() 시에 interrupt()를 발생시켜 thread가 WAITING 상태라면 깨워서 RUNNABLE 상태로 가도록 설정하였다 (interrupt() 시에 WAITING 상태가 아니라면 아무일도 일어나지 않는다) 쓰레드의 동기화 멀티 쓰레드 환경의 경우 여러 쓰레드가 같은 프로세스 내의 자원을 공유하기 때문에, 서로의 작업에 영향을 줄 수 있다 자바의 경우, 힙 영역의 경우 JVM 프로세스가 공유하는 영역이고, 스택 영역의 경우 각 쓰레드마다 별개로 가지는 영역이다 그러므로 우리는 항상 힙 영역의 데이터를 사용할 경우 주의를 기울여야 한다 아래는 힙 영역의 데이터를 공유함으로써 문제가 발생하는 것에 대한 예시이다 1234567891011121314151617181920212223242526272829303132333435363738class Account &#123; int amount; public Account(int amount) &#123; this.amount = amount; &#125; public void withdraw(int money) &#123; if(amount &gt;= money) &#123; amount-=money; &#125; &#125;&#125;class MyThread implements Runnable &#123; private Account account; public MyThread(Account account) &#123; this.account = account; &#125; @Override public void run() &#123; account.withdraw(1000); &#125;&#125;class Main &#123; public void static main(String[] args) &#123; Account account = new Account(1000); Thread thread1 = new Thread(new MyThread(account)); Thread thread2 = new Thread(new MyThread(account)); thread1.start(); thread2.start(); &#125;&#125; 잔고 1000원의 계좌를 생성하고, 1000원을 출금하는 쓰레드를 2개 생성하여 동시에 실행시켰다 결과로 항상 0원이 나올것이라 생각했지만, 돌려보면 -1000원이 나올수도 있다 (위 정도의 작업에서는 거의 무조건 0원이 나오므로, if 문 안에 Thread.sleep 을 조금 걸고 테스트해보면 확인할 수 있다) 이는 thread1과 thread2가 account 객체를 공유하기 때문에 발생하는 문제이다 예를 들면 thread1이 if문을 통과하고 아직 amount를 차감하지 않은 상태에서, thread2가 if문을 통과하는 경우가 될 것이다 이런 상황이 발생하면 결국 두 쓰레드 모두 amount 값을 차감하게 되고, 우리는 음수 결과값을 받아보게 되는 것이다 쓰레드 프로그램 작성시에 스택내의 변수만을 사용하여 위와 같은 상황을 안 만들면 되지 않느냐고 생각할 수 있지만, 로직을 처리하다 보면 결국에는 위와 같이 공용 변수에 접근하는 순간이 발생하므로, 위와 같은 현상은 피할 수 없다 그러므로 위 withdraw 같은 메서드들은 하나의 쓰레드에서 접근하고 있을 경우 다른 쓰레드에서 접근하지 못하도록 락을 걸어 데이터가 틀어지는 것을 방지해야 하는데, 이를 동기화라고 한다 자바에서는 snychronized 키워드를 사용해 이를 구현할 수 있고, 방법은 아래와 같이 메서드에 synchronized 를 선언하는 방법과, synchronized block을 선언하는 방법이 있다 12345678910111213141516171819/** * synchronized method**/public synchronized void withdraw(int money) &#123; if(amount &gt;= money) &#123; amount-=money; &#125;&#125;/** * synchronized block**/public void withdraw(int money) &#123; synchronized(this) &#123; if(amount &gt;= money) &#123; amount-=money; &#125; &#125;&#125; 그리고 이런식의 동기화 블록을 사용해서 프로그램을 작성시에는 교착상태(dead-lock)을 항상 주의해줘야 한다 만약 thread가 락을 건 상태에서 정지되거나 종료된다면 이를 기다리는 다른 쓰레드들이 전부 데드락에 빠지기 떄문이다 이같은 이유로 suspend(), stop() 메서드등이 deprecated 되었다 wait, notify 한 쓰레드가 객체에 lock을 걸 경우, 이 객체를 사용하려는 다른 쓰레드들은 lock 이 풀릴떄까지 무작정 같이 기다려야하는 비효율이 발생하게 된다 이 같은 상황을 방지하기 위해 나온 메서드가 wait()과 notify()이다 wait()과 notify(), notifyAll()은 Object 클래스에 정의된 메서드이므로 모든 객체에서 호출이 가능하며, synchronized 블록 내에서만 사용할 수 있다 wait()이 호출되면 쓰레드는 이때까지 자기가 걸고있던 모든 락을 다 풀고, WAITING 상태로 전환되면서 wait()이 호출된 객체의 대기실에서 대기하게 된다 그러다가 다른 쓰레드에 의해 해당 객체의 notify()나 notifyAll()이 호출되면, 객체의 대기실에서 나와 다시 RUNNABLE 상태로 들어가게 된다 12345678910111213141516171819202122class Account &#123; int amount; public Account(int amount) &#123; this.amount = amount; &#125; public synchronized void withdraw(int money) &#123; while(amount &lt; money) &#123; // 1 try &#123; wait(); // 2 &#125; catch(InterruptedException e) &#123; &#125; &#125; amount -= money; &#125; public synchronized void deposit(int money) &#123; balance += money; notifyAll(); // 3 &#125;&#125; 전달받은 금액을 출금하기 위해 메서드에 락을 걸고 들어왔으나, 가지고 있는 잔고보다 출금하려는 금액이 더 많은 상황이다 그러므로 출금하지 못하고, Account 객체의 wait()를 호출한다 withdraw 에 걸고 있는 락이 풀리게 된다 쓰레드는 WAITING 상태로 들어가게 된다 쓰레드는 Account 객체 인스턴스의 대기실(waiting room)에서 기다리게 된다 deposit 메서드가 호출되며 금액이 채워지고, notifyAll()을 호출한다 Account 객체 인스턴스의 대기실에 있는 모든 쓰레드들을 깨운다 깨어난 쓰레드는 2번의 위치에서 다시 흐름을 이어가게 되는데, while 문이므로 다시 출금 가능한지 검사하게 된다 if문 대신 while문을 사용한 이유이다 nofiy() 대신 notifyAll() 을 사용하나 사실 별 차이는 없지만, notify()를 사용해서 한개의 쓰레드만 꺠울 경우 어떤 쓰레드가 꺠워질지 알수 없고, 이로인해 우선순위가 높은 쓰레드가 계속 pool에 머물게 될 수 도 있으므로, notifyAll()을 통해 모든 쓰레드를 꺠워서 스케줄러에 의해 처리되도록 해주는 것이 좋다 참고 : 남궁성, 『Java의 정석 2nd Edition』, 도우출판(2010)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java thread</tag>
        <tag>synchronized</tag>
        <tag>join()</tag>
        <tag>Thread.sleep()</tag>
        <tag>Thread.yield()</tag>
        <tag>wait()</tag>
        <tag>notify()</tag>
        <tag>notifyAll()</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[kubernetes] 배포 전략]]></title>
    <url>%2Fkubernetes%2F%EB%B0%B0%ED%8F%AC-%EC%A0%84%EB%9E%B5%2F</url>
    <content type="text"><![CDATA[쿠버네티스는 2가지 방법으로 무중단 배포를 수행할 수 있다 롤링 업데이트 Deployment 속성중에 .specs.strategy.type 값을 통해 Pod 교체전략을 지정할 수 있는데, 여기서 RollingUpdate 를 사용하는 방법이다 RollingUpdate는 우리가 잘 알다시피, Pod를 하나씩 죽이고 새로 띄우면서 순차적으로 교체하는 방법이다 RollingUpdate, Recreate 2개의 값이 존재하며 Recreate의 경우 기존 파드를 모두 삭제한 다음 새로운 파드를 생성하는 방법이다(이 방식은 무중단이 아니다) 기본값은 RollingUpdate 이다 RollingUpdate를 테스트하기 위해 먼저 deployment 와 service를 정의한다 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name : test-deploymentspec: replicas: 4 selector: matchLabels: app: test-pod template: metadata: labels: app: test-pod spec: containers: - name: test-pod image: joont92/echo-version:1.0 # 단순히 요청을 받으면 version을 리턴해주는 서버이다 ports: - containerPort: 8080 12345678910apiVersion: v1kind: Servicemetadata: name: test-servicespec: ports: - port: 80 targetPort: 8080 selector: app: test-pod deployment의 기본 속성이 RollingUpdate 이므로 따로 RollingUpdate 부분을 명시하지 않았다 이제 다른 Pod에서 위 서비스명으로 호출하면 1.0 이라는 버전을 리턴해주게 된다 이를 2.0을 리턴해주는 이미지로 바꿀건데, RollingUpdate 를 이용하여 바꿔보도록 할 것이다 먼저 위의 서비스 명으로 호출하는 Pod를 하나 만들어야한다 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: update-checkerspec: containers: - name: update-checker image: joont92/update-chekcer:latest # curl 이 깔려있는 alpine 리눅스 command: - sh - -c - | while true do echo "[`date`] curl -s http://test-service/" sleep 1 done 이 Pod가 뜨게되면 1초마다 test-service를 호출하게 되고, test-service 에서 사용하는 Pod 가 받아서 0.1.0 이라는 값을 리턴해주게 될 것이다 123[Sat Jul 13 14:08:27 UTC 2019] APP_VERSION=0.1.0[Sat Jul 13 14:08:28 UTC 2019] APP_VERSION=0.1.0... 이제 Deployment 내 Pod의 이미지를 바꿔서 적용시켜보겠다 먼저 변경할 내용을 작성하고, 123456spec: template: spec: containers: - name: echo-version image: joont92/echo-version:0.2.0 # 버전 변경 기존의 Deployment 에 적용한다 1$ kubectl patch deplyment test-deployment -p "$(cat patch-deployment.yaml)" 참고로 아래처럼 할수도 있다 1$ kubectl set image deployment test-deployment test-pod=joont92/echo-version:0.2.0 변경사항을 적용하자, 컨테이너가 하나씩 삭제되고 생성되는 것을 볼 수 있다 123456789101112131415161718192021222324252627$ kubectl get pod -l app:test-pod -wNAME READY STATUS RESTARTS AGEtest-deployment-6b8d4f7967-9gwxl 0/1 Terminating 0 46stest-deployment-6b8d4f7967-f9nlf 1/1 Running 0 48stest-deployment-6b8d4f7967-knrws 1/1 Running 0 48stest-deployment-6b8d4f7967-wkd4l 0/1 Terminating 0 46stest-deployment-86658bfcfd-4kf2z 1/1 Running 0 3stest-deployment-86658bfcfd-ckxvx 0/1 ContainerCreating 0 1stest-deployment-86658bfcfd-d4znq 0/1 ContainerCreating 0 3stest-deployment-86658bfcfd-d4znq 1/1 Running 0 4stest-deployment-6b8d4f7967-knrws 1/1 Terminating 0 49stest-deployment-86658bfcfd-s96t6 0/1 Pending 0 0stest-deployment-86658bfcfd-s96t6 0/1 Pending 0 0stest-deployment-86658bfcfd-s96t6 0/1 ContainerCreating 0 1stest-deployment-6b8d4f7967-wkd4l 0/1 Terminating 0 49stest-deployment-6b8d4f7967-wkd4l 0/1 Terminating 0 49stest-deployment-86658bfcfd-ckxvx 1/1 Running 0 4stest-deployment-6b8d4f7967-f9nlf 1/1 Terminating 0 51stest-deployment-6b8d4f7967-knrws 0/1 Terminating 0 52stest-deployment-6b8d4f7967-knrws 0/1 Terminating 0 52stest-deployment-6b8d4f7967-knrws 0/1 Terminating 0 52stest-deployment-86658bfcfd-s96t6 1/1 Running 0 4stest-deployment-6b8d4f7967-f9nlf 0/1 Terminating 0 54stest-deployment-6b8d4f7967-f9nlf 0/1 Terminating 0 54stest-deployment-6b8d4f7967-f9nlf 0/1 Terminating 0 54stest-deployment-6b8d4f7967-9gwxl 0/1 Terminating 0 53stest-deployment-6b8d4f7967-9gwxl 0/1 Terminating 0 53s 롤링 업데이트 동작 제어 Deployment의 .specs.strategy 속성을 설정하여 롤링 업데이트의 동작을 제어할 수 있다 123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: name : test-deploymentspec: replicas: 4 strategy: # 요기 type: RollingUpdate rollingUpdate: maxUnvailable: 3 maxSurge: 4 selector: matchLabels: app: test-pod template: metadata: labels: app: test-pod spec: containers: - name: test-pod image: joont92/echo-version:0.1.0 ports: - containerPort: 8080 maxUnvaliable은 롤링 업데이트시 동시에 삭제할 수 있는 파드의 최대 개수를 의미하고, maxSurge는 동시에 생성될 수 있는 파드의 최대 개수를 의미한다 (둘 다 기본값은 replicas 값의 25%이다) 이 값을 적절히 수정하면 RollingUpdate 시간을 단축할 수 있으나, 갑자기 한 Pod에 부하가 몰리거나 서버의 리소스가 잠깐 급증하는 사이드이팩트도 있으니 잘 설정해줘야 한다 블루-그린 롤링 업데이트는 강력하지만, 구버전과 새버전이 공존하는 시간이 발생한다는 단점이 있다 이 문제를 해결하기 위해 사용되는게 이 블루-그린 배포 인데, 이는 서버를 새버전과 구버전으로 2세트를 마련하고, 이를 한꺼번에 교체하는 방법이다 기존에 아래와 같이 Service, Deployment가 생성되어 있었다고 치자 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name : test-deployment-bluespec: replicas: 4 selector: matchLabels: app: test-pod color: blue template: metadata: labels: app: test-pod color: blue spec: containers: - name: test-pod image: joont92/echo-version:0.1.0 # 구버전 ports: - containerPort: 8080 1234567891011apiVersion: v1kind: Servicemetadata: name: test-servicespec: ports: - port: 80 targetPort: 8080 selector: app: test-pod color: blue blue-green 배포를 하기 위해, 새로운 버전이 적용된 Deployment 세트를 하나 더 준비한다 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name : test-deployment-greenspec: replicas: 4 selector: matchLabels: app: test-pod color: green template: metadata: labels: app: test-pod color: green spec: containers: - name: test-pod image: joont92/echo-version:0.2.0 # 신버전 ports: - containerPort: 8080 아래가 블루-그린 배포를 하기 위한 핵심 부분이다 123spec: selector: color: green 1$ kubectl patch service test-service -p "$(cat patch-service.yaml)" test-service의 label selector가 업데이트 됨으로써, test-service는 새로 배포된 Pod 들만을 바라보게 변경되었다 이처럼 구버전과 신버전이 공존하는 텀이 없이 바로 신버전으로 전환할 수 있다(!!) 게다가 만약 새로 배포된 버전에 문제가 발생한다면, 다시 test-service의 label을 blue로 돌려줌으로써 쉽게 롤백도 가능하다 기존에 물려있던 트래픽에 대해서는… 롤링배포와 블루그린 배포에서 차이점이 뭘까? 참고 : 야마다 아키노리, 『도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문』, 심효섭 옮김, 위키북스(2019)]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문</tag>
        <tag>RollingUpdate</tag>
        <tag>Blue-Green</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[kubernetes] kubectl context]]></title>
    <url>%2Fkubernetes%2Fkubectl-context%2F</url>
    <content type="text"><![CDATA[쿠버네티스 클러스터를 관리하는 cli 도구인 kubectl에는 환경을 바꿔가며 클러스터를 관리할 수 있는 context 라는 기능?이 존재한다 예를 들어 내 로컬 pc에 설치된 쿠버네티스 클러스터용 context 를 사용하면 kubectl 명령으로 내 로컬 쿠버네티스 클러스터를 컨트롤 할 수 있게되며, GCP에 있는 쿠버네티스 클러스터용 context를 사용하면 kubectl로 GCP 쿠버네티스 클러스터를 컨트롤 할 수 있게 되는 것이다 설정 context 는 kubectl 을 깔면 생성되는 파일인 ~/.kube/config 파일에서 설정할 수 있다 아래는 내 PC의 config 파일이다(보기쉽게 조금 수정했다) 123456789101112131415161718192021222324252627282930313233apiVersion: v1clusters:- cluster: insecure-skip-tls-verify: true server: https://localhost:6443 name: local-cluster- cluster: certificate-authority-data: ~~~~ server: https://xxx.xxx.xxx.xxx name: gcp-clusterusers:- name: local-user user: blah blah- name: gcp-user user: blah blahcontexts:- context: cluster: gcp-cluster user: gcp-user name: gcp-context- context: cluster: local-cluster user: local-user name: local-contextcurrent-context: local-contextkind: Configpreferences: &#123;&#125; 크게 clusters, users, contexts 가 있다 clusters 말 그대로 쿠버네티스 클러스터의 정보이다 내 PC에 설치된 쿠버네티스 클러스터와, GCP에 설치된 쿠버네티스 클러스터가 있음을 볼 수 있다 각 클러스터의 이름을 local-cluster, gcp-cluster로 수정해서 알아보기 쉽게 해놓았다 처음 클러스터 생성하면 조금 복잡?한 이름으로 생성되는데, 위처럼 자신이 알아보기 쉽게 바꿔주는 것이 좋다 users 클러스터에 접근할 유저의 정보이다 각 환경마다 필요한 값들이 다르다 이 또한 알아보기 쉽게 local-user, gcp-user 로 수정해놓았다 context cluster와 user를 조합해서 생성된 값이다 cluster의 속성값으로는 위에서 작성한 cluster의 name을 지정했고, user의 속성값 또한 위에서 작성한 user의 name을 지정했다 local-context는 local-user 정보로 local-cluster에 접근하는 하나의 set가 되는 것이다 current-context 현재 사용하는 context 를 지정하는 부분이다 현재 local-context 를 사용하라고 설정해놓았으므로, 터미널에서 kubectl 명령을 입력하면 로컬 쿠버네티스 클러스터를 관리하게 된다 context 조회 및 변경(feat. kubectx) 가장 단순한 방법으로는 ~/.kube/config 파일을 컨트롤하여 context를 조회하거나 수정하는 방법이 있고, 그 다음 방법으로는 kubectl config 명령을 이용하는 방법이 있다 123456789# gcp-context 로 변경$ kubectl config use-context gcp-context# context 조회$ kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE* gcp-context gcp-cluster gcp-user local-context local-cluster local-user 하지만 이것보다 kubectx 라는 더 최적화된 도구가 있다 github : https://github.com/ahmetb/kubectx 간단하게 brew 로 설치할 수 있다 아래는 간단한 사용법이다 12345678910# context 조회$ kubectlgcp-context # 노란색으로 표시local-context# local-context 로 변경$ kubectx local-context# 이전 context로 돌아가기$ kubectx -]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubectl context change</tag>
        <tag>kubectx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[docker] network 구조]]></title>
    <url>%2Fdocker%2Fnetwork-%EA%B5%AC%EC%A1%B0%2F</url>
    <content type="text"><![CDATA[도커를 공부하면서 네트워크 부분에서 알게된 부분은 아래와 같았다 도커 컨테이너를 띄울때마다 매번 동일한 네트워크 대역의 IP가 할당된다 각 컨테이너들은 서로 통신이 가능하다 docker-compose로 컨테이너를 띄우면 다른 네트워크 대역의 IP가 할당된다 이런것을 알게되다 보니 도커의 네트워크 구성이 궁금해졌고, 찾아가며 공부한 내용을 정리하고자 한다 (참고에 있는 글들에 훨씬 잘 설명되어있으니 그걸 읽는게 더 낫다… 난 그냥 개인 정리용…ㅎ) veth interface, NET namespace 도커의 네트워크 구조를 이해하기 위해선 먼저 리눅스의 NET namespace와 veth interface에 대해 알아야한다 veth interface 간단히 말해 랜카드에 연결된 실제 네트워크 인터페이스가 아니라, 가상으로 생성한 네트워크 인터페이스이다 일반적인 네트워크 인터페이스와는 달리 패킷을 전달받으면, 자신에게 연결된 다른 네트워크 인터페이스로 패킷을 보내주는 식으로 동작하기 때문에 항상 쌍(pair)로 생성해줘야 한다 NET namespace 리눅스 격리 기술인 namespace 중 네트워크와 관련된 부분을 말한다 네트워크 인터페이스를 각각 다른 namespace에 할당함으로써 서로가 서로를 모르게끔 설정할 수 있다 (자세한 내용은 https://bluese05.tistory.com/28 를 참조한다… 100배 설명이 더 잘되어 있다) 도커 네트워크 구조 도커는 위에서 언급한 veth interface와 NET namespace 를 사용해 네트워크를 구성한다 그림으로 보면 아래와 같다 생성되는 도커 컨테이너는 namespace 로 격리되고, 그 상태에서 통신을 위한 네트워크 인터페이스(eth0)를 할당받는다 host PC의 veth interface 가 생성되고 도커 컨테이너 내의 eth0 과 연결한다 컨테이너의 네트워크 격리를 달성하기 위해 선택한 방법인 것 같다 host PC의 veth interface 는 docker0 이라는 다른 veth interface 와 연결된다 이 과정이 컨테이너가 추가될 때 마다 반복된다 여기서 나오는 docker0은 docker 실행 시 자동으로 생성되는 가상 브릿지 이다 컨테이너가 생성될 때 마다 가상 인터페이스가 생성되고, 이 브릿지에 바인딩 되는 형태라고 보면 된다 즉, 모든 컨테이너는 외부로 통신할 때 이 docker0 브릿지를 무조건 거쳐가야 되는 것이다 이러한 특징 때문에 도커 컨테이너끼리 서로 통신이 가능한 것이다 docker0 브릿지에 할당된 ip 대역에 맞춰 컨테이너들도 ip가 할당된다 e.g. docker0 = 172.17.0.1/16, container1 = 172.17.0.2, container2 = 172.17.0.3 … IP는 veth가 가지는걸까, eth0 이 가지는걸까?(가상 네트워크 인터페이스에 대한 이해가 조금 더 필요하다) 아래는 컨테이너 2개를 띄웠을 떄의 모습이다 123456789101112$ ip link ls1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:d8:72:d9 brd ff:ff:ff:ff:ff:ff3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:c0:e5:8c:93 brd ff:ff:ff:ff:ff:ff4: vethf3bf38b@if34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether 02:13:50:cf:5c:48 brd ff:ff:ff:ff:ff:ff link-netnsid 15: veth1680438@if36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether f2:b2:f7:eb:a5:55 brd ff:ff:ff:ff:ff:ff link-netnsid 2 그리고 아래는 브릿지에 연결되어 있는 veth interface 를 조회한 모습이다 1234$ brctl show docker0bridge name bridge id STP enabled interfacesdocker0 8000.0242c0e58c93 no veth1680438 vethf3bf38b 참고로 mac이나 window에서는 이 docker0 브릿지나 veth interface 들을 볼 수 없다 VM 안으로 감쳐줘 있기 때문이다 사실 docker container의 네트워크 모드는 총 4개이다 bridge, host, container, none 으로 총 4개가 존재한다 위에서 살펴본 구조가 bridge 네트워크로 생성한 container의 동작방식이며, default 값이다 kubernetes의 pod을 생성할때는 container 모드를 사용한다 자세한 내용은 https://bluese05.tistory.com/38를 참조한다 docker-compose 로 띄우면 다른 네트워크 대역을 가진다 docker-compose 를 공부할 때 docker-compose 로 띄운 컨테이너들은 모두 같은 네트워크에 속한다는 말이 있었고, 실제로도 그랬다 단독으로 띄운 도커 컨테이너와 docker-compose 를 통해 띄운 도커 컨테이너들을 들어가서 ip를 확인해보면 각자 네트워크 대역대가 다름을 알 수 있다 이는 docker-compose 로 컨테이너를 띄우면 compose 로 묶은 범위에 맞춰 브릿지를 하나 더 생성하기 때문이다 123456789101112131415161718192021222324# 이렇게 docker-compose로 컨테이너를 띄우고$ docker-compose -f docker-compose.yml up# 네트워크 인터페이스를 확인해보면$ ip link ls1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:d8:72:d9 brd ff:ff:ff:ff:ff:ff# docker0 브릿지와 아까 띄워놓은 컨테이너들3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:c0:e5:8c:93 brd ff:ff:ff:ff:ff:ff4: vethf3bf38b@if34: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether 02:13:50:cf:5c:48 brd ff:ff:ff:ff:ff:ff link-netnsid 15: veth1680438@if36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether f2:b2:f7:eb:a5:55 brd ff:ff:ff:ff:ff:ff link-netnsid 2# 여기서부터 docker-compose로 띄운 부분# bridge가 하나 더 생겼다!6: br-776e18676383: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:e0:f3:47:6b brd ff:ff:ff:ff:ff:ff7: vethc059d77@if39: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master br-776e18676383 state UP mode DEFAULT group default link/ether 0a:f7:37:0c:cd:64 brd ff:ff:ff:ff:ff:ff link-netnsid 38: veth0fea37d@if41: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master br-776e18676383 state UP mode DEFAULT group default link/ether b2:95:86:71:c6:43 brd ff:ff:ff:ff:ff:ff link-netnsid 4 이런 상태이므로 물론 docker-compose로 띄운 컨테이너와 일반 컨테이너간의 통신은 불가능하다(서로 경유하는 브릿지가 다르므로) 외부와 통신은 어떻게 할까? 우리는 컨테이너를 띄울 때 아래와 같이 포트포워딩을 설정하여 외부에 컨테이너를 공개할 수 있다(+expose) 123456# 포트포워딩 설정하여 컨테이너 생성$ docker container run -p 8080:80 nginx# 포트 listen 확인$ netstat -nlp | grep 8080tcp6 0 0 :::8080 :::* LISTEN 26113/docker-proxy- 근데 보다시피 docker-proxy 라는 프로세스가 해당 포트를 listen 하고 있음을 볼 수 있다 이는 간단히 docker host 로 들어온 요청을 해당하는 컨테이너로 넘기는 역할만을 수행하는 프로세스이다 컨테이너에 포트포워딩이나 expose를 설정했을 경우 같이 생성되는 프로세스이다 그렇지만 중요한것은, 실제로 포트포워딩을 이 docker-proxy가 담당하는 것이 아니라, host PC iptables 에서 관리한다는 점이다 12345678910111213141516171819202122$ iptables -t nat -L -nChain PREROUTING (policy ACCEPT)target prot opt source destinationDOCKER all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCALChain INPUT (policy ACCEPT)target prot opt source destinationChain OUTPUT (policy ACCEPT)target prot opt source destinationDOCKER all -- 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCALChain POSTROUTING (policy ACCEPT)target prot opt source destinationMASQUERADE all -- 172.17.0.0/16 0.0.0.0/0MASQUERADE tcp -- 172.17.0.4 172.17.0.4 tcp dpt:80Chain DOCKER (2 references)target prot opt source destinationRETURN all -- 0.0.0.0/0 0.0.0.0/0DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.17.0.4:80 보다시피 모든 요청을 DOCKER Chain 으로 넘기고, DOCKER Chain 에서는 DNAT를 통해 포트포워딩을 해주고 있음을 볼 수 있다 (이 iptables 룰은 docker daemon이 자동으로 한다) docker-proxy 는 iptables가 어떠한 이유로 NAT를 사용하지 못하게 될 경우 사용된다고 한다 참고 : NET namespace, veth interface : https://bluese05.tistory.com/28 도커 네트워크 구조 : https://bluese05.tistory.com/15 도커 네트워크 외부 통신 : https://bluese05.tistory.com/53]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker network</tag>
        <tag>docker0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[kubernetes] 주요 개념]]></title>
    <url>%2Fkubernetes%2F%EC%A3%BC%EC%9A%94-%EA%B0%9C%EB%85%90%2F</url>
    <content type="text"><![CDATA[kubernetes란? 구글이 2014년에 발표한 컨테이너 오케스트레이션 도구이다 컨테이너 오케스트레이션 도구란 많은 수의 컨테이너를 협조적으로 연동시키기 위한 통합 시스템을 말한다 도커 등장 이래로 많은 오케스트레이션 도구가 나왔지만(mesos, ECS, Swarm 등), 쿠버네티스가 가장 강력한 끝판왕으로 등장함에 따라 현재는 사실상 표준이 된 상태이다 많은 클라우드 플랫폼에서도 쿠버네티스를 연동하여 사용할 수 있는 기술을 제공한다 GCP는 GKE, AWS는 EKS, 애저는 AKS 로컬에서 kubernetes 띄우기 예전에는 로컬에서 kubernetes를 띄우려면 minikube 를 이용해야 했는데 minikube는 기존에 도커를 위해 띄워진 VM에 쿠버네티스를 띄우는 것이 아니라 새로운 VM(dockerd)를 띄우는 방식이므로 조금 까다로운 면이 있었다 하지만 요즘에는 윈도우/macOS용 도커에서 쿠버네티스 통합 기능을 제공해주므로 이를 사용하여 간단하게 쿠버네티스를 구축할 수 있다 대신 minikube 에 자동으로 설치되어 있는 대시보드 같은것은 추가로 설치해줘야 하는 불편함은 있다 mac에서 kubernetes 설치는 kubernetes 탭의 Enable Kubernetes만 클릭해주면 된다 그리고 쿠버네티스 API를 실행하기 위한 명령행 도구인 kubectl도 설치해준다 https://kubernetes.io/docs/tasks/tools/install-kubectl/ kubernetes 주요 개념 쿠버네티스 내에는 매우 다양한 리소스들이 존재하고, 이 리소스들이 클러스터내에서 서로 연동하고 협조하면서 컨테이너 시스템을 구성하는 형태이다 하나의 쿠버네티스 환경 자체를 클러스터라고 부른다 노드 쿠버네티스 내에 떠있는 호스트들(가상머신이나 물리적 서버머신)이다 이중에서도 마스터 노드와 일반 노드들로 나뉘는데, 마스터 노드에는 쿠버네티스 클러스 전체를 관리하기 위한 관리 컴포넌트들이 자리하는 곳이고, 이 관리 컴포넌트들에 의해 일반 노드들로 컨테이너들이 오케스트레이션 되는 구조이다 마스터 노드에 들어가는 관리 컴포넌트들의 종류는 아래와 같다 kube-apiserver kube-scheduler kube-controller-manager etcd 마스터 노드에는 이 관리 컴포넌트 외에 다른 컴포넌트(Pod)들은 들어갈 수 없다 여기까지가 기본적인 개념이고, 이제부터 쿠버네티스의 주요 리소스에 대해 설명하겠다 아래는 간단한 설명이며, 상세한 설명은 조대협님의 블로그를 보는 것이 좋을 것 같다 오브젝트 쿠버네티스에서 가장 중요한 부분은 오브젝트라는 개념인데, 이 오브젝트는 크게 기본 오브젝트와 컨트롤러로 나뉜다 기본 오브젝트는 리소스들의 가장 기본적인 구성 단위이며, 컨트롤러는 이 기본 오브젝트들을 생성하고 관리하는 기능을 가진 애들을 말한다 기본 오브젝트의 종류는 아래와 같고 파드 서비스 볼륨 네임스페이스 컨트롤러의 종류는 아래와 같다 레플리카 셋 디플로이먼트 스테이트풀 셋 데몬 셋 잡 참고로 아래에서 설명하겠지만, 이 오브젝트들은 모두 논리적인 단위이다 각 노드들에 생성된 수많은 도커 컨테이너, 네트워크 인터페이스들을 쿠버네티스가 논리적인 단위로 묶어서 Pod, Service, namespace 등의 개념으로 제공하게 되는 것이다 이 오브젝트에 중요한 속성이 있는데, 바로 스펙과 상태이다 스펙은 우리가 직접 설정파일 같은 것으로 작성해서 전달해줘야 하는것으로써, 오브젝트가 어떤 상태가 되어야 한다고 작성한 것을 말한다 아래는 스펙의 간단한 예제이다 1234567891011apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myappspec: containers: - name: myapp-container image: busybox # 이미지는 로컬을 참조하지 않고 항상 registry에서 땡겨오는 것 같다 command: ['sh', '-c', 'echo Hello Kubernetes! &amp;&amp; sleep 3600'] busybox 이미지를 가진 container가 하나 들어가있는 pod를 띄워야한다고 스펙에 명시했다 이 스펙을 보고 쿠버네티스는 오브젝트를 생성하게 될 것이며, 이 오브젝트에 대한 상태는 쿠버네티스에 의해 제공되게 된다 그리고 쿠버네티스는 이 오브젝트의 상태가 우리가 원한 상태와 일치하도록 계속 관리해주는 역할을 수행하게된다 원하는 상태를 직접 명시하지는 않았지만, 아마도 RUNNING 을 말하는거겠지… 기본 오브젝트 - Pod 쿠버네티스의 가장 기본적인 배포 단위(컨테이너)이다 우리가 알고있는 도커 컨테이너와는 조금 다른게, 하나의 Pod는 하나 이상의 컨테이너를 포함할 수 있는 구조이다 즉 웹서버를 구성한다고 할 때 nginx pod, spring-boot pod, mysql pod 를 각각 띄워야 하는 것이 아니라 이 컨테이너들을 모두 하나의 pod에 넣을 수 있다는 의미이다 위의 오브젝트의 스펙을 설명하는 부분에서 Pod의 설정파일 예시를 작성했는데, 보다시피 spec에 containers 로 여러 컨테이너를 받을 수 있게 되어있다 하나의 Pod는 하나의 노드에만 배치될 수 있다 Pod 내의 컨테이너가 각각 다른 노드에 배치될 수 없다 이러한 특징 때문에, 쿠버네티스 내에서 Pod를 띄울경우 내부의 컨테이너가 다 떠야 Pod 의 상태가 RUNNING 으로 표시되고(e.g. 2/2), Pod 에 접근하고자 할 경우 -c 옵션으로 접근할 컨테이너를 지정해줘야 한다 1$ kubectl exec -it myapp-pod /bin/bash -c myapp-container 그리고 추가로, Pod에는 관심사를 합칠 수 있다는 장점(컨테이너들을 여러개 묶어서 배포할 수 있으므로) 외에도 추가적인 장점이 존재한다 첫째로, Pod 내의 컨테이너들은 서로 IP와 Port를 공유한다 기존에 docker-compose로 띄웠던 컨테이너들이 서로 이름으로 참조했던 방식이 아닌, 서로 localhost로 통신할 수 있는 방식이다 즉, 1개의 Pod 내에 만약 spring-boot, mysql이 있다고 한다면 각자 서로를 localhost:8080, localhost:3306 으로 참조할수 있게 되는 것이다 각자의 IP를 가지는 컨테이너들이 어떻게 localhost로 통신할 수 있을까? 둘째로, Pod 내부의 모든 컨테이너가 공유하는 볼륨을 설정할 수 있다 즉 Pod 내부의 모든 컨테이너는 그 볼륨에 접근할 수 있고, 그 컨테이너끼리 데이터를 공유하는 것을 허용하게 되는 것이다 기본 오브젝트 - 서비스 아래에서 다시 설명하겠지만, Pod의 경우 kubernetes 에서 가장 작은 단위의 리소스라 삭제되거나 추가되는 행위가 잦은 리소스이다(scalable) 문제는 매번 삭제되고 추가될 때 마다 IP가 랜덤하게 새로 부여된다는 것이다 이러한 상황에서는 고정된 엔드포인트로 호출하는 것이 어려워진다 또한 Pod의 경우 보통 1개로 운영하지 않고, 여러개의 Pod을 띄워서 로드밸런싱을 제공해줘야 한다 즉, 이러한 역할을 해주는 리소스가 Pod들 앞단에 하나 더 존재해야하는데, 서비스가 이러한 역할을 한다 서비스는 지정된 IP로 생성이 가능하고, 여러 Pod를 묶어서 로드 밸런싱이 가능하며, 고유한 DNS 이름을 가질 수 있다 서비스가 Pod 들을 묶을 때는 레이블과 셀렉터 라는 개념을 사용하는데, 아래의 스펙을 보면 직관적으로 이해할 수 있을 것이다 12345678910apiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: app: myapp ports: - protocol: TCP port: 80 보다시피 selector 로 app: myapp 이라고 선언해놓았는데, 이는 app: myapp 이라는 레이블을 가진 Pod 들을 선택해서 my-service 라는 서비스로 묶겠다는 의미이다 그렇다면 label은 어떻게 설정하는가? 위의 Pod 를 설정할 때 metadata에 label: myapp 이라는 부분을 봤을 것이다 1234567apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myapp... 이 부분이 리소스에 label을 설정하는 부분이다 label은 여러개 설정할 수 있다 이제 이렇게 서비스를 정의했으니 이 서비스를 통해 Pod 에 접근할 수 있어야 할 것이다 이는 서비스를 생성할 때 지정하는 타입에 따라 방식이 나뉜다 쿠버네티스에서 지원하는 서비스의 타입들은 아래와 같다 ClusterIp 디폴트 설정으로, 서비스에 내부 IP(Cluster IP)를 할당한다 그러므로 클러스터 내에서는 접근이 가능하지만, 클러스터 외부에서는 접근이 불가능하다 클러스터 내의 컨테이너로 들어간 뒤 curl http://my-service[:80] 를 호출하면 myapp 레이블을 가진 pod의 80포트로 연결될 것이다(targetPort 지정가능, 지정하지 않을 시 port와 동일하게 설정) NodePort Cluster IP 로 접근가능하면서 모든 노드의 IP와 포트를 통해서도 접근이 가능하게 된다 123456789....spec: selector: app: myapp type: NodePort ports: - protocol: TCP port: 80 nodePort: 31111 클러스터 내에서 &lt;내부IP&gt;:&lt;포트&gt;으로도 접속 가능하고, 외부에서 &lt;NodeIP&gt;:&lt;NodePort&gt;로도 접근 가능하다 모든 Node의 포트가 열리고, 해당 서비스로 연결된다 30000 ~ 32767 까지 사용 가능 노드의 IP가 바뀔 수 있는 부분을 처리해줘야함 보통 ingress랑 같이 씀 ingress랑 같이 쓰면 해결됨 api gateway 붙이기 싫을 때 ingress를 쓴다? LoadBalancer 보통 클라우드 서비스에서만 설정 가능한 방식으로, 외부 IP를 가지고 있는 로드밸런서를 할당한다 외부 IP를 가지고 있기 때문에 외부에서 접근이 가능하다 클라우드 서비스 내에서 외부 IP가 할당된 로드밸런서를 생성하고 서비스에 연결시키는 구조 서비스 자체에 로드밸런싱 기능이 있는데, 왜 굳이 로드밸런서를 할당시키는지? ExternalName 외부 서비스를 쿠버네티스 내부에서 호출하고자 할 떄 사용할 수 있다 클러스터 내의 Pod들이 클러스터 밖에 있는 서비스(예를 들면 RDS)를 호출하려면 NAT 설정 등 복잡한 설정이 필요한데, 서비스를 externalName 타입으로 설정하면 이를 간단하게 해결 가능하다 123456789apiVersion: v1kind: Servicemetadata: name: my-service-for-rdsspec: selector: app: myproxy type: ExternalName externalName: xxxx-rds.amazonaws.com 이렇게 설정하면 클러스터 내의 Pod 들이 이 서비스를 호출할 경우 xxxx-rds.amazoneaws.com 으로 포워딩해주게 된다(일종의 프록시 역할) 기본 오브젝트 - 볼륨 도커로 볼륨을 연결할때를 생각해보면, 볼륨은 도커 컨테이너가 생성된 호스트에 위치해야 했다 하지만 쿠버네티스의 특성상 Pod가 여러 호스트(노드)를 교차하면서 배포되므로, 이러면 너무 번거롭다 쿠버네티스는 이를 해결하기 위해 PersistentVolume, PersistentVolumeClaim 이라는 것을 제공한다 간단히 말해 PersistentVolume은 쿠버네티스에 지정한 물리 디스크이고, PersistentVolumeClaim은 그 PersistentVolume과 Pod를 연결할 수 있게 해주는 개념이다 자세한 내용은 https://bcho.tistory.com/1259 를 참고한다 PersistentVolume 외에도 쿠버네티스는 다양한 볼륨을 지원한다 https://kubernetes.io/docs/concepts/storage/volumes/ 기본 오브젝트 - 네임스페이스 쿠버네티스 클러스터내의 논리적인 분리단위이다 네임스페이스별로 리소스들을 나눠서 관리할 수 있고, 접근권한, 리소스 할당량 등을 설정할 수 있다 컨트롤러 - ReplicaSet 알다시피 어느정도 규모가 되는 어플리케이션을 구축하려면 하나의 Pod로는 안되고, Pod를 여러개 실행해 가용성을 확보해야 한다 이럴때 사용하는 것이 RelicaSet이다 ReplicaSet는 똑같은 정의를 갖는 Pod를 여러개 생성하고 관리하기 위한 리소스이다 1234567891011121314151617apiVersion: apps/v1kind: ReplicaSetmetadata: name: frontendspec: replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v3 보다시피 크게 replicas, selector, template 3가지의 파트로 구성된다 replicas ReplicaSet에 의해 관리될 Pod의 개수이다 설정된 값보다 Pod의 수가 적으면 추가로 띄우고, Pod의 수가 더 많으면 남는 Pod를 삭제한다 selector ReplicaSet으로 관리할 Pod를 선택한다 현재는 label을 기반으로 select 하고 있다 service 의 selector 와 문법이 달라서 혼동될 수 있는데, 그냥 지원되지 않는 것이라고 한다(deployment 는 됨) https://medium.com/@zwhitchcox/matchlabels-labels-and-selectors-explained-in-detail-for-beginners-d421bdd05362 template Pod를 추가로 띄울 때 어떻게 만들지에 대한 Pod 정보를 정의해놓은 부분이다 새로 생성된 Pod도 selector에 의해 선택되어야 하므로 selector의 label과 동일하게 맞춰줘야 한다 아래는 ReplicaSet에 대해 조금 주의(?)할 부분들이다 ReplicaSet 생성 시 label이 일치하면 기존에 떠있는 Pod 들도 같이 ReplicaSet로 묶이는데, 이 Pod들이 template에 있는 Pod의 형태와 않더라도 삭제되지 않음에 주의해야 한다 e.g. 기존에 app: reverse-proxy 레이블의 apache Pod가 떠있는 상태에서, selector = app: reverse-proxy, template = nginx의 ReplicaSet을 생성하면 기존의 apache Pod는 삭제되지않고 같은 ReplicaSet이 된다 selector 가 Pod 들을 묶는 기준이니까, 기존에 ReplicaSet가 띄워진 상태에서 selector 를 바꾸게 되면 기존의 Pod 들은 삭제되지 않고 남아있게 되나? ReplicaSet으로 떠있는 상태에서 selector를 바꾸게 되면 문법적으로 오류가 발생한다 ReplicaSet에 대한 메타정보(떠있는 Pod들과 매핑 등)이 마스터 노드 어딘가에… 저장되어서 그것으로 판단하는 것 같다 컨트롤러 - Deployment ReplicaSet 보다 상위에 있는 개념으로 ReplicaSet 배포의 기본 단위가 되는 리소스이다 아래와 같은 관계이다 쿠버네티스는 이 Deployment를 단위로 애플리케이션을 배포한다 실제 운영에서는 ReplicaSet을 직접 다루기보다는 Deployment를 통해 배포하는 경우가 대부분이다 설정은 ReplicaSet와 거의 동일하게 작성한다 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: frontend label: app: frontendspec: replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v3 Deployment의 특징은 리비전을 사용해 배포를 관리할 수 있다는 점이다 Deployment를 생성한 뒤 리비전을 확인해본다 123456$ kubectl apply -f deployment.yml --record # 어떤 kubectl을 실행했는지 남기기 위함$ kubectl rollout history deployment frontenddeployment.extensions/fronendREVISION CHANGE-CAUSE1 kubectl apply --filename=deployment.yaml --record=true 결과로는 REVISION=1 값이 출력됨을 볼 수 있다 이 리비전값은 아래와 같은 특성이 있다 replicas 의 값을 바꿔도 리비전값이 올라가진 않는다 replicaSet 컨테이너의 이미지를 바꾸고 적용하면 리비전값이 올라간다 리비전을 올리기 위해 아래와 같이 컨테이너 이미지를 바꾸고 123containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v4 deployment를 다시 적용하면 리비전값이 올라감을 볼 수 있다 123456$ kubectl rollout history deployment echodeployment.extensions/frontendREVISION CHANGE-CAUSE1 kubectl apply --filename=deployment.yaml --record=true2 kubectl apply --filename=deployment.yaml --record=true 123456789$ kubectl get pods --selector app=frontendNAME READY STATUS RESTARTS AGEfrontend-6bfffbcf9f-n42dh 1/1 Running 0 1mfrontend-6bfffbcf9f-tssb9 1/1 Running 0 1mfrontend-6bfffbcf9f-vfmcj 1/1 Running 0 1mfrontend-8556ddbfb9-zpfrg 0/1 Terminating 0 3mfrontend-8556ddbfb9-9p7ld 0/1 Terminating 0 3mfrontend-8556ddbfb9-x5kvb 0/1 Terminating 0 3m 이렇게 리비전으로 관리하게 됨으로써 Deployment를 롤백이 가능하게 된다 $ kubectl rollout undo deployment frontend Pod를 확인해보면 바로 직전 리비전으로 롤백되고 있음을 볼 수 있다(바로 직전만 가능한 듯 하다) 롤백되면 리비전이 다시 1로 돌아가는 것이 아니라, 3으로 올라간다 리비전은 최대 10까지 가능한 것 같다 참고 : 야마다 아키노리, 『도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문』, 심효섭 옮김, 위키북스(2019) 쿠버네티스 개념 이해 https://bcho.tistory.com/1256?category=731548 쿠버네티스 서비스 https://bcho.tistory.com/1262]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문</tag>
        <tag>kubernetes object</tag>
        <tag>kubernetes pod</tag>
        <tag>kubernetes service</tag>
        <tag>kubernetes replicaset</tag>
        <tag>kubernetes deployment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[docker] volume container 추가하기]]></title>
    <url>%2Fdocker%2Fvolume-container-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[volume container란? 일반적으로 docker container는 컨테이너 내부에 데이터를 관리하므로, 컨테이너가 파기되면 데이터가 모두 날라가게 된다 이는 mysql 같은 데이터 스토리지를 사용할 경우 위험하게 되는데, 이를 방지하기 위해 따로 볼륨을 설정해서 데이터를 저장해줘야 한다 호스트OS 디렉토리를 마운트시켜서 데이터를 관리할 수도 있지만, 호스트쪽 디렉토리에 의존이 생기고 만약 이 디렉토리의 데이터를 잘못 손대면 애플리케이션에 부정적 영향을 미칠 수 있기 때문에 이 방식은 사용하지 않는것이 좋다 그래서 이에 대한 대안으로 추천되는것이 볼륨 컨테이너이다 볼륨 컨테이너는 말 그대로 데이터를 저장하는 것이 목적인 컨테이너이다 기본적으로 우리는 Dockerfile 작성 시 아래와 같이 볼륨을 설정할 수 있다 12345FROM mysqlVOLUME /var/lib/mysql# ... 위처럼 작성하게 되면 컨테이너 내의 /var/lib/mysql 디렉터리가 호스트 PC의 /var/lib/docker/volumes/${volume_name}/_data에 마운트 된다 (볼륨 이름은 임의의 해쉬값으로 생성된다) 참고로 mac이나 windows의 경우 /var/lib/docker/volumes 디렉토리가 없는데, 이는 mac이나 windows의 경우 docker를 바로 실행할 수 없으므로 VM을 하나 띄운 뒤, docker를 실행하기 때문이다 즉, /var/lib/docker/volumes 디렉토리는 mac과 docker 사이에 띄워진 VM 내에 감춰져있다 참고 : https://forums.docker.com/t/var-lib-docker-does-not-exist-on-host/18314 volume container는 볼륨의 이러한 특징을 사용한 것이다 컨테이너 자체를 볼륨을 관리하는 애로 만들어서 캡슐화하고, 이를 다른 컨테이너의 볼륨에 매핑해서 결합을 느슨하게 하는 것이다 만약 아래와 같은 볼륨 컨테이너를 작성하고, 1234FROM busybox # 최소한의 운영체제 기능만 제공VOLUME /var/lib/mysqlVOLUME /var/log /var/lib/mysql, /var/log 디렉토리에 대한 볼륨 2개가 생성되어 각각 /var/lib/docker/volumes/${volume_name}/_data 에 마운트 된다 빌드하고 컨테이너로 띄운 뒤, 12$ docker image build -t volume_container:latest .$ docker container run -d volume_container:latest 참고로 볼륨 컨테이너를 띄우면 바로 종료되는데, 이렇게 종료된 컨테이너를 사용해도 상관없다 --volumes-from 옵션으로 다른 컨테이너에 연결한다면 1$ docker container run --volumes-from volume_container mysql:5.7 아래와 같은 형태가 되는 것이다 12mysql container -&gt; volume_container -&gt; /var/lib/docker/volumes/$&#123;/var/lib/mysql&apos;s volume_name&#125;/_data -&gt; /var/lib/docker/volumes/$&#123;/var/log&apos;s volume_name&#125;/_data docker-compose.yml 에 volume container 추가 위의 방식처럼 컨테이너를 직접 만들고 다른 컨테이너 실행 시 --volumes-from 속성으로 연결해주는 방법도 있지만, docker-compose를 사용 시 좀 더 간단한 방법을 제공해준다 아래처럼 작성하면 된다 12345678910111213141516171819202122version: "3"services: test_database: image: mysql:5.7 environment: MYSQL_DATABASE: test_db MYSQL_ROOT_PASSWORD: root MYSQL_ROOT_HOST: '%' ports: - 3306:3306 volumes: - test_volume:/var/lib/mysql test_application: build: . expose: - 8080 depends_on: - test_databasevolumes: test_volume: 보다시피 test_volume 이라는 볼륨을 생성하고, 사용하는 쪽에서 ${volume_name}:${mount를 원하는 디렉토리} 의 형태로 지정해주면 된다 docker-compose 설정파일 v2 의 형태로 보면 좀 더 직관적으로 이해가 갈 것이다 12345678910111213141516171819202122232425version: "2"services: test_database: image: mysql:5.7 environment: MYSQL_DATABASE: test_db MYSQL_ROOT_PASSWORD: root MYSQL_ROOT_HOST: '%' ports: - 3306:3306 volumes: - test_volume test_application: build: . expose: - 8080 depends_on: - test_database test_volume: image: busybox volumes: - /var/lib/mysql - /var/log v3의 경우 컨테이너를 따로 생성하지 않아도 된다는 장점이 있다 볼륨 컨테이너는 충분히 좋은 기능이지만, 그래도 범위가 같은 도커 호스트 안이라는 사실은 변하지 않는다 이렇듯 데이터 이식 면에서는 아직 개선할 부분이 많이 남아있다 참고 : https://darkrasid.github.io/docker/container/volume/2017/05/10/docker-volumes.html https://stackoverflow.com/questions/45494746/docker-compose-volumes-from-usage-example]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문</tag>
        <tag>volume container</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[docker] docker-compose로 nginx + spring-boot + mysql 구성하기]]></title>
    <url>%2Fdocker%2Fdocker-compose%EB%A1%9C-nginx-spring-boot-mysql-%EA%B5%AC%EC%84%B1%ED%95%98%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[알다시피 일반적으로 시스템은 단일 어플리케이션 만으로 구성되지 않는다 다른 어플리케이션 서버나 미들웨어 등과 서로 통신하며 하나의 시스템이 구성된다 이번에는 가장 일반적인 구조인 리버스 프록시(nginx) + 어플리케이션 서버(spring-boot) + 데이터 스토어(mysql) 를 구성해보겠다 각각의 구성요소는 서로 의존관계가 있다 서로 통신할 수 있어야하고 각 구성요소가 올라오는 순서도 맞춰줘야 한다 물론 도커로 다 설정할 수 있긴하지만, 사람이 매번 수작업으로 맞춰줘야 하기 때문에 번거롭고 실수하기도 쉽다 그래서 이런 컨테이너의 실행을 한번에 관리할 수 있게 해주는 docker-compose를 사용할 것이다 사전 구성 spring boot로 간단히 사용자를 저장하고, 조회하는 로직을 작성한다 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// TestController.java@RequiredArgsConstructor@RequestMapping("/users")@Controllerpublic class TestController &#123; private final UserRepository userRepository; @PostMapping public ResponseEntity&lt;Void&gt; createUser(@RequestBody UserRequest request) &#123; User user = userRepository.save(request.toEntity()); return ResponseEntity.created(URI.create("/users/" + user.getId())).build(); &#125; @GetMapping public ResponseEntity&lt;List&lt;User&gt;&gt; getUsers() &#123; return ResponseEntity.ok( userRepository.findAll() ); &#125; @GetMapping("/&#123;id&#125;") public ResponseEntity&lt;User&gt; getUser(@PathVariable Long id) &#123; return ResponseEntity.ok( userRepository.findById(id) .orElseThrow(IllegalStateException::new) ); &#125;&#125;// UserRequest.java@Getter@Setterpublic class UserRequest &#123; private String name; private Integer age; public User toEntity() &#123; return new User(name, age); &#125;&#125;// User.java@NoArgsConstructor(access = AccessLevel.PROTECTED)@Entity@Table(name = "user")public class User &#123; @Getter @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = "id") private Long id; @Getter @Column(name = "name") private String name; @Getter @Column(name = "age") private Integer age; public User(String name, Integer age) &#123; this.name = name; this.age = age; &#125;&#125;// UserRepository.javapublic interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123;&#125; 보다시피 user 를 조회하고 저장하는 기능을 가진 간단한 API 서버이다 이를 도커 컨테이너로 띄우기 위해 Dockerfile을 작성한다 123456FROM openjdk:8-jdkCOPY ./test-application /test-applicationWORKDIR /test-applicationCMD ["./gradlew", "bootRun"] test-application은 spring-boot 어플리케이션이 있는 디렉토리이다 간단하게 spring-boot 어플리케이션이 있는 디렉토리 전체를 컨테이너로 복사한 뒤, ./gradlew bootRun으로 어플리케이션을 실행시킨다 spring-boot, mysql 띄우기(feat. docker-compose.yml) 이제 위 파일을 docker image로 만들어주면 되는데, 알다시피 저 Dockerfile을 사용해 docker image를 만들어봐야 사용하지 못한다 db가 없기 때문이다 mysql을 docker container 로 띄운 뒤 spring-boot 를 docker container로 띄우면 되긴 하지만, 번거로우므로 이를 같이 해줄 수 있는 docker-compose 설정 파일을 작성한다 12345678910111213141516171819version: "3"services: test_database: # 컨테이너 이름을 주고 싶다면 작성한다 # container_name: test_database image: mysql:5.7 environment: MYSQL_DATABASE: test_db MYSQL_ROOT_PASSWORD: root MYSQL_ROOT_HOST: '%' ports: - 3306:3306 test_application: build: . ports: - 8080:8080 depends_on: - test_database mysql과 spring-boot 컨테이너 2개를 띄워주는 docker-compose.yml 파일이다 mysql의 경우 이미지명을 지정해 레지스트리에서 땡겨오도록 했고, spring-boot의 경우 현 위치에 있는 Dockerfile을 참조하여 만든 image를 컨테이너로 띄우게끔 했다 (docker-compose.yml과 spring-boot Dockerfile은 같은 위치에 있다) 여기서 중요한 것은 depends_on 속성인데, 이렇게 해놓으면 mysql 컨테이너가 다 뜬 다음 spring-boot 컨테이너가 뜨게끔 설정된다 이제 spring-boot에서 mysql을 바라볼 수 있도록 application.properties에 접속 정보를 작성한다 123456789spring.datasource.url=jdbc:mysql://test_database:3306/test_db?useSSL=falsespring.datasource.username=rootspring.datasource.password=rootspring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.jpa.hibernate.ddl-auto=updatespring.jpa.database-platform=org.hibernate.dialect.MySQL5Dialectspring.jpa.generate-ddl=truespring.jpa.show-sql=true 여기서 특별한 부분은 datasource url로 ip 대신 test_database 라고 준 부분이다 이는 docker-compose.yml에 써놓은 mysql의 서비스명과 동일한데, 이는 docker-compose.yml 내에 작성한 컨테이너들은 모두 같은 네트워크 대역으로 묶어서 생성하기 때문이다 compose 단위로 새로운 네트워크 대역으로 생성한다 그냥 단일 도커들만 실행하면 기본 네트워크 대역에 할당된다 compose 없이 컨테이너 각각 띄워보고 들어가서 ip 확인해보면 대역대가 같다 하지만 compose 로 생성한 컨테이너는 대역대가 다르다 이제 작성이 끝났으니, docker-compose 로 컨테이너들을 실행시켜보자 1$ docker-compose up docker-compose는 기본적으로 명령을 실행한 위치에 있는 docker-compose.yml 파일을 참조하여 실행한다 만약 다른 경로에 있거나 다른 파일명을 사용하고 싶을 경우 -f 옵션으로 docker-compose 파일을 지정해주면 된다 mysql과 spring-boot가 차례대로 실행되는것을 볼 수 있다 -d 옵션을 주면 백그라운드로 실행시킬 수 있다 postman으로 localhost:8080으로 API를 호출해보면, 잘 동작함을 볼 수 있다 nginx 추가 이제 리버스 프록시인 nginx를 추가해보자 docker-compose.yml 파일을 수정한다 1234567891011121314151617181920212223242526version: "3"services: test_web: image: nginx ports: - 80:80 volumes: - ./nginx/conf.d:/etc/nginx/conf.d depends_on: - test_application test_database: image: mysql:5.7 environment: MYSQL_DATABASE: test_db MYSQL_ROOT_PASSWORD: root MYSQL_ROOT_HOST: '%' ports: - 3306:3306 test_application: build: . expose: - 8080 depends_on: - test_database spring-boot 컨테이너가 뜬 다음에 레지스트리에서 nginx를 땡겨와서 컨테이너로 띄우게끔 했다 nginx 를 작성한 부분에 volumes 라는 부분이 보이는데, 이는 호스트의 nginx/conf.d 폴더를 컨테이너의 /etc/nginx/conf.d 폴더로 마운트 해주겠다는 의미이다 이렇게 작성한 이유는 호스트쪽에 작성해놓은 nginx 설정 파일을 nginx 컨테이너가 뜨면서 읽게하기 위함이다 nginx는 /etc/nginx/conf.d 내에 들어있는 모든 .conf 파일을 include 한다 아래는 conf.d 안에 작성한 app.conf 파일이다 1234567891011server &#123; listen 80; access_log off; location / &#123; proxy_pass http://test_application:8080; proxy_set_header Host $host:$server_port; proxy_set_header X-Forwarded-Host $server_name; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; 이제 작성이 끝났으니, docker-compose를 down 했다가 다시 up 한다 앞에 리버스 프록시를 두었으니 url localhost로 변경했을때 잘 작동함을 볼 수 있다 참고로 test_application에 대한 포트포워딩 설정이었던 ports가 expose로 바뀌었는데, 이는 컨테이너 내부에서만 해당 포트를 인식하게끔 하는 속성이다 즉, 다른 컨테이너에서는 8080으로 통신이 가능하지만, 외부에서는 8080으로 더이상 접근할 수 없다 github url : https://github.com/joont92/docker-study/tree/master/step02 참고 : https://github.com/hellokoding/hellokoding-courses/blob/master/docker-examples/dockercompose-springboot-mysql-nginx/docker-compose.yaml]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>docker-compose</tag>
        <tag>docker</tag>
        <tag>nginx</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[docker] container 다루기]]></title>
    <url>%2Fdocker%2Fcontainer-%EB%8B%A4%EB%A3%A8%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[빌드한 도커 이미지를 실행시키면 도커 컨테이너가 된다 언급했듯이 도커 이미지는 하나의 템플릿이므로, 하나의 도커 이미지로 여러개의 도커 컨테이너를 만들 수 있다 도커 컨테이너 생명주기 도커 컨테이너는 도커 이미지를 실행시킨 것이기 때문에 상태를 가지고 있다 실행 중 ENTRYPOINT, CMD에 있는 어플리케이션이 실행된 상태를 말한다 명령행 도구 등의 컨테이너는 이 상태가 길게 유지되지 않는다 정지 실행 중 상태에 있는 컨테이너를 명시적으로 종료하거나, 컨테이너에서 실행된 어플리케이션이 정상/오류를 막론하고 종료된 경우의 상태를 말한다 정지되어도 그 상태 그대로 디스크에 남아있기 때문에(docker container ls 로 확인 가능) 다시 실행할 수 있다 파기 정지된 컨테이너를 명시적으로 삭제하면 파기 상태가 된다(docker container ls 로 확인 불가능) 파기된 컨테이너는 다시 정지된 컨테이너와 같은 상태로 돌아갈 수 없으므로 주의해서 실행해야 한다 아래는 도커 컨테이너를 관리하면서 자주 사용하는 명령어들과 그에 대한 간단한 설명이다 추가적인 명령어나 옵션이 궁금하다면 https://docs.docker.com/engine/reference/commandline/container/ 를 참조한다 docker container run 12$ docker container run [options] 이미지명[:태그명] [명령] [명령인자..]$ docker container run [options] 이미지ID [명령] [명령인자..] 도커 이미지로부터 컨테이너를 생성+실행 하는 명령이다 유용한 옵션이 많으니 활용하면 좋다 -d : 백그라운드로 실행 -p : 포트포워딩 -p 9000:8080 == 호스트 포트 9000번을 컨테이너 포트 8080으로 포워딩 –name : 컨테이너에 이름 부여 매번 컨테이너ID를 조회하는 것이 번거로우므로 이름을 지정할 수 있다 컨테이너 이름은 중복될 수 없기 때문에 개발환경 외에는 잘 사용되지 않는다(운영은 많은수의 컨테이너를 추가/삭제 하므로) -i, -t : -i는 표준 입력을 받을지 여부이고(파이프라이닝, 키보드 입력), -t는 가상터미널을 제공할지 여부이다 1$ docker container run -it ubuntu:16.04 보통 위처럼 -it를 같이 사용하고, 이러면 컨테이너에 쉘에 직접 접근할 수 있게 된다(가상 터미널로 표준 입력을 하는것이 되므로) -t 없이 -i 만 사용해도 의미가 있다(파이프라이닝으로 입력하는 방법이 있으므로) -i 없이 -t 만 사용하는건 의미가 없다(입력을 받지 못하는 상태라 가상터미널을 열어봐야) -d와 -it를 같이 사용할 수 없다(백그라운드라 입력을 대기하거나 가상터미널을 제공하는것이 불가능하다) –rm : 컨테이너를 종료할 때 컨테이너를 파기하는 옵션 같은 이름으로 컨테이너를 실행시킬 수 없기 때문에 이 옵션을 사용해주는것이 좋다 보통 --name 과 같이 사용한다 컨테이너에 할당할 자원도 어느정도 제어할 수 있다 https://jungwoon.github.io/docker/2019/01/13/Docker-6/ docker container ls 1$ docker container ls [options] 컨테이너의 목록을 보여줌 -a : 종료된 컨테이너의 목록도 같이 보여줌(원래는 실행 중인 컨테이너의 목록만 보여줌) -q : 컨테이너 ID만 추출함 1$ docker container stop $(docker container ls -q) 처럼 사용할 수 있음 –filter : 컨테이너 목록 필터링 12$ docker container ls --filter "name=container_name" # 컨테이너명$ docker container ls --filter "ancestor=image_name" # 이미지명 docker container stop 1$ docker container stop 컨테이너ID_또는_컨테이너명 실행중인 컨테이너를 종료할 떄 사용한다 docker container restart 1$ docker container restart 컨테이너ID_또는_컨테이너명 정지한 컨테이너를 재시작할 때 사용한다 start 와 차이가 뭘까? docker container rm 1$ docker container rm 컨테이너ID_또는_컨테이너명 정지된 컨테이너를 완전히 파기할 떄 사용한다 실행중인 컨테이너를 강제로 삭제하고 싶다면 -f 옵션을 사용한다 docker container logs 1$ docker container logs [options] 컨테이너ID_또는_컨테이너명 컨테이너의 표준 출력으로 출력된 내용을 보여준다 -f 옵션을 주면 새로 출력되는 표준 출력을 계속 볼 수 있다 보통은 이 내용을 수집해 웹 브라우저나 명령행 도구를 통해 보여주므로 이 명령을 사용할 일은 많이 없다 docker container exec 1$ docker container exec [options] 컨테이너ID_또는_컨테이너명 컨테이너에서_실행할_명령 실행 중인 컨테이너에서 원하는 명령을 실행할 수 있다 12$ docker container exec ubuntu_docker pwd$ docker container exec ubuntu_docker ls 아예 컨테이너로 접속하고 싶다면 아래와 같이 입력하면 된다 1$ docker container exec -it ubuntu_docker sh docker container cp 12$ docker container cp [options] 컨테이너ID_또는_컨테이너명:원본파일 호스트_대상파일$ docker container cp [options] 호스트_원본파일 컨테이너ID_또는_컨테이너명:대상파일 실행중인 컨테이너에 파일을 복사하거나 복사해 올 때 사용한다 Dockerfile의 CP는 이미지 빌드시에 호스트 -&gt; 컨테이너 방향으로의 복사만 가능하다 1$ docker container cp test_file ubuntu_docker:/tmp/test_file 참고 : 야마다 아키노리, 『도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문』, 심효섭 옮김, 위키북스(2019)]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문</tag>
        <tag>docker container</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[docker] image 다루기]]></title>
    <url>%2Fdocker%2Fimage-%EB%8B%A4%EB%A3%A8%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[도커는 크게 도커 이미지와 도커 컨테이너로 나뉜다 그 중 도커 이미지는 컨테이너를 생성하는 템플릿 역할을 한다 아래는 도커 이미지를 관리하면서 자주 사용하는 명령어들과 그에 대한 간단한 설명이다 추가적인 명령어나 옵션이 궁금하다면 https://docs.docker.com/engine/reference/commandline/image/ 를 참조한다 docker image build 1$ docker image build [options] Dockerfile_경로 Dockerfile에 기술된 내용을 따라 도커 이미지를 생성하는 명령이다 Dockerfile은 필수로 필요하다 마지막 인자로 Dockerfile의 경로를 주는것에 유의해야한다 -t 옵션 1$ docker image build -t javatest:latest . 도커 이미지에 이름과 태그를 붙이는 옵션으로 실제 사용에선 거의 필수적으로 쓰인다 지정한 경로에서 Dockerfile 이라는 이름의 파일을 찾고 그 내용을 토대로 이미지를 만든다 태그명은 생략 가능한데, 생략하면 기본적으로 latest가 붙는다 -f 옵션 1$ docker image build -f Dockerfile_test -t javatest:latest . Dockerfile이 아닌 다른 이름을 쓰고 싶을 경우 사용한다 –pull 옵션 1$ docker image build --pull=true -t javatest:latest . 기본적으로 도커 이미지 빌드 시 FROM 인스트럭션에 지정한 이미지를 레지스트리에서 받은 후, 이를 호스트 운영체제 저장해놓고 재사용한다 이를 매번 새로 받아오게 하고 싶을 경우 사용하면 좋다 이 옵션은 아래와 같은 상황을 대비하기 위해 사용하면 좋다 123FROM somelibrary:latest... 도커 이미지를 빌드한다 FROM 인스트럭션의 somelibrary:latest 이미지를 받고 로컬에 저장해둔다 somelibrary 이미지에 버그가 있어서 수정하고, 다시 latest 태그로 레지스트리에 올렸다 위의 Dockerfile 로 이미지를 새로 빌드한다 레지스트리의 변경사항이 생성된 이미지에 반영되지 않는다 somelibrary:latest가 이미 로컬에 있기 때문에 새로 받아오지 않고 재사용한다 이러한 이유로 실무에서는 latest 대신 보통 태그명을 직접 입력한다 docker search 1$ docker search [options] 검색_키워드 도커 레지스트리에서 이미지를 검색할 때 사용한다 레지스트리를 따로 설정하지 않았다면 기본적으로 도커 허브가 사용된다 도커 허브(Docker Hub)? 도커 사 자체에서 관리하는 도커 이미지 레지스트리로, 깃허브처럼 자신의 계정이나 조직 이름으로 리포티저티를 만들고 이미지를 올릴 수 있다 도커 허브에는 이미 매우 많은 리포지터리가 등록되어 있어서, 직접 도커 이미지를 만들 필요없이 만들어놓은 이미지를 간단하게 사용할 수 있다 참고로 여기서 몇가지 용어의 혼재가 있을 수 있다 도커 이미지가 저장되고 관리되는 공간을 도커 레지스트리라고 부른다 도커 허브, ECR, 사내 도커 레지스트리 등등이 될 수 있다 이러한 도커 레지스트리내에서 이미지를 저장해두는 공간을 리포지터리라고 부른다 그러므로 레지스트리에서 리포지터리를 찾는것은 이미지를 찾는것과 동일한 행위이다 리포지터리명:태그로 접근한다 네임스페이스는 레지스트리내에서 리포지터리의 이름이 중복되는 것을 막기 위해 리포지터리 이름 앞에 작성하는 것을 말한다 12$ docker search mysql$ docker search --limit 5 mysql # 5건만 검색 검색결과는 STARS(깃헙과 동일) 순으로 출력되고, 태그명까지는 검색할 수 없다 mysql 처럼 namespace가 없는 애들도 있는데 이는 mysql의 공식 리포지터리라서 그렇다 docker image name format 로컬 내에서는 상관없지만, 도커 허브와 같은 registry 를 끼게 되면 이미지 이름(태그)에 일종의 포멧이 생기게 된다 1[레지스트리_호스트/]리포지터리명[:태그] 레지스트리 호스트를 지정해서 리포지터리를 땡겨올 수 있다 생략하면 기본적으로 docker.io 가 추가된다(도커 허브) 도커 허브를 사용할 경우 리포지터리명 앞에 namespace로 자신의 계정을 줘야한다 그래야 도커 허브 시스템 내에서 리포지터리를 구분할 수 있다 docker image pull 1$ docker image pull [options] 리포지터리명[:태그명] 도커 레지스트리에서 도커 이미지를 내려받을 때 사용한다 1$ docker image pull jenkins 태그명을 생략하면 default 태그가 사용된다(latest) docker image ls 1$ docker image ls [options] [이미지명[:태그명]] 현재 호스트 운영체제에 저장된 도커 이미지의 목록을 보여준다 참고로 여기서 조회되는 IMAGE ID는 CONTAINER ID와는 별개의 것이므로 주의해야 한다 도커 IMAGE ID 도커는 이미지를 빌드할 때 마다 새로운 이미지 ID가 부여된다 docker 파일을 빌드하여 이미지를 만든다 1$ docker image build -t javatest:latest . docker image ls로 빌드된 이미지가 잘 저장되었는지 확인한다 1234$ docker image ls -aREPOSITORY TAG IMAGE ID CREATED SIZE javatest latest 2327e281c9b2 1 hour ago 4.41MB Dockerfile을 조금 수정하고 다시 빌드하고, docker image ls 로 확인해본다 123456$ docker image build -t javatest:latest .$ docker image ls -a REPOSITORY TAG IMAGE ID CREATED SIZE javatest latest 57ca5e531d73 1 hour ago 4.40MB &lt;none&gt; &lt;none&gt; 2327e281c9b2 2 hour ago 4.41MB IMAGE ID가 다른 이미지가 새롭게 빌드되었음을 볼 수 있다 (Dockerfile을 수정할 때 뿐만 아니라 COPY 되는 대상이 바뀌거나 하여도 새로운 이미지로 빌드된다) 똑같은 이미지명:태그명 으로 빌드했기 때문에 기존의 이미지에서 REPOSITORY와 TAG가 &lt;none&gt;으로 표시되는 것을 볼 수 있다 결국 IMAGE ID는 도커 이미지의 버전이라고 봐도 무방하다 docker image tag 위에서 봤다시피 태그는 이미지 ID를 식별하기 위해 붙이는 일종의 별칭이다 1$ docker image tag 기반이미지명[:태그] 새이미지명[:태그] 이미지를 가리키는 태그를 추가 생성할 떄 사용한다 새이미지명[:태그]가 기반이미지명[:태그]가 가리키고 있던 이미지 ID를 가리키는 상태로 추가된다 1$ docker image tag joont92/javatest:latest joont92/javatest:1.0.0 보통은 이처럼 latest의 특정 시점에 버전 넘버를 태그로 붙이기위해 사용한다 이렇게 하면 아래와 같이 생성된다 12joont92/javatest 1.0.0 4a1fc394fbefjoont92/javatest latest 4a1fc394fbef latest 가 가지고 있던 이미지를 가리키는 1.0.0 이 추가로 생성되었다 docker image push 1$ docker image push [options] 리포지터리명[:태그] 레지스트리로 도커 허브를 사용할 경우 리포지터리명 앞에 자신의 도커 ID로 네임스페이스를 붙여줘야한다 1234# 네임스페이스를 추가해주고$ docker image tag javatest:latest joont92/javatest:latest# push$ docker image push joont92/javatest:latest 리포지터리에 올라갔음을 볼 수 있다 참고 : 야마다 아키노리, 『도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문』, 심효섭 옮김, 위키북스(2019)]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문</tag>
        <tag>docker image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[docker] Dockerfile]]></title>
    <url>%2Fdocker%2FDockerfile%2F</url>
    <content type="text"><![CDATA[Dockerfile이란? 도커 이미지를 만들 때 꼭 필요한 설정파일이다 이 파일내에 작성된 인스트럭션들을 참조하여 이미지가 만들어진다 기본으로 Dockerfile 이라는 이름을 사용하고, 이름을 변경하고 싶다면 이미지 빌드시에 추가 옵션을 줘야한다(-f) 인스트럭션 Dockerfile 내에 있는 명령어들을 말한다 아래는 각 인스트럭션에 대해 간단히 나열한 것이며, 자세한 내용이 궁금하면 https://docs.docker.com/engine/reference/builder/ 를 참조한다 자주 사용하는 인스트럭션 예시로 사용할 자바 어플리케이션과 Dockerfile 이다 Test.java 12345public class Test &#123; public static void main(String[] args) &#123; System.out.println("Hello World!"); &#125;&#125; Dockerfile 123456FROM openjdk:8-jdkCOPY Test.java .RUN javac Test.javaCMD [&quot;java&quot;, &quot;Test&quot;] FROM 해당 도커 이미지의 바탕이 될 베이스 이미지를 지정하는 인스트럭션이다 openjdk를 베이스 이미지로 땡겨왔기 때문에 javac, java 명령어가 실행 가능함을 볼 수 있다 openjdk의 Dockerfile을 따라가보면 FROM에 ubuntu를 사용하고 있다 이런식으로 이미지가 겹겹이 포장되어 빌드되는 형식이다 openjdk 는 이미지명이며, 8-jdk는 태그명이다 레지스트리를 따로 지정하지 않았기 때문에 도커 허브에서 땡겨온다 COPY 호스트 머신의 파일이나 디렉터리를 도커 컨테이너 안으로 복사하는 인스트럭션이다 이미지가 빌드될 때 1번만 실행되는 명령어이다 컨테이너안의 현재 디렉토리로 호스트 머신의 Test.java 이 복사된다 ADD 기본적인 기능은 COPY와 동일하고, 아래의 기능이 추가로 더 있다 source가 remote URL 이면 다운받은 뒤 destination 에 복사한다 source가 잘 알려진 압축파일 형식이면(tar, zip 등) 압축을 풀어준다 source가 remote URL + 압축파일 형식이면 압축을 풀지는 않는다 아무래도 위와 같이 특수한 상황이 아니라면 COPY를 쓰는게 낫다 명시적이기 때문이다 RUN 도커 이미지를 실행할 컨테이너 안에서 실행할 명령을 정의한다 이 또한 이미지가 빌드될 때 1번만 실행되는 명령어이다 복사된 Test.java를 javac로 컴파일하고 있다 CMD 컨테이너 안에서 실행할 프로세스(명령)를 지정한다 이는 이미지가 컨테이너화 될 때(실행될 때)마다 실행되는 명령어이다 컴파일된 Test.class 파일을 실행하고 있다 작성법이 조금 특이한데, 총 3가지 작성법을 제공한다 CMD command param1 param2 […] 가장 익숙한 형태이다 FROM 으로 설정한 이미지에 포함된 쉘 파일을 사용하여 명령을 실행한다 쉘 스크립트 구문을 사용할 수 있다 CMD [“executable”, “param1”, “param2” [, …]] 쉘 없이 바로 실행하면서 매개변수를 던져주는 형태이다 도커에서 권장하는 형태이다 쉘 스크립트 구문을 사용할 수 없다 123CMD ["echo", "Hello, $name"]$ Hello $name 만약 쉘 스크립트를 사용하고 싶다면 쉘을 실행시키면서 인자로 전달해줘야 한다 1CMD ["/bin/bash", "-c", "echo Hello, $name"] CMD [“param1”, “param2” [, …]] ENTRYPOINT에 지정된 명령에 사용할 인자를 전달한다 CMD는 Dockerfile 내에 하나만 작성할 수 있다 만약 CMD를 여러개 작성한다면 가장 앞부분껀 전부 무시되고 가장 마지막에 있는 명령만이 실행된다 12CMD ["javac", "Test.java"]CMD ["java", "Test"] 했다가 안되서 찾아봤음… CMD 명령 오버라이드도 가능하다 위와 같이 선언된 상황에서 12$ docker container run javatest:latest echo joont92$ joont92 # 출력 CMD 명령이 무시됨을 볼 수 있다 그 외 인스트럭션 ENTRYPOINT CMD와 마찬가지로 컨테이너 안에서 실행될 프로세스(명령)를 지정하는 인스트럭션이다 CMD와 다른점은 조금 기준점(?) 이 되는 프로세스를 지정하는 것이랄까… ENTRYPOINT를 입력하면 CMD에 전달된 인자들은 전부 ENTRYPOINT의 인자로 전달된다 1234FROM openjdk:jdk-8ENTRYPOINT ["java"]CMD ["version"] 또한 아래와 같이 사용해서 컨테이너의 용도를 어느정도 제한 할수도 있다 1234FROM golang:1.10ENTRYPOINT ["go"]CMD [""] 자동으로 go 가 입력된 상태라고 보면 된다 go 이후의 명령어만 인자로 넘기면 된다 12$ docker container run gotest:latest version$ go version go1.10.3 linux/amd64 # 출력 LABEL 이미지를 만든 사람의 이름 등을 적을 수 있다 1LABEL maintainer="joont92@github.com" ENV 도커 안에서 사용할 환경 변수를 지정한다 123ENV CLASSPATH=/workspace/javatestCMD ["java", "Main"] ARG 이미지 빌드할 떄 환경변수를 정의하여 사용할 수 있다 1234ARG classpath=.ENV CLASSPATH=$&#123;classpath&#125;CMD ["java", "Main"] 외부에서 환경변수를 전달 받을수도 있다 --build-arg를 통해 인자를 전달한다 1$ docker image build --build-arg classpath=/workapce/javatest -t javatest:latest . 이미지 빌드시에만 사용할 수 있다 컨테이너 생성시에 클래스패스를 바꾸는 것은 불가능하다 참고 : 야마다 아키노리, 『도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문』, 심효섭 옮김, 위키북스(2019)]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문</tag>
        <tag>docker image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[docker] 도커(docker)란?]]></title>
    <url>%2Fdocker%2F%EB%8F%84%EC%BB%A4-docker-%EB%9E%80%2F</url>
    <content type="text"><![CDATA[기존의 가상화 기술 기존에 우리가 알고 있는 가상화는 하이퍼바이저(VMM)라는 기술을 이용하여 구현된 것이다 하이퍼바이저는 호스트 컴퓨터에서 다수의 운영체제를 동시에 실행하기 위한 논리적 플랫폼을 말한다 하이퍼바이저를 이용한 가상화 방식에는 크게 2가지가 있다 Virtual Machine Type1(native) 호스트 OS 없이 하이퍼바이저가 하드웨어 바로 위에서 직접 동작하는 방식이다 중간에 OS가 없으므로 하드웨어에서 가상화 기술을 지원해줘야 하는 단점이 있지만, 요즘 CPU들은 대부분 가상화를 기본으로 지원하기 떄문에 그다지 단점이 되진 않는다 type1에서도 구현방식이 또 2가지로 나뉜다 전가상화 하드웨어 전체를 가상화한 뒤 가상머신을 올리는 방법을 말한다 모든 가상머신의 요청은 항상 하이퍼바이저를 통해서 가게 된다 반가상화 하드웨어 전체를 다 가상화하진 않는 방식을 말한다 특정 요청은 하드웨어에 직접 요청가능하고, 특정 요청은 하이퍼바이저를 통해서만 가능하다(CPU, 메모리 할당 등) 이러한 특성 때문에 OS를 조금 수정해줘야하는 단점이 있지만, 속도면에서는 당연히 더 빠르다는 장점이 있다 Virtual Machine Type2(hosted) 호스트 운영체제 위에서 하이퍼바이저가 동작하는 방식이다 우리가 잘 아는 VMWare, VirtualBox 등이 여기 속한다 당연히 속도는 더 느리다 컨테이너 가상화 기술? 우리가 원하는건 특정 환경에 종속되지 않은 상태로 어플리케이션을 띄우는 것이다 하지만 말 그대로, 단순히 어플리케이션만을 띄우고 싶을 뿐인데 OS까지 띄우는것은 엄청난 낭비이다 하지만 이러한 요구를 만족시키기 위해선 어플리케이션 격리를 해결해야 한다 필요한 것 : 격리된 CPU, 메모리, 디스크, 네트워크를 가진 공간을 만들고 이 공간에서 프로세스를 실행해서 유저에게 서비스 Cgroup, namespace 위의 필요한 부분을 달성하기 위해 사용한 대표적인 2가지 기술이다(리눅스 기술이다) Croup는 CPU, 시스템 메모리, 네트워크 대역폭과 같은 자원을 시스템에서 실행 중인 프로세스에 할당할 수 있는 기술을 말한다 처음에는 process containers 라는 이름으로 구글에서 개발되었으나, 이후 Cgroup 으로 이름이 변경되었다 개발 이후 리눅스 메인 커널에 하나의 기능으로 들어갔다 namespace는 리소스들을 서로 다른 네임스페이스 그룹으로 나눠서, 서로 다른 네임스페이스에서는 볼 수 없다록 하는 기술이다 네임스페이스를 구성하는 방식에는 pid, uts, network 등등 총 6가지의 방법이 있다(?) 결과적으로 이 Cgroup와 namespace를 사용하여 컨테이너라는 기술이 탄생하게 된다 Cgroup을 이용하여 프로세스에 리소스를 할당 namespace를 이용하여 리소스간 볼 수 없게하여 격리시킴 이 Cgroups, namespaces를 표준으로 정의해둔 것을 OCI 스펙이라고 하고, 도커에서 사용했던 LXC나 현재 사용하는 runC는 모두 이 스펙을 구현한 구현체이다 도커란? 컨테이너 기반의 오픈소스 가상화 플랫폼을 말한다 앞서 언급했던 컨테이너 가상화 기술을 사용하는 것은 물론이고, 그것을 관리하기 위한 명령어들, 이미지 버전관리, 도커 레지스트리 등등의 많은 기능을 제공한다 이곳에 잘 설명되어 있다 https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html 제공하는 기능 외에 도커의 주요 장점을 간단히 살펴보면 아래와 같다 코드로 관리하는 인프라와 불변 인프라 알다시피 애플리케이션 배포는 항상 인프라의 가변성을 포함하고 있어서 문제가 많다 (어플리케이션이 동일하더라도 배포하는 서버의 환경에 따라 여러가지 문제를 가져올 수 있다) 이러한 문제를 해결하는 가장 좋은 방법은 애플리케이션이 의존하는 환경의 차이를 가능한 한 배제하는 것 이다 도커는 특정 시점의 서버 상태를 복제하는 기능(docker image)과 이를 코드로 관리할 수 있는 기능(Dockerfile)을 제공한다 그리고 애플리케이션과 환경을 같이 묶어서 빌드할 수 있게 함으로써 기존에 존재하던 환경의 차이를 최소한으로 줄여버렸다 이로인해 높은 이식성 또한 갖추게 되었다 빌드된 이미지는 도커가 설치된 머신이라면 어디서든 실행할 수 있기 때문이다 구성관리의 용이함 일정규모를 넘는 시스템은 보통 여러개의 어플리케이션과 미들웨어를 조합하는 형태로 구성된다 도커를 사용한다고해도 이 부분은 여전히 어려운 문제이다 하지만 도커는 이 문제를 해결했다 설정파일을 사용하여 컨테이너 간 의존 관계와 시작 순서롤 제어할 수 있는 docker compose를 개발했고, 대규모 트래픽에 맞춰 어플리케이션을 오케스트레이션 해주는 docker swarm 을 개발했다 그리고 최근에는 끝판왕으로 등장한 구글의 쿠버네티스가 있다 이렇듯 도커는, 도커를 편리하게 사용할 수 있게 해주는 주변 도구가 잘 갖춰져 있다는 장점도 가지고 있다 이러한 장점으로 도커는 굉장히 빠른 속도로 전파되어갔고, 사실상 표준(de fecto)가 되어 새로운 개발스타일을 많이 만들어내게 되었다 도커 설치 및 실행 앞서 언급한 Cgroups, namespaces는 리눅스 기술이기 때문에 mac이나 Windows에서 바로 사용할 수 없다 그러므로 LinuxKit 이라는 경량 시스템을 실행하고, 그 위에서 컨테이너를 실행하는 구조로 동작한다 물론 직접 설치할 필요는 없고 mac, windows 용 도커 설치하고 실행 시 자동으로 같이 실행된다 설치 및 실행법은 좋은 글이 있어서 아래의 글로 대체한다 https://subicura.com/2017/01/19/docker-guide-for-beginners-2.html 참고 : 야마다 아키노리, 『도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문』, 심효섭 옮김, 위키북스(2019) https://www.joinc.co.kr/w/man/12/docker/InfrastructureForDocker/about https://tech.ssut.me/what-even-is-a-container/]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>도커/쿠버네티스를 활용한 컨테이너 개발 실전 입문</tag>
        <tag>도커</tag>
        <tag>가상화</tag>
        <tag>컨테이너 가상화</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[aws] VPC, subnet]]></title>
    <url>%2Faws%2FVPC-subnet%2F</url>
    <content type="text"><![CDATA[VPC 우리가 공유기 깔고 네트워크 구성하듯이, VPC는 AWS 내에서 논리적 가상 네트워크를 구성할 수 있는 기능을 말한다 EC2 인스턴스, RDS 인스턴스 같은 리소스들은 전부 VPC 내에 등록하게 되며, VPC에 ACL 적용, Security Group 적용 등 여러가지 보안 정책을 추가할 수 있다 참고로 VPC 메뉴를 가면 이미 VPC가 하나 생성되어 있는것을 볼 수 있는데, 이는 AWS 계정 생성시 자동으로 생성되는 default VPC이다 172.31.0.0/16 대역을 사용하며 각 리전마다 1개씩 생성되어 있다(VPC는 리전을 기준으로 생성할 수 있다) default VPC는 리전마다 1개만 설정할 수 있다 VPC 메뉴로 가서 Create VPC 버튼을 눌러 간단하게 등록할 수 있다 나만의 작은 IDC(혹은 공유기 환경) 생성한다고 생각하면 편하다 VPC 이름과 IP 대역(CIDR 형식)을 입력해줘서 간단히 생성할 수 있다 CIDR 형식 서브넷 마스크를 2진수로 바꿨을 때 1의 개수를 나타낸다 e.g. 255.255.255.0 == 24 192.168.0.0 IP 대역을 사용하고, 255.255.255.0 의 서브넷마스크를 사용한다면 192.168.0.0/24와 같이 표기한다 IP 대역에는 가상 네트워크를 구성할 것이기 IP 대역을 입력한다 RFC 1918에 정의된 사설 IP 대역을 입력해주면 된다(10.0.0.0 ~ 10.255.255.255, 172.16.0.0 ~ 172.31.255.255, 192.168.0.0 ~ 192.168.255.255) AWS에서는 이처럼 RFC 1918에 정의된 사설 IP 대역을 사용하는 것을 권장한다고 하는데, 공인 IP 대역을 입력할 수 있기나 한가? 공인 IP 대역을 사용하면 같은 공인 IP를 만났을 때 통신이 불가능하지 않는가? 만약 192.168.0.0/16의 형태로 입력했다고 하면 해당 VPC는 192.168.0.1 ~ 192.168.255.254 까지의 IP 대역을 가지게 되는 것이다 만약 이 IP 영역내에 AWS에서 사용하는 예약 IP가 있다면 이는 제외된다 서브넷 아쉽게도 VPC에서 네트워크 대역을 설정했다고 해서 바로 그 대역을 사용할 수 있는것이 아니다 생성된 VPC 대역 내에서 실제로 사용할 대역을 추가로 설정하고, 어떤 AZ에 위치시킬지 설정하는 서브넷이라는 것을 하위에 추가로 만들어줘야 한다 서브넷은 가용영역을 기준으로 생성한다 아래는 AWS 계정 생성 시 자동으로 생성되는 default VPC 안에 들어있는 default 서브넷 목록이다 default VPC의 IP 대역대는 172.31.0.0/16 이었고(172.31.0.0 ~ 172.31.255.255), 이를 다시 3개로 쪼개서(CIDR 20) 각각 다른 AZ에 할당한 것을 볼 수 있다 VPC 생성시에 172.31.0.0/20 으로 생성하면 자동으로 3개로 나눠질텐데, 굳이 이렇게 하는 이유는 AZ를 설정하기 위함일까? 그렇다면 위에서 CIDR 값을 받을 필요가 없지않나? 이제 AWS 리소스를 생성할 때 이렇게 생성된 VPC와 서브넷 대역을 지정해주면 영역 내에서의 IP가 private IP로 리소스에 박히게 된다 첫번째 서브넷을 선택하면 172.31.0.1 ~ 172.31.15.255 사이의 IP가 private IP로 박히게 된다 리소스 자체에 public IP를 박을 수 있는데, 이는 어떻게 처리되는 것인지? VPC에서 NAT를 수행해서 통신 가능하게끔 해주는 것인지? 게이트웨이, 라우팅테이블을 읽어보면 이 부분이 해소될 것 같다 추가로 ACL, SG 설정하는 부분도 봐야할 듯 https://bcho.tistory.com/779 private, public subnet을 나누는 기준이 뭔가? 외부 통신 기준인가? 통신은 어떻게 하는가? gateway에 ip가 많은 이유는 동적할당을 위해서? VPC 의 CIDR 값은 서브넷 값을 더 크게 설정하지 못하기 위한 일종의 fence 같은 것 AWS 계정 전용 가상 네트워크, AZ가 달라도 논리적으로 묶어서 VPC 형성 가능 다른 가상 네트워크들과 논리적으로 분리 서브넷은 VPC IP 주소 범위. AWS 리소스 생성시에는 VPC가 필요함 AWS 리소스 보호를 위해 ACL 등등을 서브넷에 설정할 수 있다 기본 VPC에는 인터넷 게이트웨이가 포함된다 퍼블릭 IP를 가진 인스턴스가 있는 서브넷의 경우 퍼블릭 서브넷이라고 부른다 라우터는 각각 서브넷 앞에 있어야하지… 않는가? 그러므로 VPN 연결되는 서브넷을 VPN 전용 서브넷이라고 하는것은 이해가 간다 퍼블릭 IP주소는 직접 지정해줘야하고, 프라이빗 IP 주소는 항상 있음 기본적인 공유기 환경에서 NAT 를 타고 인터넷에 접속하는것과는 다른 구조 VPC 내의 라우터는 VPC 내 서브넷간의 통신만을 담당한다 여기 인터넷 게이트웨이를 붙임으로써 외부 통신이 가능해진다 인터넷 게이트웨이는 외부와 private IP를 이어주는 역할인 것 같다 외부와 통신하려면 무조건 인터넷 게이트웨이를 연결해야한다 그러므로 VPC 내의 모든 인스턴스가 외부 통신을 가능하게 하려면 NAT 디바이스를 사용하고, 이를 인터넷 게이트웨이와 연결시켜 줘야한다 인터넷 게이트웨이는 private IP, public IP의 mapping 정도를 해주는 애라서 가장 바깥쪽(VPC)에 있다 서브넷은 각각 라우터를 가지는데, 여기에 NAT를 붙이고 다시 이 NAT가 인터넷 게이트웨이와 붙어서 서브넷 내의 모든 리소스들의 외부 통신이 가능해진다 VPC 생성하면 Route Table, ACL, SG가 기본으로 생긴다 기본세팅으로 설정되어 있음 라우트테이블의 경우 local 이 들어가있는데, 이는 서브넷간의 리소스들은 모두 통신할 수 있음을 말함 Route Table VPC 내의 서브넷간의 통신? 모든 서브넷은 생성시 private 서브넷으로 생성되고, 기본적으로 다른 서브넷간의 연결을 제한함 인터넷 게이트웨이를 만드려면 무조건 라우트테이블을 생성하고 등록해줘야한다 여기서 추가설정으로 서브넷간 통신이나 인터넷 게이트웨이 등을 연결할 수 있다]]></content>
      <categories>
        <category>aws</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>VPC</tag>
        <tag>subnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[aws] region, az]]></title>
    <url>%2Faws%2Fregion-az%2F</url>
    <content type="text"><![CDATA[region AWS의 모든 서비스(리소스)가 위치하고 있는 물리적인 장소 정확히 물리적인 장소는 아니고 조금은 추상적 개념 네트워크 기술이 아무리 발전했더라도, 물리적으로 먼 거리는 시간이 많이 걸릴 수 밖에 없음 경유하는 라우터의 개수가 많기 떄문 어떻게든 물리적인 거리를 좁히는게 최적의 방법임 리전을 바꾸고 테스트해보면 속도차이를 확실히 느낄 수 있음 https://dezang.github.io/aws-ping-latency-check/ 전 세계 주요 지역에 위치하고 있음 https://aws.amazon.com/ko/about-aws/global-infrastructure/ 서비스가 크다면 자연재해 등을 대비하여 여러 리전을 사용하는것을 권장함 seoul 리전이 맛탱이 갔을때 쿠팡, 배민 등의 서비스가 모두 중지되었었다 영업 손실이 보통 수준이 아니었을 것이다 리전간 리소스는 공유되지 않음 리전간 리소스 복사는 가능함 계정당 액세스 할 수 있는 리전이 정해져있다? 한국은 여러곳에 열려있다? e.g. 중국은 몇몇 지역 접근 못함 각자가 사용할 수 있는 리전과 가용영역이 있다 가용영역(AZ, Avaliability Zone) 실질적으로 IDC 센터가 위치하는 장소 리전은 여러개의 AZ를 가짐 Seoul 리전이라고 해서 서울에 IDC가 있는것이 아님 수도이기 때문에 그렇게 표기한 것임 가용영역의 위치가 실제 IDC가 있는 위치임 Seoul 리전의 경우 3개의 AZ가 있는데, 이는 한국에 AWS IDC가 3군데 있음을 뜻함 가용영역의 위치는 비공개임 물리적 피해를 막기 위함 가용영역간 fail over를 elastic IP로 해결할 수 있다? 리소스가 리전의 가용 영역에 걸쳐 배포될 수 있도록 AWS는 각 AWS 계정의 이름에 가용 영역을 독립적으로 매핑합니다. 예를 들어 AWS 계정의 us-east-1a 가용 영역은 다른 AWS 계정에 대한 us-east-1a 가용 영역과 위치가 동일하지 않을 수 있습니다 1a가 특정 위치를 명시하는게 아닐 수 있음을 말하는 듯? AZ ID를 설정하면 특정 위치를 아예 지정할 수 있다 이번 AZ 추가가 테라폼 설정에 왜 문제가 되었을까? AZ간 통신은 가능한가? 리전간 통신은 가능한가? AZ간 VPC는 묶지만, 리전간은 안된다? 리전간 failover는 가능한가? 리전간 스위칭이 가능한가?(seoul -&gt; tokyo) 최종 도메인에 연결되는 AWS 주소를 바꿔주면 될 듯 하다 데이터베이스 동기화는 어떻게하는가? AZ가 망가지는 경우? 리전이 망가지는 경우? 컨테이너 4개를 띄운다고 하면 2개는 2a에 2개는 2c에? 가용영역에 영향받을 수 있는 리소스들은 가용영역을 선택하지 않는다 아마도 모든 가용영역끼리 복사되어있지 않을까? e.g. Elastic Beanstalk 등 beanstalk 은 가용영역이 없고 ec2 를 가용영역을 선택해서 띄운다 loadbalancer에서 az 범위 선택하고 그 범위 내에서 로드밸런싱]]></content>
      <categories>
        <category>aws</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[매개변수 제거]]></title>
    <url>%2Frefactoring%2F%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-%EC%A0%9C%EA%B1%B0%2F</url>
    <content type="text"><![CDATA[메서드가 어떤 매개변수를 더 이상 사용하지 않을 땐 그 매개변수를 삭제하자 동기 프로그래머는 매개변수 추가는 부담없이 하면서도 매개변수 삭제는 꺼린다 껍데기뿐인 매개변수를 놔둔다고 문제가 생기진 않을테고, 혹시 나중에 다시 필요할수도 있다는 자기합리화 때문이다 매개변수는 필요한 정보를 나타낸다 호출 부분에서 무슨 값을 전달할지 신중해야 한다 매개변수를 제거하지 않으면 그 메서드를 사용하는 모든곳이 불필요한 추가 작업을 수행하게 된다 이는 매우 혹독한 대가이고, 매개변수 제거가 어렵지 않다라는 사실을 생각하면 더욱 혹독한 대가이다 방법 메서드 시그니쳐가 상위클래스나 하위클래스에 선언되어 있는지 검사한다 재정의 된 곳에서 해당 매개변수를 사용하지 않는지 확인해봐야 한다 메서드 추가나 메서드 변경보다 좀 더 신중해야한다 매개변수를 제거한 새 메서드를 선언하고, 원본 메서드의 내용을 복사한다 원본 메서드에서 새 메서드를 호출하도록 수정한다 참조하는 부분이 별로 없다면 생략 가능하다 원본 메서드 호출 부분을 전부 찾아서 새 메서드 호출로 바꾼다 원본 메서드를 삭제한다 원본 메서드의 모든 호출 부분에 접근할 수 없다면 @Deprecated로 표시한다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>remove parameter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[ddd] 리포지터리와 모델구현(JPA)]]></title>
    <url>%2Fddd%2F%EB%A6%AC%ED%8F%AC%EC%A7%80%ED%84%B0%EB%A6%AC%EC%99%80-%EB%AA%A8%EB%8D%B8%EA%B5%AC%ED%98%84-JPA%2F</url>
    <content type="text"><![CDATA[데이터 보관소로 RDBMS를 사용할 때 객체 기반의 도메인 모델과 관계형 데이터 모델간의 매핑을 처리하는 기술로 ORM 만한 것이 없다 리파지토리 구현 모듈 위치 리포지터리 인터페이스는 애그리거트와 같이 도메인 영역에 속하고, 리포지터리를 구현한 클래스는 인프라스트럭쳐 영역에 속한다 리포지터리 구현 클래스를 domain.impl 같은 패키지에 위치시키는 것은 좋은 설계 방법이 아니다 기본 기능 구현 리포지터리의 기본 기능은 다음 2가지 이다 아이디로 애그리거트 조회 애그리거트 저장 이를 제공하는 인터페이스의 형식은 다음과 같다 1234interface OrderRepository &#123; public Order findById(OrderNo no); public void save(Order order);&#125; 인터페이스는 애그리거트 루트를 기준으로 작성한다 애그리거트를 조회하는 기능의 이름을 지을 때 널리 사용되는 규칙은 findBy프로퍼티(프로퍼티 값) 의 형식을 사용하는 것이다 e.g. findById, findByName, findByOrdererId ID가 아닌 값으로 조회할때는 JPQL을 이용한다 조회에 해당하는 애그리거트가 존재하면 애그리거트 도메인(e.g. Order)를 반환하고, 존재하지 않는다면 null이나 Optional을 반환한다 1건이상 존재하면 List 컬렉션을 반환한다 JPA는 트랜잭션 범위에서 변경한 데이터를 자동으로 DB에 반영하므로 따로 수정 메서드를 추가할 필요없다 삭제 기능을 구현한다면 삭제할 애그리거트 객체를 파라미터로 전달받게끔 한다 soft delete로 구현하는 것이 좋다 모델(매핑) 구현 기본 매핑 애그리거트 루트는 엔티티이므로 @Entity로 매핑한다 1234@Entityclass Order &#123; // ...&#125; 벨류는 @Embeddable로 매핑한다 1234@Embeddableclass Orderer &#123; // ...&#125; 밸류를 사용하는 곳에서는 @Embedded로 매핑한다 12345@Entityclass Order &#123; @Embedded private Orderer orderer;&#125; 벨류가 다른 벨류를 포함할 수 있다 12345@Embeddableclass Orderer &#123; @Embedded private MemberId memberId;&#125; 데이터베이스 컬럼명이 다를 경우 @AttributeOverride(s) 어노테이션을 사용한다 12345678910@Embeddableclass ShippigInfo &#123; @Embedded @AttributeOverrides(&#123; @AttributeOverride(name = "zipcode", column = @Column(name="shipping_zipcode")), @AttributeOverride(name = "address1", column = @Column(name="shipping_address1")), @AttributeOverride(name = "address2", column = @Column(name="shipping_address2")), &#125;) private Address address;&#125; 기본 생성자 생성자는 기본적으로 객체를 생성할 때 필요한 것을 받는 용도로 사용되는 것이다 벨류 오브젝트의 경우 생성 시점에 모든 값을 전달받고, setter를 제공하지 않으므로, 기본 생성자가 필요없다 하지만 JPA의 @Entity나 @Embeddable을 사용하려면 기본 생성자를 제공해야한다 기존 객체를 상속한 프록시 객체를 사용하여 지연로딩 기능을 구현하기 때문이다 이러한 이유 때문에 기본 생성자를 추가해줘야하는데, 알다시피 기본 생성자를 추가하면 객체가 온전하지 못한 상태로 제공될 수 있게된다 그러므로 기본 생성자를 protected로 선언해서 상속한 프록시 객체에서만 사용할 수 있게 해야한다 필드 접근 방식 사용 JPA는 기본적으로 필드와 메서드(프로퍼티)의 두가지 방식으로 매핑을 처리할 수 있다 필드 1234567@Entity@Access(AccessType.FIELD)class Order &#123; @Column(name = "state") @Enumberated(EnumType.STRING) private OrderState state;&#125; 메서드(프로퍼티) 12345678910111213@Entity@Access(AccessType.PROPERTY)class Order &#123; @Column(name = "state") @Enumerated(EnumType.STRING) public OrderState getState() &#123; return state; &#125; public void setState(OrderState state) &#123; this.state = state; &#125;&#125; 하이버네이트는 @Access가 없으면 @Id나 @EmbeddedId를 보고 접근 방식을 결정한다 공개 getter/setter를 추가하는 메서드(프로퍼티) 방식을 사용하게 되면 도메인의 의도가 사라지고, 객체가 아닌 데이터 기반으로 엔티티를 구현할 가능성이 높아지게 된다 그러므로 가능하다면 필드 방식을 사용해서 객체가 제공할 기능 중심으로 구현하게끔 해야한다 AttributeConverter를 이용한 벨류 매핑 개발하다보면 가끔 벨류 프로퍼티 하나를 한 개 컬럼에 매핑해야 할 때가 있다 12345678/** * 이 벨류 오브젝트를 DB 컬럼 "WIDTH VARCHAR(20)" 에 매핑 * e.g. 1000mm **/class Length &#123; private int value; private String unit;&#125; 이럴땐 JPA 2.1 이후로 추가된 AttributeConverter를 이용하면 된다 12345678910111213141516interface AttributeConverter&lt;X, Y&gt; &#123; // X = 벨류 타입, Y = DB 타입 public Y convertToDatabaseColumn(X attribute); // 벨류 타입 -&gt; DB 컬럼 값 public Y convertToEntityAttribute(Y dbData); // DB 컬럼 값 -&gt; 벨류 타입&#125;class MoneyConverter implements AttributeConverter&lt;Money, Integer&gt; &#123; @Override public Integer converToDatabaseColumn(Money money) &#123; return money.getValue(); &#125; @Override public Money convertToEntityAttribute(Integer value) &#123; return new Money(value); &#125;&#125; 작성한 컨버터를 적용시키려면 아래 2가지 방법을 사용하면 된다 (컨버터를 사용했기 때문에 @Embedded를 사용하지 않고 @Column으로 직접 매핑한다) 특정 시점에만 적용 12345class Order &#123; @Column(name = "totalAmounts") @Converter(converter = MoneyConverter.class) private Money totalAmounts;&#125; 리포지터리로 Order 를 handling 할 때 적용된다 모든 벨류 오브젝트에 적용 1234@Converter(autoApply = true)class MoneyConverter implements AttributeConverter&lt;Money, Integer&gt; &#123; // ...&#125; 모든 Money 타입 프로퍼티에 자동 적용된다]]></content>
      <categories>
        <category>ddd</category>
      </categories>
      <tags>
        <tag>DDD start!</tag>
        <tag>리포지터리</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[메서드명 변경]]></title>
    <url>%2Frefactoring%2F%EB%A9%94%EC%84%9C%EB%93%9C%EB%AA%85-%EB%B3%80%EA%B2%BD%2F</url>
    <content type="text"><![CDATA[메서드명을 봐도 기능을 알 수 없을땐 메서드를 직관적인 이름으로 바꾸자 동기 복잡한 메서드를 잘게 쪼개는 것은 중요하지만, 이를 잘못 적용하면 오히려 그 작은 메서드들의 역할을 파악하기 힘들어질 수도 있다 이러한 문제를 방지하려면 메서드명을 잘 지어야 한다 메서드명만 봐도 그 메서드의 의도를 한눈에 알 수 있어야 한다 메서드명이 적절치 않다면 반드시 변경해야 한다 코드는 컴퓨터보다 인간이 알아보기 쉽게 작성해야 한다 이름을 잘 짓게 되는 것이야말로 진정으로 노련한 프로그래머가 되는 열쇠다 매개변수를 재정렬해서 코드를 알아보기 쉬워진다면 매개변수 재정렬을 실시해야한다 방법 메서드 시그니쳐가 상위클래스나 하위클래스에 선언되어 있는지 검사한다 구현되어 있다면 이 과정을 모든 구현부마다 실행한다 새 이름으로 메서드를 선언하고, 원본 코드를 복사한 뒤 적절히 수정한다 새 메서드를 호출하게 원본 메서드의 내용을 수정한다 원본 메서드 호출 부분을 전부 찾아서 새 메서드 호출로 바꾼다 원본 메서드를 삭제한다 원본 메서드의 모든 호출 부분에 접근할 수 없다면 @Deprecated로 표시한다 요즘 IDE에서는 2~5번이 필요없이 매우 간단히 수행할 수 있다 인텔리제이 짱짱 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>rename method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[매개변수 추가]]></title>
    <url>%2Frefactoring%2F%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-%EC%B6%94%EA%B0%80%2F</url>
    <content type="text"><![CDATA[메서드가 자신을 호출한 부분의 정보를 더 많이 알아야 할 땐 그 정보를 전달할 수 있는 매개변수를 추가한다 동기 사실 이 리팩토링은 자주 실행되면 안된다 매개변수를 추가하는 대신 다른 방법을 사용할 수 있을 때도 많고, 그럴 땐 가능하면 그 대안을 사용하는 것이 낫다 매개변수를 추가하면 매개변수 세트가 더 길어지고, 기억하거나 알아보기 힘들어진다 대안이 없다면 실행해도 된다 기존 매개변수 세트를 살펴보면서 새 매개변수를 추가하면 어떻게 될지 생각해봐야 한다 방법 메서드 시그니쳐가 상위클래스나 하위클래스에 선언되어 있는지 검사한다 구현되어 있다면 이 과정을 모든 구현부마다 실행한다 추가한 매개변수를 전달받는 새 메서드를 선언하고, 원본 메서드의 내용을 복사한다 새 메서드를 호출하게 원본 메서드의 내용을 수정한다 12345678public int originMethod(int a, int b) &#123; return newMethod(a, b, 0);&#125;public int newMethod(int a, int b, int c) &#123; // do somtehing return someVar;&#125; 새 매개변수가 객체일 경우는 보통 null을 전달한다 primitive 타입일 경우는 명백히 이상한 값을 전달해서 참조하는 부분을 찾아낸다(?) int일 경우 보통 0을 전달한다 참조하는 부분이 별로 없다면 생략 가능하다 원본 메서드 호출 부분을 전부 찾아서 새 메서드 호출로 바꾼다 원본 메서드를 삭제한다 원본 메서드의 모든 호출 부분에 접근할 수 없다면 @Deprecated로 표시한다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>add parameter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[매개변수 세트를 객체로 전환]]></title>
    <url>%2Frefactoring%2F%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-%EC%84%B8%ED%8A%B8%EB%A5%BC-%EA%B0%9D%EC%B2%B4%EB%A1%9C-%EC%A0%84%ED%99%98%2F</url>
    <content type="text"><![CDATA[참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[ddd] 애그리거트]]></title>
    <url>%2Fddd%2F%EC%95%A0%EA%B7%B8%EB%A6%AC%EA%B1%B0%ED%8A%B8%2F</url>
    <content type="text"><![CDATA[애그리거트란? 도메인 모델이 복잡해졌을 때, 개별 객체 수준에서 모델을 바라보면 전반적인 구조나 큰 수준에서 도메인간의 관계를 이해하기 어려워진다 주요 도메인 개념간의 관계를 파악하기 어렵다는 것은 코드를 변경하고 확장하는 것이 어려워진다는 것을 의미한다 상위 수준에서 모델이 어떻게 엮여있는지 알아야 전체 모델을 망가뜨리지 않으면서 추가 요구사항을 모델에 반영할 수 있는데, 세부적인 모델만 이해한 상태로는 코드를 수정하기가 두렵기 때문에 코드 변경을 최대한 회피하는 쪽으로 요구사항을 협의하게 된다 즉, 이러한 문제점을 없애려면 상위 수준에서 모델을 조망할 수 있는 방법이 필요하고, 그것이 바로 애그리거트 이다 애그리거트는 관련된 모델을 하나로 모은 것이기 때문에 한 애그리거트에 속한 객체는 유사하거나 동일한 라이프사이클을 갖는다 애그리거트는 경계를 갖는다 한 애그리거트에 속한 객체는 다른 애그리거트에 속하지 않는다 각 애그리거트는 자기 자신만을 관리할 뿐 다른 애그리거트를 관리하지 않는다 경계를 설정할 때 기본이 되는 것은 도메인 규칙과 요구사항이다 도메인 규칙에 따라 함께 생성되는 구성요소는 한 애그리거트에 속할 가능성이 높다 A가 B를 갖는다는 대부분 한 애그리거트이지만, 무조건 한 애그리거트인 것은 아니다 상품과 리뷰가 좋은 예시이다 대부분의 애그리거트는 한 개의 엔티티 객체만 갖는 경우가 많으며 두 개 이상의 엔티티로 구성되는 애그리거트는 드물게 존재한다 애그리거트 루트 애그리거트는 여러 객체로 구성되기 때문에 한 객체만 상태가 정상이어서는 안된다 도메인 규칙을 지키려면 애그리거트에 속한 모든 객체가 정상 상태를 가져야한다 애그리거트에 속한 모든 객체가 일관된 상태를 유지하려면 애그리거트 전체를 관리할 주체가 필요한데, 이 책임을 지는 것이 바로 애그리거트의 루트 엔티티이다 애그리거트에 속한 모든 엔티티는 루트 엔티티에 직접 또는 간접적으로 속한다 애그리거트 루트의 핵심 역할은 애그리거트의 일관성이 깨지지 않도록 하는 것이다 애그리거트 루트는 애그리거트가 제공해야 할 도메인 기능을 구현한다 애그리거트 루트가 제공하는 메서드는 도메인 규칙에 따라 애그리거트에 속한 객체의 일관성이 깨지지 않도록 구현해야 한다 애그리거트 루트가 아닌 다른 객체가 애그리거트에 속한 객체를 직접 변경하면 안된다 이러면 애그리거트 루트가 강제하는 규칙을 적용할 수 없어 모델의 일관성을 깨는 원인이 된다 12ShippingInfo si = order.getShippingInfo();si.setAddress(newAddress); 이는 도메인 규칙을 무시하고 DB 테이블에서 직접 데이터를 수정하는 것과 같다 그렇다고 이를 막는 로직을 서비스 레이어에서 구현하게 되면, 해당 로직이 여러 서비스 레이어에서 중복될 가능성이 높아진다 습관적으로 작성하는 setter 메서드를 피해야한다 도메인 로직을 도메인 객체가 아닌 응용이나 표현 영역으로 분산되게 만드는 원인이 된다 이렇게 되면 도메인 로직이 한곳에 응집되어 있지 않으므로 코드를 유지보수할때도 시간이 훨씬 많이 들게된다 setter만 넣지 않아도 이런 상황을 대부분 방지할 수 있다 밸류 객체는 불변 타입으로 구현한다 애그리거트 루트의 기능 구현 애그리거트 루트는 다른 객체들을 조합해서 기능을 완성한다 기능 실행을 위임하기도 한다 1234567891011121314151617class Order &#123; private OrderLines orderLines; private Long totalAmounts; public void changeOrderLines(List&lt;OrderLine&gt; newOrderLines) &#123; orderLines.changeOrderLines(newOrderLines); // delegate this.totalAmounts = orderLines.getTotalAmounts(); &#125;&#125;class OrderLines &#123; // first-level collection private List&lt;OrderLine&gt; orderLines; public void changeOrderLines(List&lt;OrderLine&gt; newOrderLines) &#123; this.orderLines = newOrderLines; &#125;&#125; Order에 getOrderLines() 같은 메서드가 제공될 경우, 외부에서 OrderLines의 메서드를 직접 호출하면 도메인 로직이 깨질 우려가 있으므로(totalAmounts가 반영안됨), 아래와 같이 변경해줘야 한다 OrderLines를 불변으로 선언한다(OrderLines의 changeOrderLines에서 새로운 OrderLines 객체 반환) OrderLines내 changeOrderLines의 접근 제한자를 패키지나 protected 범위를 사용한다(같은 애그리거트라 같은 패키지에 속하므로) 트랜잭션 범위 트랜잭션의 범위는 작을수록 좋다 한 트랜잭션에서 한 애그리거트만 수정하는 것을 권장한다 아래와 같은 경우에는 한 트랜잭션에서 두 개 이상의 애그리거트를 수정하는 것을 고려해볼 수 있다 도메인 이벤트와 비동기를 사용할 수 없을 경우(보통 두 애그리거트 수정이 필요하면 이 방식을 이용한다) UI 구현의 편리(한 화면에서 여러 상태를 변경하고 싶을 때) 한 트랜잭션에서 두 개 이상의 애그리거트를 수정해야 한다면 애그리거트에서 다른 애그리거트를 직접 수정하지 말고 응용 서비스에서 두 애그리거트를 수정하도록 구현해야한다 1234567891011121314151617181920212223// bad@Transactionalpublic void doSomething(boolean someFlag) &#123; // 응용 계층 Order order = orderRepository.findOne(id); order.doSomething();&#125;public void doSomething(boolean someFlag) &#123; // 도메인 계층 if(someFlag) &#123; order.getProduct().doSomething(); // 다른 애그리거트의 도메인 로직을 수행함 &#125;&#125;// good@Transactionalpublic void doSomething(OrderId id, boolean someFlag) &#123; Order order = orderRepository.findOne(id); order.doSomething(); if(someFlag) &#123; order.getProduct().doSomething(); // 응용 계층에서 다른 도메인 로직을 수행함 &#125;&#125; 리포지터리와 애그리거트 애그리거트는 개념상 완전한 한 개의 도메인 모델을 표현하므로, 객체의 영속성을 처리하는 리포지터리는 애그리거트 단위로 존재한다 별도 DB 테이블에 저장한다고 해서 리포지터리를 따로 만들지 않는다 Order가 애그리거트 루트이고, OrderLine은 애그리거트 구성요소이므로 리포지터리는 Order만 존재한다 애그리거트는 개념적으로 하나이므로 리포지터리는 애그리거트 전체를 저장소에 영속화해야 한다 12345// 애그리거트 전체 영속화orderRepository.save(order);// 완전한 애그리거트 제공Order order = orderRepository.findById(orderId); 애그리거트 전체(OrderLine 등 포함)를 영속화해야 한다 완전한 order 애그리거트를 제공해야 한다 == OrderLine 등을 전부 제공해야 한다 완전한 애그리거트가 아니라면 NullPointerException 같은 문제가 발생한다(OrderLine이 비어있는둥) 애그리거트를 영속화하는 저장소로 무엇을 사용하든지간에 애그리거트의 상태가 변경되면 모든 변경을 원자적으로 저장소에 반영해야한다 애그리거트간 참조 애그리거트의 관리 주체는 애그리거트 루트이므로, 애그리거트를 참조한다는 것은 애그리거트 루트를 참조한다는 것과 같다 12345678class Orderer &#123; // Order 애그리거트 구성요소 private Member member; private String name;&#125;class Member &#123; // 다른 애그리거트의 루트&#125; 이런식으로 직접 다른 애그리거트 루트를 참조하는 것이 편리하긴하나, 다음과 같은 문제점을 야기한다 편한 탐색의 오용 한 애그리거트 내부에서 다른 애그리거트에 접근할 수 있는 편리함 때문에 다른 애그리거트를 수정하고자 하는 유혹에 빠지기 쉽다 성능에 대한 고민 JPA를 사용할 경우 즉시로딩을 사용해야할지 지연로딩을 사용해야할지 고민해야 한다 확장 어려움 트래픽이 증가하여 도메인별로 시스템을 분리했을때(다른 저장소 사용, 다른 기술 사용 등) 문제가 된다 분리된 시스템에서 몽고DB를 사용할 경우 JPA 같은 단일 기술을 사용할 수 없다 ID를 이용하여 다른 애그리거트를 참조하면 위의 문제점을 완화할 수 있다 123456789101112class Order &#123; private Orderer orderer; // 객체 참조&#125;class Orderer &#123; private MemberId memberId; // ID 참조 private String name;&#125;class Member &#123; private MemberId memberId;&#125; 애그리거트끼리는 ID 참조로 연결되고, 애그리거트에 속한 객체들끼리는 객체 참조로 연결된다 애그리거트간 경계를 명확히 하고 애그리거트간 물리적 연결을 제거하기 때문에 모델의 복잡도를 낮춰준다 애그리거트간 의존이 제거되므로 응집도가 높아지는 효과도 있다 이제 다른 애그리거트가 필요하면 응용 서비스에서 아이디를 이용해서 로딩하면 된다 12345678910@Transactionalpublic void doSomething(OrderId id, boolean someFlag) &#123; Order order = orderRepository.findOne(id); order.doSomething(); if(someFlag) &#123; Member member = memberRepository.findById(order.getOrderer().getMemberId()); member.doSomething(); &#125;&#125; 이렇게 됨으로써 한 애그리거트에서 다른 애그리거트를 수정하는 문제를 원천적으로 방지할 수 있고 즉시 로딩을 할지 지연 로딩을 할 지 걱정하지 않아도 되며 애그리거트별로 다른 구현 기술을 사용하는 것도 가능해진다 ID를 이용한 참조와 조회 성능 // TODO 이 부분은 보완이 필요하다 동기는 이해했으나 구현을 이해하지 못하겠다 View는 어느 영역이라고 봐야하는가? 애그리거트간 집합 연관 1:N 연관 개념적으로는 한쪽 애그리거트에 컬렉션으로 연관을 만든다 123class Category &#123; private Set&lt;Product&gt; products;&#125; 근데 이런식으로 1:N을 구현에 반영하는 것이 요구사항을 충족하는 것과 상관없는 경우가 종종 있다 위의 경우만 해도, 카테고리내의 상품들을 보여주는 경우 보통 페이징이 들어가게 된다 즉, 아래와 같이 구현된다는 것을 의미한다 12345678class Category &#123; private Set&lt;Product&gt; products; public List&lt;Product&gt; getProducts(int page, int size) &#123; List&lt;Product&gt; sortedProducts = sortById(products); return sortedProducts.subList((page - 1) * size, page * size); &#125;&#125; 이렇게 구현할 경우 Category 내의 모든 Product를 들고와서 필터하는 것이 되므로, 성능상 심각한 문제가 발생한다 그러므로 보통 이렇게 구현하는 경우는 드물고, 상품 입장에서 자신이 속한 카테고리를 N:1로 연관지어 구한다 123class Product &#123; private CategoryId categoryId;&#125; 그리고 응용서비스에서 이를 이용하여 Product 목록을 구한다 1234567class ProductService &#123; public Page&lt;Product&gt; getProductOfCategory(Long categoryId, int page, int size) &#123; Category category = categoryRepository.findById(new CategoryId(categoryId)); return productRepository.findByCategoryId(category.getId(), PageRequest.of(page, size)); // spring data jpa 기능 사용 &#125;&#125; 연관은 필요하지만 성능때문에 리포지터리에서 구현했다고 생각하면 될까? N:M 연관 개념적으로는 양쪽 애그리거트에 컬렉션으로 연관을 만든다 하지만 이것도 1:N 관계처럼 요구사항을 고려한 뒤 이 구현을 포함시킬지 말지를 결정해야 한다 보통 특정 카테고리에 속한 상품 목록을 보여줄 때 목록 화면에서 각 상품이 속한 모든 카테고리를 표시하지 않고 (==카테고리에서 상품으로의 집합 연관은 필요없다) 상품 상세에서 제품이 속한 모든 카테고리를 보여준다 (==상품에서 카테고리로의 집합 연관은 필요하다) 이부분 또한 성능때문에 카테고리에서 상품으로의 연관은 고려하지 않는다 즉, 카테고리로의 단방향 N:M 연관만 적용하면 된다 1234567891011class Product &#123; @EmbeddedId private ProductId id; @ElementCollection @CollectionTable( name = "product_category", joinColumns = @JoinColumn(name = "product_id") ) private Set&lt;CategoryId&gt; categoryIds; // ID 참조&#125; 근데 이렇게하면 Product에서 Category를 조회하는(상품 상세) 부분에서는 N+1이 일어나지 않는가? Product 목록을 구해오는 리포지터리 부분만 N:M에 맞게 변경해주면 된다 나는 Spring Data Jpa를 썼다고 가정하므로 메서드명만 수정하였다 1234567class ProductService &#123; public Page&lt;Product&gt; getProductOfCategory(Long categoryId, int page, int size) &#123; Category category = categoryRepository.findById(new CategoryId(categoryId)); return productRepository.findByCategoryIdsIn(category.getId(), PageRequest.of(page, size)); // spring data jpa 기능 사용 &#125;&#125; 애그리거트를 팩토리로 사용하기 정지된 상점이 아닐 경우 상품을 추가해주는 로직이다 1234567891011121314class ProductService &#123; public ProductId registerNewProduct(NewProductRequest req) &#123; Store store = storeRepository.findById(req.getStoreId()); if(store.isBlocked()) &#123; throw new IllegalStateException(); &#125; Product product = new Product(store.getId(), /** 속성들 **/); productRepository.save(product); return product.getId(); &#125;&#125; 다시보면 도메인 기능이 응용 서비스에 노출되어 있음을 알 수 있다 상점이 상품을 생성할 수 있는지 여부를 판단하고 상품을 생성하는 것은 논리적으로 하나의 도메인 기능이기 때문이다 이 기능을 구현하기에 더 좋은 장소는 Store 애그리거트이다 1234567891011121314151617181920class Store &#123; public Product createProduct(/** 속성들 **/) &#123; if(isBlocked()) &#123; throw new IllegalStateException(); &#125; return new Product(/** 속성들 **/); &#125;&#125;class ProductService &#123; public ProductId registerNewProduct(NewProductRequest req) &#123; Store store = storeRepository.findById(req.getStoreId()); Product product = store.createProduct(/** 속성들 **/); productRepository.save(product); return product.getId(); &#125;&#125; 이로써 Product 생성 가능 여부를 확인하는 도메인 로직을 변경해도 도메인 영역의 Store만 변경하면 되고 응용 서비스는 영향을 받지 않게 된다 == 도메인의 응집도가 높아졌다 애그리거트를 팩토리로 사용할 때 얻을 수 있는 장점이다 이처럼 애그리거트가 갖고 있는 데이터를 이용해서 다른 애그리거트를 생성해야 한다면 애그리거트에 팩토리 메서드를 구현하는 것을 고려해보는 것이 좋다 Product의 경우 Store의 식별자와 속성들을 필요로 하는데, Store에서 이를 팩토리 메서드로 구현함으로써 필요한 데이터의 일부를 직접 제공하면서 중요한 도메인 로직을 구현할 수 있었다 참고 : 최범균, 『DDD Start!』, 지앤선(2016)]]></content>
      <categories>
        <category>ddd</category>
      </categories>
      <tags>
        <tag>DDD start!</tag>
        <tag>리포지터리</tag>
        <tag>애그리거트 루트</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[메서드 이동]]></title>
    <url>%2Frefactoring%2F%EB%A9%94%EC%84%9C%EB%93%9C-%EC%9D%B4%EB%8F%99%2F</url>
    <content type="text"><![CDATA[메서드가 자신이 속한 클래스보다 다른 클래스의 기능을 더 많이 이용할 땐 그 메서드가 제일 많이 이용하는 클래스 안에서 비슷한 내용의 새 메서드를 작성하자 기존 메서드는 간단히 대리(위임) 메서드로 전환하든지 아예 삭제하자 동기 클래스에 기능이 너무 많거나 클래스가 다른 클래스와 과하게 연동되어 의존성이 지나칠 경우에는 메서드를 옮기는 것이 좋다 메서드가 자신이 속한 객체보다 다른 객체를 더 많이 참조하는 경우에도 메서드를 옮기는 것이 좋다 옮길만한 메서드가 발견되면 그 메서드를 호출하는 메서드, 그 메서드가 호출하는 메서드, 상속 계층에서 그 메서드를 재정의하는 메서드를 살펴본다 판단이 힘들다면 직감에 따라 판단하고, 나중에 판단을 변경해도 된다 방법 원본 클래스의 원본 메서드에 사용된 모든 기능을 검사해서 그 기능들도 전부 옮겨야할지 판단한다 옮길 메서드에만 사용되는 기능일 경우 그 메서드와 함께 옮겨야한다 원본 클래스의 하위 클래스와 상위 클래스에서 그 메서드에 대한 다른 선언이 있는지 검사하자 오버라이딩 등 그 메서드를 대상 클래스안에 선언하고, 원본 메서드의 코드를 복사하고, 잘 돌아가게끔 수정한다 메서드 이름을 적절히 바꿔도 된다 원본 클래스의 변수를 사용한다면 매개변수로 던져주도록 한다 옮길 원본 클래스내에 원본 클래스의 다른 메서드가 들어있거나 원본 클래스의 변수를 2개 이상 사용한다면, 원본 클래스를 매개변수로 던진다 근데 이 상태면 메서드를 이동하는게 맞는지 다시 한번 생각해봐야한다 예외처리 코드가 들어있다면 예외를 논리적으로 어느 클래스가 처리할 지 정해야한다 원본 클래스에서 옮겨진 대상 클래스 메서드를 참조할 방법을 정한다 원본 메서드를 삭제한다 찾아바꾸기 기능을 사용하면 빠르게 할 수 있다 위임 메서드를 사용한다 참조가 많을땐 이 방법이 더 편하다 예시 아래에서 moveMethod를 B 클래스로 옮겨야 하는 상황이라고 가정한다 123456789101112131415161718class A &#123; private int someVar; private B b; public void moveMethod() &#123; if(b.someCondition()) &#123; // do something someVar; // do something &#125; &#125; public void someMethod() &#123; // do something moveMethod(); // do something &#125;&#125; moveMethod를 B 클래스로 옮기고 적절히 수정한다 123456789class B &#123; public void moveMethod(int someVar) &#123; if(b.someCondition()) &#123; // do something someVar; // do something &#125; &#125;&#125; 여기서 적절한 수정이란, moveMethod를 B 클래스로 옮겼을 떄 여전히 필요한 A 클래스의 기능에 대한 것이다 여기선 A 클래스의 인스턴스 변수로 있었던 someVar가 된다 이럴경우 원본 클래스(A 클래스)의 기능을 사용하려면 아래 4가지 중 하나를 실시하면 된다 그 기능을 대상 클래스로 옮긴다 대상 클래스에서 원본 클래스로의 참조를 생성하여 사용한다 원본 객체를 대상 객체 메서드의 매개변수로 전달한다 그 기능이 변수라면 변수를 매개변수로 전달한다 지금은 그 기능이 변수였기 때문에 4번째 방법을 사용했다 만약 A 클래스의 다른 메서드를 사용해야하거나 참조하는 인스턴스 변수가 2개 이상이라면 클래스 자체를 매개변수로 전달해야한다 이제 A 클래스에서 원본 메서드를 삭제하거나 위임하는 방식으로 변경한다 123456789101112131415161718192021222324252627class A &#123; private int someVar; private B b; public void moveMethod() &#123; b.moveMethod(someVar); &#125; public void someMethod() &#123; // do something moveMethod(); // do something &#125;&#125;// 또는 class A &#123; private int someVar; private B b; public void someMethod() &#123; // do something b.moveMethod(someVar); // do something &#125;&#125; 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>move method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] AOP]]></title>
    <url>%2Fspring%2FAOP%2F</url>
    <content type="text"><![CDATA[메서드 전반적으로 공통기능을 넣고싶다 특정 메서드들에 공통된 기능을 넣고 싶을 경우(로깅, 트랜잭션 등등)가 있다 해당 기능들을 직접 메서드에 넣으면 클래스의 관심사가 1개 이상이 되는 문제가 있으니 이를 분리해야 한다 https://jojoldu.tistory.com/69?category=635883 상속 적용 비즈니스 로직을 추상메서드로 가지는 부모클래스를 생성함 추상클래스 앞 뒤로 before(), after() 같은 메서드들을 실행함 하위 클래스에서 추상 메서드를 오버라이드 한다 단일 책임 원칙을 지킬 수 있게 됨 괜찮은 방법이긴 하지만 상속의 고질적인 문제점을 그대로 가져가게 됨 https://jojoldu.tistory.com/70?category=635883 데코레이터 패턴 적용 인터페이스를 하나 만듦(e.g. UserService) UserService를 구현하고, 비즈니스 로직을 구현하는 구현체를 만듦(e.g. UserServiceImpl) UserService를 구현하고, 공통 기능을 위한 구현체를 만듦(e.g. UserPerformenceServiceImpl) 3번 객체의 인자로 2번 객체를 가짐 UserPerformanceServiceImpl에서 UserServiceImpl을 DI 받음 UserService로 DI받으면 UserPerformanceImpl이 injection 됨 UserController -&gt; UserService(UserPerformanceServiceImpl -&gt; UserServiceImpl) 의 형태로 수행됨 이후 여러가지 방식이 시도됨 AOP란? https://jojoldu.tistory.com/71?category=635883 AOP = Aspect Oriented Programming(관점 지향 프로그래밍) 애플리케이션 전체에 걸쳐 사용되는 기능을 재사용하도록 지원하는 것 부가기능(로깅, 트랜잭션)이라는 관점에서 어플리케이션을 바라보고, 횡단 관심사를 잘라냄(크로스 컷팅) 그리고 이를 모듈화하여 재사용하고자 함 AOP 용어 타겟 부가기능을 부여할 대상 애스펙트 부가기능 모듈을 뜻함 어드바이스 + 포인트컷 어드바이스 실질적인 부가기능을 가지고 있는 모듈 무엇을, 언제를 포함하고 있음 특정 오브젝트에 종속되지 않음 포인트컷 부가기능(어드바이스)가 적용될 대상을 선정하는 룰 조인포인트 어드바이스가 적용될 위치 스프링은 기본적으로 메서드 조인포인트만 지원함(프록시 방식이기 떄문) 다른 조인포인트를 사용하고 싶다면 AspectJ 같은 것을 써야함(byte instrument?) 위빙 타겟에 애스펙트가 적용되어 프록시 객체가 생성되는 과정 적용 123456789101112131415161718@Aspectpublic class Performance &#123; @Around("execution(* com.blogcode.board.BoardService.getBoards(..))") public Object calculatePerformanceTime(ProceedingJoinPoint proceedingJoinPoint) &#123; Object result = null; try &#123; long start = System.currentTimeMillis(); result = proceedingJoinPoint.proceed(); long end = System.currentTimeMillis(); System.out.println("수행 시간 : "+ (end - start)); &#125; catch (Throwable throwable) &#123; System.out.println("exception! "); &#125; return result; &#125;&#125; 스프링 부트에서는 몇가지 설정을 추가한 후 이렇게 애스펙트를 정의하면 간단히 적용가능하다 어드바이스 @Before (이전) @After (이후) @AfterReturning (정상적 반환 이후) @AfterThrowing (예외 발생 이후) @Around (메소드 실행 전후) procceed()를 꼭 호출해줘야함 포인트컷 종류는 위 글 마지막 부분 참조 expression 사용법은 아래와 같다 1execution([접근제한자] 리턴타입 [클래스타입(패키지포함).]메서드이름(타입 | &quot;..&quot;, ...) [throws 예외]) 메서드의 풀 시그니처를 문자열로 비교한다고 보면 된다. getMethod를 통해 메서드 풀 시그니쳐를 뽑아보면 아래와 같다. 1public int springbook.learningtest.spring.pointcut.Target.minus(int,int) throws java.lang.RuntimeException 확장 https://jojoldu.tistory.com/72?category=635883 @PointCut 어노테이션을 이용해서 어노테이션을 메서드로 정의하고, 재사용할 수 있음]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[linux] 파일 날짜 변경]]></title>
    <url>%2Flinux%2F%ED%8C%8C%EC%9D%BC-%EB%82%A0%EC%A7%9C-%EB%B3%80%EA%B2%BD%2F</url>
    <content type="text"><![CDATA[https://webdir.tistory.com/151 현재시간 : touch -c 지정시간 : touch -t YYYYMMDDhhmm 동기화 : touch -r oldfile newfile]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[필드 자체 캡슐화]]></title>
    <url>%2Frefactoring%2F%ED%95%84%EB%93%9C-%EC%9E%90%EC%B2%B4-%EC%BA%A1%EC%8A%90%ED%99%94%2F</url>
    <content type="text"><![CDATA[필드에 직접 접근하던 중 그 필드로의 결합에 문제가 생길 땐, 그 필드용 읽기/쓰기 메서드를 작성해서 두 메서드를 통해서만 필드에 접근하게 만들자 동기 변수 직접 접근파 VS 변수 간접 접근파 직접 접근파 : 변수가 정의되어 있는 클래스 안에서는 변수에 자유롭게 접근할 수 있어야 한다 간접 접근파 : 클래스 안에서라도 반드시 접근 메서드를 통해서만 접근 가능해야 한다 직접 접근의 장점 코드를 알아보기 쉽다 간접 접근의 장점 하위 클래스에서 해당 속성을 가져오는 방식을 재정의 할 수 있다(getter override) 속성 초기화를 사용 시점으로 미룰 수 있다 처음에는 직접 접근을 사용하다가 이상한 점이 생길 때 간접 접근으로 바꾸는게 좋다 상위 클래스의 변수를 하위 클래스에서 계산된 값으로 재정의 해야한다거나 사용하는 시점에 속성을 초기화하고 싶다거나 등등등 방법 필드의 읽기 메서드와 쓰기 메서드를 작성한다 필드를 참조하는 부분을 전부 찾아서 읽기 메서드와 쓰기 메서드로 고친다 필드를 private으로 만든다 예시 1234567891011121314151617class SomeClass &#123; private int a; private int b; private int c; public void doSomething() &#123; setC(getA() + getB()); &#125; public int returnSomething(boolean flag) &#123; if(flag) &#123; return getA() + getB(); &#125; return getC(); &#125;&#125; 필드 자체 캡슐화를 할 때는 생성자 안에 쓰기 메서드를 사용하는 것을 주의해야한다 대체로 쓰기 메서드는 객체가 생성된후에 사용하므로, 쓰기 메서드에 초기화 시점과 다른 기능이 추가됐을 수 있다고 전제하는 경우가 많기 때문이다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>self encapsulate field</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[ddd] 아키텍쳐]]></title>
    <url>%2Fddd%2F%EC%95%84%ED%82%A4%ED%85%8D%EC%B3%90%2F</url>
    <content type="text"><![CDATA[4개의 영역 아키텍쳐를 설계할 때 출현하는 전형적인 영역은 아래와 같다 표현 HTTP 요청을 응용 영역이 필요로 하는 형식으로 변환해서 응용 영역에 전달하고, 응용 영역의 응답을 HTTP 응답으로 변환해서 전송한다 e.g. 요청 파라미터를 객체로 받고 결과를 JSON으로 리턴 응용 시스템이 사용자에게 제공해야 할 기능을 구현한다 응용 영역은 기능을 구현하기 위해 도메인 영역의 도메인 모델을 사용한다 응용 서비스는 로직을 직접 수행하기보다는 도메인 모델에 로직 수행을 위임한다 도메인 도메인의 핵심 로직을 구현한다 e.g. 주문 도메인의 경우 ‘배송지 변경’, ‘결제 완료’ 같은 핵심 로직을 도메인 모델에서 구현한다 인프라스트럭쳐 구현 기술에 대한 것을 다룬다 e.g. RDBMS 연동, 몽고 DB, 메시지 큐 전송 등 계층 구조 아키텍쳐 1234567 표현 ↓ 응용 ↓ 도메인 ↓인프라스트럭쳐 계층 구조는 상위 계층에서 하위 계층으로의 의존만 존재하고 하위 계층에서 상위 계층에 의존하지는 않는다 계층 구조를 엄격하게 적용하면 상위 계층은 바로 아래 계층에만 의존을 가져야하지만, 구현의 편리함을 위해 계층 구조를 유연하게 적용한다 이 말인 즉, 표현, 응용, 도메인 계층이 상세한 구현 기술을 다루는 인프라스트럭쳐에 의존할 수 있다는 점이다 예를 들면 아래처럼 될 수 있다 12345678910111213141516171819class CalculateDiscountService &#123; private DroolsRuleEngine ruleEngine = new DroolsRuleEngine(); public Money calculateDiscount(List&lt;OrderLine&gt; orderLines, String customerId) &#123; Customer customer = findCustomer(customerId); // 초기 돈 MutableMoney money = new MutableMoney(0); // 조건들 추가하고 List&lt;?&gt; facts = Arrays.asList(customer, money); facts.addAll(orderLines); // DroolsRulsEngine을 이용해 할인율 적용 ruleEngine.evaluate("discountCalculation", facts); return money.toImmutableMoney(); &#125;&#125; 위 처럼 도메인에 메시지를 보내는 것 외에 특정 엔진을 사용해야하는 상황이다 (특정 엔진을 사용하는 것이 더 나은 상황) 하지만 위 코드는 아래 2가지 문제점을 가지고 있다 테스트하기 어렵다 DroolsRuleEngine이 완벽하게 동작해야만 CalculateDiscountService를 테스트할 수 있다 구현 방식을 변경하기 어렵다 DroolsRuleEngine이 아니라 다른 엔진을 사용하도록 변경하고자 한다면 많은 부분이 변경되어야 할 것이다 고수준 모둘이 제대로 동작하려면 저수준 모듈을 사용해야 하는데, 인프라스트럭쳐의 경우 특정 기술을 직접 구현하므로 이런 문제점이 발생하게 된다. 이를 어떻게 처리할 수 있을까? DIP 정답은 저수준 모델이 고수준 모델에 의존하도록 바꾸는 것이다 다시 한번 CalculateDiscountService를 살펴보면, discount를 얻는데 어떤 엔진을 사용했느냐는 중요하지 않다 단지 고객정보와 구매정보에 룰을 적용해서 할인 금액을 구한다는 것이 중요할 뿐이다 이 부분을 추상화해서 인터페이스로 만들 수 있다 123interface RuleDiscounter &#123; public Money applyRules(Customer customer, List&lt;OrderLine&gt; orderLines);&#125; 이 인터페이스를 사용하여 CalculateDiscountService에서 DroolsRuleEngine을 제거할 수 있다 12345678910111213141516class CalculateDiscountService &#123; private RuleDiscounter ruleDiscounter; public CalculateDiscountService(RuleDiscounter ruleDiscounter) &#123; this.ruleDiscounter = ruleDiscounter; &#125; public Money calculateDiscount(List&lt;OrderLine&gt; orderLines, String customerId) &#123; Customer customer = findCustomer(customerId); return ruleDiscounter.applyRules(customer, orderLines); &#125;&#125;class DroolsRuleDiscounter implements RuleDiscounter &#123; // ...&#125; CalculateDiscountService는 더 이상 구현기술인 Drools(저수준)에 의존하지 않고, 룰을 이용한 할인 금액 계산 을 표현하는 RuleDiscounter 인터페이스(고수준)에 의존한다 그림에서 보이다시피 고수준 모듈이 저수준 모듈을 사용함에도 불구하고 저수준 모듈이 고수준 모듈에 의존하고 있다 이를 DIP(Dependency Inversion Principle, 의존 역전 원칙) 이라고 부른다 그리고 인터페이스를 구현한 저수준 모듈은 외부에서 생성해 주입(Dependency Injection) 해주게 된다 이와 같이 DIP를 적용함으로써 기존의 고수준 모듈에서 저수준 모듈 사용에서 오던 문제점들을 해결할 수 있게 된다 테스트하기 쉬워진다 특정 클래스가 아니라 인터페이스에 의존하므로 mockito를 사용한 stub 등을 사용한다면 직접 구현체를 구현하지 않고도 테스트를 진행할 수 있게 된다 구현 방식을 변경하기 숴워진다 저수준 모듈에 강하게 결합되어 있는 구조가 아니기 때문에, 구현 방식을 변경하고 싶다면 인터페이스를 구현한 구현체를 하나 더 만들어서 DI 해주면 CalculateDiscountService의 코드변경 없이 구현 방식을 변경할 수 있다(OCP) DIP 주의사항 DIP 결과 구조만 보고 인터페이스를 잘못 추출한 결과이다 RuleEngine은 고수준 모델인 도메인 관점이 아니라 엔진이라는 저수준 모듈 관점에서 도출된 것이다 즉, 여전히 고수준 모듈이 저수준 모듈에 의존하고 있는 셈이다 DIP를 적용할 때 하위 기능을 추상화한 인터페이스는 고수준 모듈 관점에서 도출해야 한다 도메인 영역의 주요 구성요소 엔티티 고유의 식별자를 가지고 자신의 라이프 사이클을 갖는 객체 데이터와 데이터와 관련된 기능을 함께 제공한다 벨류 고유의 식별자를 갖지 않고 주로 도메인 객체의 속성을 표현할 떄 사용되는 객체 다른 벨류 타입의 속성으로도 사용될 수 있다 애그리거트 관련된 엔티티와 벨류 객체를 개념적으로 하나로 묶은 것 리포지터리 도메인 모델의 영속성을 처리함 도메인 서비스 특정 엔티티에 속하지 않은 도메인 로직을 제공함 도메인 로직이 여러 엔티티와 벨류를 필요로 할 경우 여기에서 로직을 구현한다 엔티티와 벨류 도메인 모델의 엔티티와 DB 모델의 엔티티는 다르다 도메인 모델의 엔티티는 단순히 데이터를 담고 있는 데이터 구조라기보다는 데이터와 함께 기능을 제공하는 객체이다 도메인 관점에서 기능을 구현하고 기능 구현을 캡슐화해서 데이터가 임의로 변경되는 것을 막는다 도메인 모델의 엔티티는 두 개 이상의 데이터가 개념적으로 하나인 경우 벨류 타입을 이용해서 표현할 수 있다 RDBMS는 벨류를 제대로 표현하기 힘들다 ORDER_NAME, ORDER_EMAIL 필드로 표시하거나 또는 ORDER_ORDERER 테이블로 표시하더라도 딱히 벨류 타입의 느낌을 주지 못한다 애그리거트 도메인이 커질수록 개발할 도메인 모델도 커지게 되고, 많은 엔티티와 벨류가 생기면서 모델이 점점 더 복잡해진다 이렇게 도메인 모델이 복잡해지면 개발자가 전체 구조가 아닌 한개 엔티티와 벨류에 집중하게 되는 경우가 발생한다 지도를 볼 때 매우 상세하게 나온 대축적 지도를 보면 큰 수준에서 어디에 위치하고 있는지 이해하기 어려우므로 큰 수준에서 보여주는 소축적 지도를 함꼐 봐야 현재 위치를 보다 더 정확하게 이해할 수 있다 이와 비슷하게 도메인 모델도 개별 객체가 아니라 상위 수준에서 모델을 볼 수 있어야 전체 모델과 개별 모델을 이해하는데 도움이 된다 이게 바로 애그리거트(AGGREGATE) 이다 주문 애그리거트는 주문, 주문자, 배송정보 등을 포함한다 애그리거트는 군집에 속한 객체들을 관리하는 루트 엔티티를 가진다 루트 엔티티는 애그리거트에 속해 있는 엔티티와 벨류 객체를 이용해서 애그리거트가 구현해야 할 기능을 제공한다 애그리거트를 사용하는 코드는 애그리거트가 제공하는 기능을 실행하고 애그리거트 루트를 통해서 간접적으로 애그리거트 내의 엔티티나 벨류 객체에 접근하게 된다 애그리거트를 구현할 때는 고려할 것이 많다 리포지터리 도메인 객체를 지속적으로 사용하려면 RDBMS 같은 물리적 저장소에 도메인 객체를 보관해야 하고, 이를 위한 도메인 모델이 리포지터리 이다 리포지토리는 애그리거트 단위로 도메인 객체를 조회하고 저장하는 기능을 제공한다 리포지터리는 도메인 객체를 영속화하는데 필요한 기능을 추상화 한 것이기 떄문에 고수준 모듈에 속하고, 실제 구현 클래스는 인프라스트럭쳐 영역에 속한다 리포지터리의 사용 주체는 응용 서비스이다 응용 서비스는 필요한 도메인 객체를 구하거나 저장할 때 리포지터리를 이용한다 그러므로 응용 서비스가 필요로 하는 메서드를 제공한다 기본은 저장과 식별자로 조회 메서드이다 요청 처리 흐름 응용 서비스는 도메인 모델을 이용해서 기능을 구현한다 도메인 객체를 조회해야하거나 새로 생성해야 할 경우 리포지터리를 이용한다 인프라스트럭쳐 표현, 응용, 도메인 영역을 모두 지원하는 영역이다 DIP에서 언급했듯이 인프라스트럭쳐를 직접 사용하는 것 보단 각자의 영역에서 정의한 인터페이스를 인프라스트럭쳐 영역에서 구현하는 것이 시스템을 더 유연하고 테스트하기 쉽게 만들어준다 하지만 무조건 인프라스트럭쳐에 대한 의존을 없애는 것이 좋은 것은 아니다 예를 들면 스프링의 @Transactional이 있다 코드에서 스프링에 대한 의존을 없애려면 복잡한 스프링 설정을 사용해야하는데, 이럴때는 굳이 의존을 없애지 않는 것이 좋다 구현의 편리함 또한 다른 장점들만큼 중요한 부분이기 때문이다 표현 영역 또한 인프라스트럭쳐와 쌍을 이룬다. 그것도 항상. 모듈 구성 기본적으로 우리는 항상 아키텍쳐가 각 영역의 별도 패키지에 위치하는 형태로 구성한다 1234com.bookstore.uicom.bookstore.applicationcom.bookstore.domaincom.bookstore.infrastructure 애그리거트의 모델과 리포지터리는 같은 패키지에 위치시킨다 하지만 패키지 구성 방식에 정답이 있는것은 아니다 도메인이 크면 하위 도메인마다 별도 패키지를 구성할 수 있다 1234567891011com.bookstore.catelog.uicom.bookstore.catelog.applicationcom.bookstore.catelog.domaincom.bookstore.catelog.infrastructurecom.bookstore.order.uicom.bookstore.order.applicationcom.bookstore.order.domaincom.bookstore.order.infrastructure... 만약 카탈로그 도메인이 상품 애그리거트와 카테고리 애그리거트로 구성된다면 domain에서 패키지를 나눌 수 있다 12345com.bookstore.catelog.uicom.bookstore.catelog.applicationcom.bookstore.catelog.domain.productcom.bookstore.catelog.domain.categorycom.bookstore.catelog.infrastructure 만약 도메인 서비스 계층이 따로 있으면 아래와 같이 나눌수도 있다 123com.bookstore.catelog.domain.productcom.bookstore.catelog.domain.categorycom.bookstore.catelog.domain.service service는 product와 category 의 도메인 서비스 계층이다 이러한 룰들은 domain 뿐만이 아니라 다른 계층에도 적용될 수 있다 그리고 알다시피, 모듈 구성에 정답은 없다 하지만 가능하면 한 패키지내에 10개 미만으로 타입 개수를 유지하는 것이 좋다 참고 : 최범균, 『DDD Start!』, 지앤선(2016)]]></content>
      <categories>
        <category>ddd</category>
      </categories>
      <tags>
        <tag>DDD start!</tag>
        <tag>계층 구조</tag>
        <tag>DIP</tag>
        <tag>DI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[필드 캡슐화]]></title>
    <url>%2Frefactoring%2F%ED%95%84%EB%93%9C-%EC%BA%A1%EC%8A%90%ED%99%94%2F</url>
    <content type="text"><![CDATA[public 필드가 있을 땐 그 필드를 private로 만들고 필드용 읽기 메서드와 쓰기 메서드를 작성하자 동기 객체지향의 주요 원칙 중 하나는 캡슐화이다 그러므로 데이터는 절대 public 타입으로 선언되면 안된다 데이터를 public 타입으로 만들면 데이터가 있는 객체가 모르는 사이에 다른 객체가 데이터 값을 읽고 변경할 수 있다 이로 인해 데이터와 기능이 분리된다 데이터와 데이터를 사용하는 기능이 한 곳에 모여있으면 코드를 수정하기 쉽다 방법 캡슐화 할 필드를 위한 읽기 메서드(getter)와 쓰기 메서드(setter)를 작성한다 클래스 외부에서 그 필드를 참조하는 모든 부분을 찾아서 1에서 작성한 메서드로 대체한다 참조하는 부분을 모두 수정했다면 필드를 private으로 변경한다 사실상 getter, setter를 적용하는 것은 public 메서드를 쓸때와 별 다를것이 없고, 문제도 해결되지 않는다 일단 이런식으로 필드 직접 접근을 막은 다음, 불필요한 읽기/쓰기 메서드 호출 부분을 제거해야한다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>encapsulate field</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[필드 이동]]></title>
    <url>%2Frefactoring%2F%ED%95%84%EB%93%9C-%EC%9D%B4%EB%8F%99%2F</url>
    <content type="text"><![CDATA[어떤 필드가 자신이 속한 클래스보다 다른 클래스에서 더 많이 사용될 때는 대상 클래스 안에 새 필드를 선언하고 그 필드 참조 부분을 전부 새 필드 참조로 수정하자 동기 어떤 필드가 자신이 속한 클래스보다 다른 클래스에 있는 메서드에서 더 많이 참조한다면, 그 필드를 옮기는 것을 생각해봐야 한다 만약 클래스 추출을 할 경우 필드 추출이 우선이고, 메서드 추출이 그 다음이다 방법 책의 예제에는 잘 안나와있는데, 난 아래와 같이 이해했다 A 클래스안에서 B 클래스의 field 필드를 자주 이용한다면 B 클래스의 field를 A 클래스로 옮겨버릴 수 있다 12345678910111213141516171819202122232425262728293031class A &#123; private B b; public void someMethod1() &#123; // do something b.getField(); // 이런식으로 자주 사용된다면 // do something &#125; public void someMethod2() &#123; // do something b.getField(); // 이런식으로 자주 사용된다면 // do something &#125; public void someMethod3() &#123; // do something b.getField(); // 이런식으로 자주 사용된다면 // do something &#125;&#125;class B &#123; private int field; // getter/setter public void someMethod() &#123; // do something; field; // do something; &#125;&#125; 하지만 B 클래스의 field 필드도 B 클래스내에서 사용하는 부분이 어느정도 있다(고 가정한다). 이럴땐 필드 이동으로 인한 변경을 최소화 하기 위해 이동 대상 클래스 필드(B의 field)에 필드 자체 캡슐화를 적용한다 1234567891011121314class A &#123; // ...&#125;class B &#123; private int field; // getter/setter public void someMethod() &#123; // do something; getField(); // 필드 자체 캡슐화 // do something; &#125;&#125; A 클래스에 field 필드와 getter/setter를 작성하고, B 클래스를 참조하던 부분을 변경한다 필드 이동 시 getter/setter는 세트로 같이 옮긴다 12345678910111213141516171819202122class A &#123; private int field; // getter/setter public void someMethod1() &#123; // do something field; // do something &#125; public void someMethod2() &#123; // do something field; // do something &#125; public void someMethod3() &#123; // do something field; // do something &#125;&#125; B 클래스에서 A 클래스의 field를 참조할 방법을 정한다 참조하는 부분에서 직접 a.getField(), a.setField() 로 접근하는 방법 필드 자체 캡슐화한 메서드만 변경하는 방법 2번에서 이미 수행했고, 이 방식이 변경을 가장 최소화 할 수 있다 12345678910111213class B &#123; private A a; public int getField() &#123; return a.getField(); &#125; public void someMethod() &#123; // do something; getField(); // 변경 없음 // do something; &#125;&#125; B 클래스에서 field 필드를 삭제한다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>move field</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[클래스 추출]]></title>
    <url>%2Frefactoring%2F%ED%81%B4%EB%9E%98%EC%8A%A4-%EC%B6%94%EC%B6%9C%2F</url>
    <content type="text"><![CDATA[두 클래스가 처리해야 할 기능이 하나의 클래스에 들어 있을 땐(하나의 클래스에서 2가지 일 이상을 수행할 땐) 새 클래스를 만들고 기존 클래스의 관련 필드와 메서드를 새 클래스로 옮기자 동기 클래스는 확실하게 추상화되어야 하며, 두세 가지의 명확한 기능을 담당해야 한다 하지만 개발자는 클래스에 점증적으로 어떤 기능이나 데이터를 추가하기 때문에, 클래스는 시간이 갈수록 방대해지기 마련이다 이런 클래스는 너무 많은 메서드와 데이터가 들어있어 이해하기 힘드므로, 분리할 부분을 궁리해서 떼어내야 한다 데이터의 일부분과 메서드의 일부분이 한 덩어리이거나, 주로 함께 변화하거나 서로 유난히 의존적인 데이터의 일부분일 경우 클래스로 떼어내기 좋다 방법 위의 동기를 참조해서 클래스의 기능 분리 방법을 정한다 분리할 기능을 넣을 새 클래스를 작성한다 기능이 분리됨으로써 원본 클래스의 이름이 변경될 수 있다 원본 클래스에서 새 클래스로의 링크를 만든다 필요할 때 까지 역방향 링크를 만들지 않는다 옮길 필드마다 필드 이동을 적용한다 메서드 이동을 적용한다 각 클래스를 다시 검사해서 인터페이스를 줄인다 양방향 링크를 단방향 링크로 바꿀 수 있는지 등 추출 된 클래스의 공개 범위를 결정한다 공개할 경우 몇가지 예시 아래와 같은 코드가 있다 12345678910class Person &#123; private String name; private String officeAreaCode; private String officeNumber; // getter/setter public String getTelephoneNumber() &#123; return "(" + officeAreaCode + ")" + officeNumber; &#125;&#125; officeAreaCode와 officeNumber를 합쳐서 TelephoneNumber로 분리하기로 한다 필드 이동을 적용하기 전에 필드 자체 캡슐화를 적용한다 12345678910class Person &#123; private String name; private String officeAreaCode; private String officeNumber; // getter/setter public String getTelephoneNumber() &#123; return "(" + getOfficeAreaCode() + ")" + getOfficeNumber(); &#125;&#125; 새로 생성한 클래스에 필드 이동을 적용한다 12345class TelephoneNumber &#123; private String areaCode; private String number; // getter/setter&#125; Person에서 officeAreaCode, officeNumber를 삭제한다 이때 getter는 위임 코드로 전환한다 12345678910111213141516class Person &#123; private TelephoneNumber officeTelephone; private String name; public String getOfficeAreaCode() &#123; return officeTelephone.getAreaCode(); &#125; public String getOfficeNumber() &#123; return officeTelephone.getNumber(); &#125; public String getTelephoneNumber() &#123; return "(" + getOfficeAreaCode() + ")" + getOfficeNumber(); &#125;&#125; getTelephoneNumber()에 메서드 이동을 적용한다 123456789class TelephoneNumber &#123; private String areaCode; private String number; // getter/setter public String getTelephoneNumber() &#123; return "(" + areaCode + ")" + number; &#125;&#125; Person을 정리한다 12345678class Person &#123; private TelephoneNumber officeTelephone; private String name; public String getTelephoneNumber() &#123; return officeTelephone.getTelephoneNumber(); &#125;&#125; 여기서 TelephoneNumber에 대한 공개 여부에 대한 이야기가 나오는데 잘 이해가 가지 않는다 외부에 공개하면 불변 객체로 만들어서 수정이 불가능하게끔 해야하고, 참조 객체로 만드려면 외부에 공개하지 않아야 하는것이 아닐까? 모르겠당… 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>extract class</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DI(Dependency Injection)]]></title>
    <url>%2Fetc%2FDI-Dependency-Injection%2F</url>
    <content type="text"><![CDATA[https://www.slideshare.net/baejjae93/dependency-injection-36867592 OOP는 원래 객체간 메시지 전달을 하는 방식으로 진행되는 프로그래밍 방식아므로, 클래스간 의존은 어쩌피 존재한다. 우리의 목적은 의존을 없애야하는게 아니라 결합을 약하게 해야하는 것이다. 클래스에서 특정 클래스에 직접 의존하는 것이 아니라, 행위(인터페이스)에 의존해야 한다 (클래스에 직접 의존하면 클래스에 대해 더 많이 가정해야 하므로 강한 결합이 형성된다) 행위를 구현한 클래스들은 외부에서 생성해서 주입하도록 한다 근데 매번 이렇게 외부에서 구현체를 생성해서 넣어주는게 귀찮고 불편하므로, 이런걸 제공해주는 프레임워크를 사용하는 것이 좋다 == 스프링 스프링의 최대 장점은 DI이다]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>DI</tag>
        <tag>의존성 주입</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[millisecond, microsecond, nanosecond]]></title>
    <url>%2Fetc%2Fmillisecond-microsecond-nanosecond%2F</url>
    <content type="text"><![CDATA[1 second == 1,000 millisecond 1 millisecond == 1,000 microsecond 1 microsecond == 1,000 nanosecond 1 second == 1,000,000,000 nanosecond]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>millisecond</tag>
        <tag>microsecond</tag>
        <tag>nanosecond</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[알고리즘 전환]]></title>
    <url>%2Frefactoring%2F%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%A0%84%ED%99%98%2F</url>
    <content type="text"><![CDATA[알고리즘을 더 분명한 것으로 교체해야 할 땐 해당 메서드의 내용을 새 알고리즘으로 바꾼다 특징 어떤 기능을 수행하기 위해 비교적 간단한 방법이 있다면, 복잡한 방법을 더 간단한 방법으로 교체해야한다 기본적으로 메서드를 잘게 쪼개놔야 가능하다 길고 복잡한 알고리즘은 수정하기 어렵기 때문이다 방법 테스트가 꼭 필요하다 기존 알고리즘을 간결한 알고리즘으로 바꾸면서 계속해서 테스트를 실행한다 모든 테스트 결과가 같으면 성공이다 다르다면 디버깅을 실시해 비교해본다 기존 알고리즘과 새로운 알고리즘의 출력값을 비교하면서 진행하는 것도 좋은 방법이다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>substitute algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[템플릿 메서드 형성]]></title>
    <url>%2Frefactoring%2F%ED%85%9C%ED%94%8C%EB%A6%BF-%EB%A9%94%EC%84%9C%EB%93%9C-%ED%98%95%EC%84%B1%2F</url>
    <content type="text"><![CDATA[하위클래스 안의 두 메서드가 거의 비슷한 단계들을 같은 순서로 수행할 땐 그 단계들을 시그니처가 같은 두 개의 메서드로 만들어서 두 원본 메서드를 같게 만든 후, 두 메서드를 상위클래스로 옮긴다 동기 하위 클래스에 있는 두 메서드가 비슷하다면 둘을 합쳐서 하나의 상위 클래스로 만드는것이 좋다 두 메서드가 완전히 똑같지 않다면, 중복된 부분은 가능한 한 전부 없애고 차이가 있는 필수 부분만 그대로 둬야한다 방법 비슷해보이는 같은 클래스에 있다면, 두 메서드가 어떤 공통 상위클래스의 하위클래스가 되게 정리해야 한다 123456789class Printer &#123; public String textPrint() &#123; // ... &#125; public String imagePrint() &#123; // ... &#125;&#125; 를 아래의 구조로 변경한다 12345678910111213141516171819class Printer &#123; public String textPrint() &#123; return new TextPrinter().print(); &#125; public String imagePrint() &#123; return new ImagePrinter().print(); &#125;&#125;class TextPrinter extends Printer &#123; public String print() &#123; // ... &#125;&#125;class ImagePrinter extends Printer &#123; public String print() &#123; // ... &#125;&#125; 리팩토링은 작게작게 진행해야하므로, 하위클래스에 위임하는 방식으로 구현했다 메서드상향이 용이하도록 두 하위클래스의 메서드명을 같게했다 두 메서드를 잘게 분리하는데, 공통된 부분과 공통되지 않은 부분이 나오게끔 분리한다 1234567891011121314151617181920class TextPrinter extends Printer &#123; public String print() &#123; // ... printBody(); // ... &#125; public String printBody() &#123; // 다른 부분 &#125;&#125;class ImagePrinter extends Printer &#123; public String print() &#123; // ... printBody(); // ... &#125; public String printBody() &#123; // 다른 부분 &#125;&#125; 이 기법의 핵심이다 하나의 클래스에 대해 메서드 상향을 실시하자 12345678910111213abstract class Printer &#123; public String print() &#123; // ... printBody(); // ... &#125; public abstract String printBody(); // 달랐던 부분&#125;class TextPrinter extends Printer &#123; public String printBody() &#123; // 다른 부분 &#125;&#125; 달랐던 부분은 abstract로 선언하고, 하위클래스에서 구현하도록 한다 위임하던 코드는 삭제한다 나머지 하나의 클래스에도 동일하게 적용한다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>form template method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] git pull request checkout]]></title>
    <url>%2Fgit%2Fgit-pull-request-checkout%2F</url>
    <content type="text"><![CDATA[직접 checkout git config 추가 1$ git config --add remote.origin.fetch "+refs/pull/*/head:refs/remotes/origin/pr/*" fetch로 가져올 때 origin 말고 pr도 가져올 수 있게함 12$ git fetch origin$ git checkout origin/pr/[PR number] intellij 에서 checkout https://blog.jetbrains.com/idea/2018/10/intellij-idea-2018-3-eap-github-pull-requests-and-more/ shift 2번 눌러서 search everywhere 실행시킨 뒤 view pull request 클릭 생성된 pr에서 보고 싶은 pr을 선택하여 local branch 만들고 checkout]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>github PR</tag>
        <tag>pull request</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] travis ci로 github blog 자동 배포]]></title>
    <url>%2Fgit%2Ftravis-ci%EB%A1%9C-github-blog-%EC%9E%90%EB%8F%99-%EB%B0%B0%ED%8F%AC%2F</url>
    <content type="text"><![CDATA[github app에서 travis CI 설치 github setting - developer setting에서 personal access token 발급 .travis_ci, _config.yml 설정 push하고 travis에서 빌드 잘 되는지 확인 theme 테마는 submodule로 저장해줘야하고, 테마에서 _config.yml을 수정할 것이므로 theme fork한것을 submodule에 넣어야함 travis ci app 설치 및 진행 방식 https://okky.kr/article/515352 .travis.yml, _config.yml은 여기를 참조하면 됨 https://medium.com/@changjoopark/travis-ci를-이용한-github-pages-hexo-블로그-자동-배포하기-6a222a2013e6]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>travis ci</tag>
        <tag>hexo travis ci</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] git submodule]]></title>
    <url>%2Fgit%2Fgit-submodule%2F</url>
    <content type="text"><![CDATA[https://www.git-scm.com/book/ko/v1/Git-도구-서브모듈]]></content>
      <categories>
        <category>git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[ddd] 도메인 모델]]></title>
    <url>%2Fddd%2F%EB%8F%84%EB%A9%94%EC%9D%B8-%EB%AA%A8%EB%8D%B8%2F</url>
    <content type="text"><![CDATA[도메인이란? 온라인 서점 시스템을 구현한다고 할 때, 소프트웨어로 해결하고자 하는 문제의 영역인 온라인 서점이 도메인이 된다 한 도메인은 다시 여러개의 하위 도메인으로 나뉠 수 있다 온라인 서점의 하위 도메인 : 상품, 회원, 주문, 정산, 배송 등등 모든 도메인마다 고정된 하위 도메인이 존재하는 것은 아니다. 상황에 따라 달라진다 대상이 기업인지, 사용자인지 등등 특정 도메인을 위한 소프트웨어라고 해서 도메인이 제공해야 할 모든 기능을 구현하는 것은 아니다 외부 업체의 배송 시스템이나, 결제 시스템 같은것들을 이용한다 도메인 모델 특정 도메인을 개념적으로 표현한 것이다 이를 사용하면 여러 관계자들이 동일한 모습으로 도메인을 이해하고, 도메인 지식을 공유하는데 도움이 된다 도메인을 이해하려면 도메인이 제공하는 기능과 주요 데이터 구성을 파악해야 한다 보통은 기능과 데이터를 함꼐 보여주는 클래스 다이어그램이 적합하다 꼭 UML만 사용해야 하는 것은 아니다. 도메인을 이해하는데 도움이 된다면 표현방식이 무엇인지는 중요하지 않다 여러 하위 도메인을 하나의 다이어그램에 모델링하면 안된다 각 하위 도메인마다 별도로 모델을 만들어야 한다 모델의 각 구성요소는 특정 도메인을 한정할 때 비로소 의미가 완전해지기 때문이다 카탈로그의 상품과 배송의 상품은 다르다 도메인 모델 패턴(★★★) 일반적인 어플리케이션의 아키텍쳐는 아래의 4계층으로 구성된다 Presentation(표현) 사용자의 요청을 처리하고 사용자에게 정보를 보여준다 외부 시스템도 사용자가 될 수 있다 Application(응용) 사용자가 요청한 기능을 실행한다 업무 로직을 직접 구현하지 않으며 도메인 계층을 조합해서 기능을 실행한다 Domain(도메인) 시스템이 제공할 도메인의 규칙을 구현한다 Intrastructure(인프라스트럭쳐) 외부 시스템과의 연동을 처리한다(DB, Messaging 등) 여기서 도메인 영역에 해당하는, 도메인의 규칙을 객체지향 기법으로 구현하는 패턴이 도메인 모델 패턴이다 e.g. 주문 상태 변경에 대한 규칙을 처리하는 Order class 1234567891011class Order &#123; // ... public void changeShippingInfo(ShippingInfo newShippingInfo) &#123; if(!isShippingChangeable()) &#123; throw new IllegalStateException(); &#125; // ... &#125;&#125; 주문 상태 변경 로직을 도메인에서 직접 처리하고 있다 이처럼 핵심 규칙을 구현한 코드가 도메인 모델에만 위치하기 때문에 규칙이 바뀌거나 규칙을 확장해야 할 때 다른 코드에 영향을 덜 주고 변경 내역을 모델에 반영할 수 있게 된다 모델 작성시 주의사항 개념 모델을 만들때 처음부터 완벽하게 도메인을 표현하는 모델을 만드는 시도를 할 수 있지만 실제로 이는 불가능에 가깝다 소프트웨어를 개발하는 동안 도메인을 점점 더 이해하게 되기 때문에, 시간이 지나감에 따라 모델을 수정하는 경우가 많기 때문이다 (지식이 쌓이면서 완전히 다른 의미로 해석되어지는 상황도 존재한다) 그러므로 처음에는 개요 수준의 개념 모델을 작성하여 도메인에 대한 전체 윤곽을 이해하는데 집중하고, 구현하는 과정에서 개념 모델을 구현 모델로 점진적으로 발전시켜 나가야한다 도메인 모델 도출 아무리 천재 개발자라도 도메인에 대한 이해없이 코딩을 시작할수는 없다 기획서, 유스 케이스, 유저 스토리 같은 요구사항과 관련자와의 대화를 통해 도메인을 이해하고, 이를 바탕으로 도메인 모델 초안을 만들어야 코드를 작성할 수 있다 도메인을 모델링할 때 기본이 되는 작업은 모델을 구성하는 핵심 구성요소, 규칙, 기능을 찾는 것이다. 이는 모두 요구사항을 분석하면서 찾아낼 수 있다 요구사항을 통해 제공해야 하는 기능을 도출해낼 수 있다 출고 상태로 변경하기, 배송지 정보 변경하기 등 요구사항을 통해 특정 항목이 어떤 데이터로 구성되어야 하는지 알 수 있다 e.g. 한 상품을 한개 이상 주문할 수 있다 요구사항을 통해 각 항목간의 관계를 알 수 있다 e.g. 최소 한 종류 이상의 상품을 주문해야 한다 특정 조건이나 상태에 따라 제약이나 규칙이 달리 적용되는 경우도 많다 e.g. 출고를 하면 배송지 정보를 변경할 수 없다 요구사항을 이해하다보면 설계가 바뀌는 경우가 종종 생긴다 문서화 코드는 상세한 모든 내용을 다루고 있기 때문에 코드를 이용해서 전체 소프트웨어를 분석하려면 많은 시간을 투자해야한다 전반적인 기능 목록이나 모듈 구조는 상위 수준에서 정리된 문서를 참조하는 것이 소프트웨어 전반을 빠르게 이해하는데 도움이 된다 코드 자체도 문서화의 대상이 될 수 있다 도메인 지식이 잘 묻어나고, 도메인을 잘 표현하도록 코드를 작성하면 코드의 가독성이 높아지면서 코드가 문서로서의 의미도 가지게 된다 엔티티와 벨류 모델은 크게 엔티티와 벨류로 구분할 수 있다 이 둘의 차이를 제대로 구분해야 도메인을 올바르게 설계하고 구현할 수 있으므로 차이를 명확하게 이해해야 한다 엔티티 타입 가장 큰 특징은 식별자를 갖는다는 것이다 식별자는 엔티티마다 고유해서 각 엔티티는 서로 다른 식별자를 가진다 엔티티를 생성하고 엔티티의 속성을 바꾸고 엔티티를 삭제할 때 까지 식별자는 유지된다 두 엔티티의 식별자가 같으면 두 엔티티는 같다고 볼 수 있다 equals와 hashCode를 식별자를 기준으로 구현한다 항상 변화하고, 추적 가능해야 한다 식별자 생성 엔티티 식별자 생성은 도메인의 특징과 사용하는 기술에 따라 달라진다 흔히 식별자는 아래 방식으로 생성한다 특정 규칙에 따라 생성 흔히 사용하는 규칙은 현재 시간과 다른 값을 함께 조합하는 것이다 여기서 주의할 점은 같은 시간에 동시에 식별자를 생성할 때 같은 식별자가 만들어지면 안된다는 것이다 UUID 마땅한 규칙이 없을때 사용하면 좋다 대부분의 개발 언어는 UUID 생성기를 제공하고 있다 값 직접 입력 회원 id, email 같은 것이다 직접 입력하는 것이기 때문에 중복되지 않도록 사전에 방지하는 것이 중요하다 일련번호 사용 주로 데이터베이스에서 제공하는 자동 증가 기능을 사용한다 벨류 타입 개념적으로 완전한 하나를 표현할 때 사용한다 우편번호, 주소, 상세주소 는 묶어서 하나의 주소로 표현될 수 있다 보다 명확하게 표현할 수 있게되고, 코드의 가독성도 올라간다 벨류 타입 자체만 봐도 의미를 쉽게 이해할 수 있다 벨류 타입으로 구성된 클래스 또한 쉽게 이해 가능해진다 벨류 타입이 꼭 2개 이상의 데이터를 가져야하는 것은 아니다 의미를 명확하게 하기 위해 int 대신 Money를 쓸 수 있다 밸류 타입을 위한 기능을 추가할 수 있다는 장점도 있다 Address를 사용하면 주소를 위한 기능을 Address 클래스에 추가할 수 있다 두 벨류 객체가 같은지 비교할때는 모든 속성이 같은지 비교해야한다 벨류 타입을 불변으로 선언하기 모든 필드를 final private로 만듦 값의 변경이 있을떄마다 해당 필드의 값을 변경하는 것이 아니라 변경된 값을 가진상태의 새로운 객체를 생성해서 반환 추적성과 별칭 문제에 대해 부담이 없어짐 일반적으로 날짜, 금액 등의 작은 개념을 의미하므로 새로운 객체를 만들어도 오버헤드가 적고, 추적성에도 관심을 가질 필요가 없기 떄문에 굳이 동일 객체를 유지할 필요가 없다 기존 객체는 가비지컬렉션의 대상이 됨 엔티티 식별자에 벨류 타입 사용 엔티티의 식별자는 단순한 문자열이 아니라 도메인에서 특별한 의미를 지니는 경우가 많기 때문에, 식별자를 위한 벨류 타입을 사용해서 의미가 잘 드러나도록 할 수 있다 123class Order &#123; private OrderNo no;&#125; 그냥 String no 라고 썼다면 이게 주문 번호인지 알아보기 힘들었을 것이다 혹은 변수명으로 나타내야 했을텐데, 벨류 타입을 사용하는 것이 좀 더 확실하다 참고로 응용 서비스 계층에서 Order를 조회하는 메서드를 만들때, 파라미터를 OrderNo 대신 String으로 받는다 응용 서비스 계층을 호출하는 프레젠테이션 계층에서 도메인 계층의 요소를 알지 못하게 하기 위함일까? e.g. orderRepository.findById(new OrderNo(orderNo)) set 사용하지 않기 도메인 모델에 무조건 getter/setter를 추가하는 것은 좋지않은 버릇이다 특히 setter는 도메인의 핵심 개념이나 의도를 코드에서 사라지게 한다 changeShippingInfo()는 배송지 정보를 새로 변경한다는 의미를 가지지만, setShippingInfo()는 단순히 배송지 값을 설정한다는 것을 뜻한다 구현할때에도 setter에 추가적 처리를 하기가 애매해진다 습관적으로 setter는 필드값만 변경하고 끝나는 경우가 많았기 때문이다 이러한 특징으로 인해 도메인 지식이 코드에서 사라지게 되는것이다 setter 는 도메인 객체를 생성할 때 완전한 상태가 아닐수도 있게끔 한다 12345678Order order = new Order();order.setOrderLine(lines);order.setShippingInfo(shippingInfo);// ...// 주문자를 설정하지 않은 상태에서 주문 완료 처리가 되어버렸다// 그렇다고 여기다가 주문자 null 체크를 넣는것 또한 이상하다order.setState(OrderState.PREPARING); 도메인 객체가 불완전한 상태로 사용되는것을 막으려면 생성시점에 필요한 것을 모두 전달해줘야 한다 DTO는 괜찮다 도메인 로직을 가지고 있지 않기 때문이다 프레임워크에서 setter 없이도 값을 할당하게끔 해주기도 하는데, 이럴경우 그 기능을 최대한 활용해서 DTO까지 VO의 특징을 가지게 하는것이 좋다 도메인 용어 도메인 용어를 코드에 반영하지 않으면 개발자가 코드의 의미를 해석해야 하는 부담이 생긴다 123public enum OrderState &#123; STEP1, STEP2, STEP3, STEP4 ..&#125; 각각의 STEP이 어느 상태를 나타내는지 누군가에게 물어서 알아내야하고, 그 상태로 매번 변환하는(머리속에서) 과정이 추가되게 된다 그러므로 되도록 도메인 용어를 코드에 직접 사용해서 이런 불필요한 과정을 줄이고, 코드가 바로 이해될 수 있도록 해주는 것이 좋다 참고로 우리는 한국인이라 도메인 용어를 영어로 나타내는데 많은 어려움이 있다 하지만 여기 시간을 투자하지 않는다면 코드가 점점 도메인에서 멀어지게 되니, 도메인 용어에 알맞은 단어를 찾는 시간을 아까워하지 말아야 한다 참고 : 최범균, 『DDD Start!』, 지앤선(2016)]]></content>
      <categories>
        <category>ddd</category>
      </categories>
      <tags>
        <tag>DDD start!</tag>
        <tag>도메인</tag>
        <tag>도메인 모델 패턴</tag>
        <tag>Entity</tag>
        <tag>VO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[메서드 상향]]></title>
    <url>%2Frefactoring%2F%EB%A9%94%EC%84%9C%EB%93%9C-%EC%83%81%ED%96%A5%2F</url>
    <content type="text"><![CDATA[이름이 같은 메서드가 여러 하위클래스에 들어 있을 땐, 그 메서드를 상위 클래스로 옮긴다 특징 다른 리팩토링 단계를 마친 후 적용하는 것이 일반적이다 다른 클래스에 있는 두 개의 메서드가 매개변수로 전환되어 결국 같은 메서드가 될 수도 있다 하위클래스가 상위 클래스 기능을 재정의했음에도 불구하고 기능이 같다면, 바로 실시해야 한다 두 메서드가 똑같진 않고 비슷한 부분이 있다면 템플릿 메서드 형성을 실시하는 방법도 있다 방법 메서드가 서로 같은지 검사한다 거의 비슷한데 똑같진 않다면 알고리즘 전환을 적용해서 메서드를 똑같게 만든다 메서드 시그니처가 서로 다르다면 모든 시그니처를 상위클래스에서 사용하고자 하는 시그니처로 수정한다 상위클래스에 새 메서드를 작성하고, 하위클래스의 메서드를 복사해서 넣은 뒤 적절히 수정하고 컴파일한다 메서드가 하위클래스의 메서드를 사용한다면 상위클래스에 abstract 타입 메서드를 선언한다 메서드가 하위클래스의 필드를 사용한다면 필드 상향이나 필드 자체 캡슐화를 적용한다 하위클래스 메서드를 하나 삭제하고 컴파일과 테스트를 실시한다 4번과 같은 식으로 상위클래스 메서드만 남을때까지 메서드를 계속 삭제한다 메서드 호출하는 부분을 상위클래스로 대체할 수 있는지 파악하고, 수정할 수 있으면 수정한다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>pull up method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VirtualBox에서 공인인증서 인식시키기]]></title>
    <url>%2Fetc%2FVirtualBox%EC%97%90%EC%84%9C-%EA%B3%B5%EC%9D%B8%EC%9D%B8%EC%A6%9D%EC%84%9C-%EC%9D%B8%EC%8B%9D%EC%8B%9C%ED%82%A4%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[VirtualBox에서 공인인증서 인식시킨 방식에 대해 공유하고자 함 VirtualBox 하드디스크에 위치시키는 방법은 안된다 내 VirtualBox는 윈도우 7이었으므로, 인터넷에서 찾아보고 공인인증서를 C://Users/{userId}/AppData/LocalLow/NPKI 로 위치시켰는데도 전혀 인식하지 못했다 C://Program Files/NPKI, C://Program Files(x86)/NPKI 또한 마찬가지였다 VirtualBox라서 경로를 다르게 인식하는 것 같았다 아마도 mac 파일시스템으로 인식했겠지… ~/VirtualBox/cdrive 이런식으로…(왜 이 생각을 못했을까 ㅠㅠ) 어찌됬든 이런 문제때문에 USB로 공인인증서를 불러오게끔 해야한다 VirtualBox에서 usb 인식시키기 https://imitator.kr/Windows/2826 여기 따라서 VirtualBox에서 USB를 인식시키게 하면 된다 https://www.virtualbox.org/wiki/Download_Old_Builds 여기서 자기 VirtualBox 버전에 맞춰 들어간다음, Extension Pack을 선택해서 설치한다 위 블로그대로 수행하면 가상머신안에서도 USB 인식이 가능해지고, 공인인증화면에서 USB로 공인인증서를 찾을 수 있다 망할놈의 공인인증서 _]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>virtualbox NPKI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] TDD로 화폐 개발하기(3)]]></title>
    <url>%2Ftdd%2FTDD%EB%A1%9C-%ED%99%94%ED%8F%90-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0-3%2F</url>
    <content type="text"><![CDATA[다시 한번 작은 테스트로 시작 이제 더하기를 구현해야하는데, 아직까진 $5 + 10CHF = $10에 대한 테스트를 작성하기가 어렵다 그래서 좀 더 작은 단위($5 + $5 = $10)로 줄여서 시작해본다 1234public void testSimpleAddiction() &#123; Money sum = Money.dollar(5).plus(Money.dollar(5)); assertThat(sum).isEqualTo(Money.dollar(10));&#125; 어떻게 구현해야할지 명확하므로 바로 작성해본다 1234// MoneyMoney plus(Money addend) &#123; return new Money(amount + addened.amount, currency);&#125; TDD를 하면서 이런식의 단계조절을 계속해서 배워야한다 지금처럼 구현이 명백히 떠오를때는 조금 성큼성큼 나가도 되고, 사려깊게 고민해야할때는 천천히 나가는 것이 좋다 우리는 다중 통화 사용에 대한 내용을 시스템의 나머지 코드에게 숨겨야하는데(설계상 가장 어려운 제약), 현재의 Money 객체로는 그 행위가 불가능하다 이처럼 사용하는 객체가 우리가 원하는 방식으로 동작하지 않을 경우엔, 그 객체와 외부 프로토콜이 같으면서 내부 구현은 다른 새로운 객체(imposter)를 만들 수 있다 TDD는 적절한때에 번뜩이는 통찰을 보장하지는 못한다(우리가 다 생각해야함) 그렇지만 확신을 주는 테스트와 조심스럽게 정리된 코드를 통해, 통찰에 대한 준비와 함께 통찰이 번뜩일때 그걸 적용할 준비를 할 수 있다 우리는 Money와 비슷하게 동작하지만 사실은 두 Money의 합을 나타내는 imposter를 만들것이다 imposter가 될 수 있는 후보로 생각해본것들은 아래와 같다 지갑 같은 객체. 여러 화폐들이 들어갈 수 있다. (2 + 3) x 5 같은 수식 객체. 2$와 같은 Money가 수식의 가장 작은 단위가 되고, 수식들을 연산한 결과도 수식이 나온다 최종적으로 수식에 환율을 적용하여 단일통화를 얻게된다 (어떻게 이런 생각을 도출해냈는지 잘 떠오르지를 않는다…) 2번을 택하기로 하고, 테스트를 작성해본다 1234public void testSimpleAddiction() &#123; Money reduced = // ... assertThat(reduced).isEqualTo(Money.dollar(10));&#125; reduced는 수식에 환율을 적용하여 나온 단일통화(Money)가 된다 reduced를 얻는 과정을 좀 더 작성하면 아래와 같다 12345678public void testSimpleAddiction() &#123; Money five = Money.dollar(5); Expression sum = five.plus(five); Money reduced = bank.reduce(sum, "USD"); assertThat(reduced).isEqualTo(Money.dollar(10));&#125; 덧셈의 과정으로 수식(Expression)이 나오게되고, 여기에 환율을 적용하여 단일통화를 얻게끔 했다 사실상 현재과정에서 은행 없이 수식에서 reduce를 구현할수도 있지만 그렇게 하지 않은 이유는, Expression이 우리가 하려는 일의 핵심이기 때문에, 다른 부분(환율 적용)에 대해서는 최대한 모르게 하기 위함이다 그렇게 해야 핵심 객체가 가능한 오래 유지되고, 테스트하기 쉽고, 재활용하기 쉬운 상태로 남을 수 있게된다 환율적용 외에도 Expression과 관련있는 오퍼레이션이 많을 수 있기 때문이다 그때마다 모든 오퍼레이션을 Expression에만 추가하면다면 Expression은 무한히 커질 것이다 이제 컴파일 에러를 잡아야한다 먼저 plus 메서드가 Expression을 반환해야 한다 1234// MoneyExpression plus(Money addend) &#123; return new Money(amount + addened.amount, currency);&#125; 클래스로 만들 수 있지만 더 가벼운 인터페이스를 선택한다 123456interface Expression &#123;&#125;class Money implements Expression &#123; // ...&#125; Bank stub을 가볍게 작성해서 테스트를 통과시킨다 12345class Bank &#123; Money reduce(Expression source, String to) &#123; return Money.dollar(10); &#125;&#125; 메타포를 선택하고 빠르게 테스트 작성하고, 그를 통과시키는 과정? 순방향 진행 이제 bank.reduce()에 작성한 가짜 구현을 제거해줘야하는데, 이번 경우는 어떻게 (거꾸로)작업해야 할지가 명확하지가 않다 그래서 이번에는 순방향(?)으로 작업해보기로 한다 먼저 현재 bank.reduce() 메서드는 인자로 넘기는 source와 반환하는 Money의 값이 중복이다. source에 넘겨주는 값과 리턴하는 Money의 값이 사실상 동일한 값이기 때문이다(삼각측량을 이용해서 가짜구현을 제거하더라도 동일하다) 이 시점에서 우리가 Expression을 만들때 생각했던, 구현체인 Sum을 등장시켜보자 Money.plus()가 Money가 아닌 Expression(Sum)을 반환하도록 변경해주도록 하자 테스트 먼저 작성해본다 123456789@Testpublic void testPlusReturnsSum() &#123; Money five = Money.dollar(5); Expression result = five.plus(five); Sum sum = (Sum) result; assertThat(sum.augend).isEqualTo(five); assertThat(sum.addend).isEqualTo(five);&#125; 이 테스트는 너무 구현 종속적이라 오래가지 못할것이다) 이제 정확한 expected/actual 형태가 나오도록 수정해야한다 Money.plus에서 Sum을 반환하도록 수정하고, Sum 클래스를 만들어야한다 1234567891011121314// MoneyExpression plus(Money addend) &#123; return new Sum(this, addend);&#125;public class Sum implements Expression &#123; public Money augend; public Money addend; public Sum(Money augend, Money addend) &#123; this.augend = augend; this.addend = addend; &#125;&#125; 좀 빠른감이 있지만 구현이 명백하게 떠오르니 바로바로 진행한다 Sum을 작성하고 나니 추가적인 테스트가 바로 떠오른다 Sum에 전달한 Money 통화가 모두 동일하고, reduce를 통해 얻어내고자 하는 통화 역시 같다면 결과는 Sum 내의 amount를 합친 값을 갖는 Money 객체여야한다 123456public void testReduceSum() &#123; Expression sum = new Sum(Money.dollar(3), Money.dollar(4)); Bank bank = new Bank(); Money result = bank.reduce(sum, "USD"); assertThat(result).isEqualTo(Money.dollar(7));&#125; 테스트를 통과시킨다 12345public Money reduce(Expression source, String to) &#123; Sum sum = (Sum) source; int amount = sum.augend.amount + sum.addend.amount; return new Money(amount, to);&#125; 이 코드는 현재 2가지 이유로 지저분하다 형변환. reduce()는 모든 Expression에 대해 동작해야 한다 Sum의 public 필드와 sum.augend.amount 같이 2단계에 걸친 레퍼런스 2번 문제는 간단히 고칠 수 있다. 메서드 일부를 Sum 클래스 내부로 옮겨버리면 된다 1234567891011// Sumpublic Money reduce(String to) &#123; int amount = augend.amount + addend.amount; return new Money(amount, to);&#125;// Bankpublic Money reduce(Expression source, String to) &#123; Sum sum = (Sum) source; return sum.reduce(to);&#125; 덧셈은 됐으니 환율 적용에 대해 생각해보자 그냥 Money가 인자로 왔을 경우 환율을 적용시킨 Money를 내보내야 한다 근데 우린 지금 Money 부터 받을수가 없어서, 이를 먼저 통과시켜야 한다 테스트를 바로 작성해보자 123456@Testpublic void testReduceMoney() &#123; Bank bank = new Bank(); Money result = bank.reduce(Money.dollar(1), "USD"); assertThat(result).isEqualTo(Money.dollar(1));&#125; 12345678// Bankpublic Money reduce(Expression source, String to) &#123; if(source instanceof Money) &#123; return (Money) source; &#125; Sum sum = (Sum) source; return sum.reduce(to);&#125; 코드가 너무 지저분해졌다 다른 환율에 대한 테스트를 작성하기 전에, 지저분한 코드들부터 정리하고 가는것이 좋겠다 이런식으로 클래스를 명시적으로 검사하는 코드가 있을떄는 항상 다형성을 적용해주는 것이 좋다 Money에도 reduce()를 구현해준다 1234@Overridepublic Money reduce(String to) &#123; return this;&#125; 이제 Expression을 구현하는 Money, Sum에 reduce() 메서드가 있으니 인터페이스에도 선언할 수 있다 123public interface Expression &#123; Money reduce(String to);&#125; 이로써 불필요한 캐스팅 코드를 모두 제거할 수 있다 123public Money reduce(Expression source, String to) &#123; return source.reduce(to);&#125; 환율 적용! 이제 다른 통화간 환율을 적용하는 테스트를 작성해본다 1234567@Testpublic void testReduceMoneyDifferentCurrency() &#123; Bank bank = new Bank(); bank.addRate("CHF", "USD", 2); Money result = bank.reduce(Money.franc(2), "USD"); assertThat(result).isEqualTo(Money.dollar(1));&#125; Money에서 직접 환율을 관장할수도 있지만, 별로 좋은 방식이 아니다 환율에 관한건 Bank가 처리하게 해야한다 reduce() 하기전에 Bank에 환율 관련된 부분을 물어보게끔 처리하면 될 것 같다 Bank를 인자로 전달하게끔 파라미터를 변경하자 12345public interface Expression &#123; Money reduce(Bank bank, String to);&#125;// Money, Sum 적용 환율을 물어볼 메서드를 작성한다 1234567// Bankpublic int rate(String from, String to) &#123; if(from.equals("CHF") &amp;&amp; to.equals("USD")) &#123; return 2; &#125; return 1;&#125; Money에서 rate()에 환율을 물어본다 12345@Overridepublic Money reduce(Bank bank, String to) &#123; int rate = bank.rate(currency, to); return new Money(amount / rate, to);&#125; 보다시피 아직 좋은 방법이 아니다. 게다가 addRate()로 환율 추가하는 메서드까지 만들어놓고 전혀 활용하지 않고 있다. addRate()로 해시테이블 같은 곳에 환율을 추가하고(환율표), 필요할 때 매번 찾아보게 하면 될 것 같다 해시테이블에서 바로 찾기 위해 환율의 from과 to를 위한 객체를 따로 만든다 그리고 이 Pair 클래스는 키로 사용될 것이므로 equals와 hashCode를 구현해준다 (현재는 리팩토링 과정중이므로 따로 테스트를 작성하지 않는다. 리팩토링이 끝난 후 모든 테스트가 잘 통과한다면 리팩토링이 잘 되었다고 판단할 수 있기 때문이다.) 1234567891011121314151617181920class Pair &#123; String from; String to; public Pair(String from, String to) &#123; this.from = from; this.to = to; &#125; @Override public boolean equals(Object obj) &#123; Pair pair = (Pair) obj; return from.equals(pair.from) &amp;&amp; to.equals(pair.to); &#125; @Override public int hashCode() &#123; return 0; &#125;&#125; 0은 최악의 해시코드지만, 지금은 빠르게 달려야하니까 그냥 저렇게 작성한다 나중에 많은 통화를 다루게 될 경우 추가적으로 수정한다 이제 이 환율표를 사용하도록 Bank를 수정한다 12345678910// BankMap&lt;Pair, Integer&gt; rateTable = new Hashtable&lt;&gt;();public void addRate(String from, String to, int rate) &#123; rateTable.put(new Pair(from, to), rate);&#125;public int rate(String from, String to) &#123; return rateTable.get(new Pair(from, to));&#125; 잘 동작할 줄 알았는데 테스트가 실패한다! 살펴보니 같은 통화일떄가 문제였다. 이렇게 뜻밖지 못하게 발견한 일의 경우 테스트를 추가해서 다른 사람들이 알게끔 해줘야 한다 12345@Testpublic void testIdentityRate() &#123; Bank bank = new Bank(); assertThat(bank.rate("USD", "USD")).isEqualTo(1);&#125; 이렇게 리팩토링하다가 실수한 경우 이 문제를 분리하기 위해 또 다른 테스트를 작성하고, 전진해나간다 이제 rate() 를 수정하자 1234567public int rate(String from, String to) &#123; if(from.equals(to)) &#123; return 1; &#125; return rateTable.get(new Pair(from, to));&#125; 다른 통화간 더하기 드디어 5$ + 10CHF = 10$ 를 테스트 해볼 떄가 왔다 아래가 우리가 최종적으로 원하는 테스트의 모습이다 1234567891011@Testpublic void testMixedAddition() &#123; Expression dollar = Money.dollar(5); Expression franc = Money.franc(10); Bank bank = new Bank(); bank.addRate("CHF", "USD", 2); Money result = bank.reduce(dollar.plus(franc), "USD"); assertThat(result).isEqualTo(Money.dollar(10));&#125; 하지만 안타깝게도 컴파일 에러가 난다 좀 더 천천히 진행해보기로 하고(모든 에러를 컴파일러가 잡아줄것이라는 기대?), 한 단계만 뒤로 물러나보자 먼저 testMixedAddition() 상단의 Expression을 Money로 바꿔서 컴파일 에러를 제거하고, 테스트를 돌려보자 123456@Testpublic void testMixedAddition() &#123; Money dollar = Money.dollar(5); Money franc = Money.franc(10); // ...&#125; 테스트가 실패한다. 10$ 대신 15$가 나오는 것이 축약을 하지 않는 것 처럼 보인다. 123456@Overridepublic Money reduce(Bank bank, String to) &#123; int amount = augend.reduce(bank, to).amount + addend.reduce(bank, to).amount; return new Money(amount, to);&#125; 테스트가 통과했으니, 처음 컴파일 오류에서 봤던 내용을 다시 생각해보자 사실상 모든 Money는 Expression이어야 한다. 이제 이를 조금씩 없애도록 하자. 파급효과를 피하기 위해 가장자리부터 작업해 나가기 시작해서 테스트 케이스까지 거슬러 올라가도록 한다 먼저 Sum 부터 고친다 12345678910111213public class Sum implements Expression &#123; // 1 public Expression augend; public Expression addend; // 2 public Sum(Expression augend, Expression addend) &#123; this.augend = augend; this.addend = addend; &#125; // ...&#125; 인스턴스 변수 타입을 고치고, 파라미터 타입도 바꾼다 이제 Sum을 사용하는 곳에서는 Expression을 받을 수 있다 Money.plus()의 파라미터를 Expression으로 바꾼다. 바꾸는 김에 times()의 반환 타입도 바꾼다 1234567public Expression plus(Expression addend) &#123; return new Sum(this, addend);&#125;public Expression times(int multiplier) &#123; return new Money(amount * multiplier, currency);&#125; 이제 다시 testMixedAddition()의 참조변수들을 바꾼다 123456@Testpublic void testMixedAddition() &#123; Expression dollar = Money.dollar(5); Expression franc = Money.franc(10); // ...&#125; 컴파일러가 Expression에 plus()를 구현해야 한다고 알려주고 있다 컴파일러의 지시대로 따라가자 1234567891011121314// Expressionpublic interface Expression &#123; // ... Expression plus(Expression addend);&#125;// Money// 이미 구현되어 있음// Sum@Overridepublic Expression plus(Expression addend) &#123; return null; // stub&#125; 추상화 Expression.plus()를 끝마치려면 Sum.plus를 구현해야 한다 테스트를 작성한다 12345678910111213@Testpublic void testSumPlusMoney() &#123; Expression dollar = Money.dollar(5); Expression franc = Money.franc(10); Bank bank = new Bank(); bank.addRate("CHF", "USD", 2); Expression sum = new Sum(dollar, franc).plus(dollar); Money result = bank.reduce(sum, "USD"); assertThat(result).isEqualTo(Money.dollar(15));&#125; 테스트가 통과하게끔 작성한다 1234@Overridepublic Expression plus(Expression addend) &#123; return new Sum(this, addend);&#125; Money와 형태가 똑같아져서, 추상클래스로 분리할 수 있을 것 같다 이제 Expression.times를 작성해야 한다 Sum.times를 작성한다면 Expression.times를 선언하는 일은 어렵지 않을 것 같다 Sum.times에 대한 테스트를 작성한다 12345678910111213@Testpublic void testSumTimes() &#123; Expression dollar = Money.dollar(5); Expression franc = Money.franc(10); Bank bank = new Bank(); bank.addRate("CHF", "USD", 2); Expression sum = new Sum(dollar, franc).times(2); Money result = bank.reduce(sum, "USD"); assertThat(result).isEqualTo(Money.dollar(20));&#125; Expression에 times 메서드를 선언하고, Sum에도 times를 작성한다 123456789interface Expression &#123; // ... Expression times(int multiplier);&#125;@Overridepublic Expression times(int multiplier) &#123; return new Sum(augend.times(multiplier), addend.times(multiplier));&#125; 난 사실 이 장이 잘 이해되지 않는다…]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] TDD로 화폐 개발하기(2)]]></title>
    <url>%2Ftdd%2FTDD%EB%A1%9C-%ED%99%94%ED%8F%90-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0-2%2F</url>
    <content type="text"><![CDATA[이번 예제는 아래의 것들을 설명해주고 있다 큰 단위의 테스트를 작게 접근하는 방법 테스트를 기반으로 수월하게 리팩토링 하는 방법 중복되는 두 클래스의 형태를 동일하게 바꾼 뒤 상위 클래스로 올리기(push up) 해결법이 보이지 않을때 다시 되돌아가서 시도하는 방법 큰 단위의 작업에 접근하기 이제 처음에 복잡해보여서 시도하지 못했던 $5 + 10CHF = $10(환율이 2:1일 경우) 을 작성해보자 알다시피 이 요구사항의 경우 테스트 단위가 크다 우리는 이렇게 큰 테스트를 공략할 수 없으므로 진전을 나타낼 수 있는 작은 테스트를 만들어야 한다 일단 뭐 잘 모르겠고, 다시 보니 Dollar와 비슷하게 Franc이 필요해보이니 추가해보자 1234567891011121314// FrancTest@Testpublic void testMultiplication() &#123; Franc five = new Franc(5); assertThat(five.times(2)).isEqaulTo(new Franc(10)); assertThat(five.times(3)).isEqaulTo(new Franc(15));&#125;@Testpublic void testEquality() &#123; assertThat(new Franc(5)).isEqualTo(new Franc(5)); assertThat(new Franc(5)).isNotEqualTo(new Franc(6));&#125; 알다시피 이 테스트를 빨리 초록막대에 도달하게 해야한다 가장 빠른 방법은 Dollar를 복사히여 Franc을 만드는 것이다(뭐 테스트도 복사한 마당에…) 복사 붙여넣기라니, 굉장히 이상해보일 수 있다 하지만 알다시피 5단계(중복제거)에서 다 수정하면 된다 4단계까지는 설계보다 속도가 더 중요하고, 그것을 위해선 어떤 죄악도 저지를 수 있다 물론 5단계에서 이 죄악을 수습하지 않고는 다음 단계로 나아갈 수 없다 여기서 적절한 설계를 하고, 돌아가게 만들고, 올바르게 만들어야 한다 죄를 수습하자 중복을 제거하기 위한 방법은 뭐가 있을까? 두 클래스의 공통된 상위 클래스를 추출해보는건 어떨까? 먼저 Dollar 부터 적용시켜보자 먼저 Dollar와 Franc의 공통 클래스로 Money라는 애를 만들고, 이를 상속받게 한다 123456class Money&#123;&#125;class Dollar extends Money &#123; private int amount; // ...&#125; amount 변수를 Money로 올리고(protected), Dollar에서 amount 변수를 제거할 수 있다 1234567class Money&#123; protected int amount;&#125;class Dollar extends Money &#123; // ...&#125; 이제 equals 부분을 Money로 올려볼 수 있다 우선 equals 부분을 Money에 맞춰 변경하고 1234public boolean equals(Object object) &#123; Money money = (Money) object; return amount == money.amount;&#125; Money 클래스로 올린다 Dollar에서 equals는 삭제한다 각 과정을 진행할때마다 계속 테스트를 돌리면서 진행했고, 수월하게 리팩토링 할 수 있었다 이제 Franc도 똑같이 진행할 것이다 아까 DollerTest를 복사해서 FranTest를 만들어줬기 때문에 Franc 또한 Dollar 처럼 수월하게 리팩토링이 가능하다 만약 Franc에 대한 테스트를 추가하지 않았다면 라팩토링 전에 추가해주는 것이 좋다 (이 외에도 있어야 할 것 같은 테스트가 있다면 작성해줘야 한다) 그렇게 하지 않으면 리팩토링 하다가 결국 뭔가 꺠트릴 것이고, 리팩토링에 대해 안좋은 느낌을 갖게 되고, 리팩토링을 덜 하게 되고, 코드의 질이 떨어지게 되고, 해고당한다(!!!) 또 한번 찾아온 불길한 느낌 우리는 앞 단계에서 배웠다 부작용에 대한 혐오감이 생기는 경우, 즉시 테스트를 추가해서 결과를 확인해봐야 한다 123public void testEquality() &#123; assertThat(new Dollar(5)).isNotEqualTo(new Franc(5));&#125; 테스트가 실패한다! 그래도 불안감을 눈으로 확인해보니 마음이 한결 편해졌다 이 부분은 객체의 클래스를 비교함으로써 간단하게 구현가능하다 12345public boolean equals(Object object) &#123; Money money = (Money) object; return amount == money.amount &amp;&amp; getClass() == money.getClass();&#125; 간단하게 테스트를 통과시켰다 클래스 타입 비교로 통화를 비교한다는게 조금 그렇긴하지만 일단은 그냥 넘어간다 더 많은 동기가 있기 전에는 더 많은 설계를 하지 않는것이 좋다 하위클래스를 없애기 위한 시도 - 직접 참조 제거 상위 클래스 추출이 성공했으니, 남아있는 times() 메서드의 리턴타입도 상위 클래스로 변경해도 괜찮겠다 1234567891011class Dollar &#123; Money times(int multiplier) &#123; return new Dollar(amount * multiplier); &#125;&#125;class Franc &#123; Money times(int multiplier) &#123; return new Franc(amount * multiplier); &#125;&#125; 이쯤되니 두 하위 클래스의 형태가 많이 비슷해졌다 바로 제거하는 테크를 타고 싶긴하지만, 그렇게 한번에 큰 step을 밟는것은 좀 위험하고 TDD에도 맞지 않으니 단계적으로 진행하도록 한다 첫번째 단계는 하위클래스에 대한 직접적인 참조들을 제거하는 것이다 new Dollar(5) 와 같은 강력한 직접 참조들을 먼저 제거해보자 직접 참조를 제거하는 방법으로 팩토리 메서드를 써보면 좋을 것 같다 123456789101112// DollarTest@Testpublic void testEquality() &#123; assertThat(Money.dollar(5)).isEqualTo(new Dollar(5)); assertThat(Money.dollar(5)).isNotEqualTo(new Dollar(6));&#125;public void testMultiplication() &#123; Dollar dollar = Money.dollar(5); assertThat(Money.dollar(10)).isEqualTo(dollar.times(2)); assertThat(Money.dollar(15)).isEqualTo(dollar.times(3));&#125; 팩토리 메서드는 아래와 같이 만든다 123static Dollar dollar(int amount) &#123; return new Dollar(amount);&#125; times()의 직접 참조도 제거하고, 123public Money times(Integer multiplier) &#123; return Money.dollar(amount * multiplier);&#125; Dollar에 대한 참조까지 제거해주자 1234public void testMultiplication() &#123; Money money = Money.dollar(5); // ...&#125; Money에 times가 없기 때문에 오류가 발생한다 이를 위해 추상 클래스를 하나 추가해준다(더 먼저해야 했을수도 있었다) 123abstract class Money &#123; abstract Money times(int multiplier);&#125; 이제 완벽하게 new Dollar()를 팩토리 메서드로 대체할 수 있다 하위 클래스의 존재를 테스트 메서드에서 분리(decoupling)헀으므로, 이제 테스트에 영향을 주지 않고 상속구조를 마음껏 변경할 수 있게 된다 테스트 메서드뿐 아니라 모든 클라이언트 코드에서도 이런식으로 결합도를 낮추면 변화에 유연해진다 이제 Franc에 대해서도 똑같이 작업해준다 똑같이 변경해놓고 보니, DollarTest와 FrancTest의 테스트들의 형태가 매우 중복되어 보인다 나는 이 시점에서 두 테스트를 합쳐서 MoneyTest로 만들었다 이런식으로 하위클래스가 분리되다보면 몇몇 테스트가 불필요한 여분의 것이 된다 하위클래스를 없애기 위한 시도 - 추상화 times()의 모양을 같게 하려면 Money.dollar와 Money.franc을 같게 만들어야한다 그러므로 둘을 추상화 할수있는 뭔가가 필요하다 통화라는 개념을 도입하면 뭔가 하위 클래스 제거에 좀 더 가까워질 것 같다 먼저 통화 개념에 대한 테스트를 작성해본다 1234public void testCurrency() &#123; assertThat(Money.dollar(1).currency()).isEqualTo("USD"); assertThat(Money.franc(1).currency()).isEqualTo("CHF");&#125; 통화를 인스턴스 변수에 저장하고 메서드에서 그걸 반환해주면 될 것 같다 (좀 더 step by step으로 갈수도 있긴 하지만 바로 떠오르니깐 뭐) 1234567891011121314151617181920212223// Dollarprivate Strnig currency;public Dollar(int amount) &#123; this.amount = amount; currency = "USD";&#125;public String currency() &#123; return currency;&#125;// Francprivate String currency;public Franc(int amount) &#123; this.amount = amount; currecny = "CHF";&#125;public String currency() &#123; return currency;&#125; 변수 선언과 currency() 메서드가 동일하므로, 이를 위로 올릴(push up) 수 있다(야호!) 123456// Moneyprotected String currency;String currency() &#123; return currency;&#125; Dollar와 Franc의 생성자에서 currency 세팅하는 부분을 파라미터로 받게하면 두 생성자의 형태가 똑같아질수 있을 것 같다 123456// Dollarpublic Dollar(int amount, String currency) &#123; this.amount = amount; this.currency = currency;&#125;// Franc도 동일 이렇게 바꿨더니 아래의 메서드가 깨진다(ㅠㅠ) 1234567public static Dollar dollar(Integer amount) &#123; return new Dollar(amount);&#125;public static Franc franc(Integer amount) &#123; return new Franc(amount);&#125; 우린 하위클래스의 형태를 맞추고 push up을 진행하는 중이었는데, 구조상의 변경으로 인해 이런식의 상황이 종종 발생할떄가 있다 이럴때는, 이걸 지금 고쳐야할까, 아니면 나중에 고쳐야할까? 교리상은 하던일을 중단하지 않고 다 끝낸 다음에 고치는것이 맞지만, 이정도의 짧은 중단은 그냥 받아들이고 가도 괜찮다 중요한것은 하던 일을 중단하고 다른 일을 하는 상태에서 또 그 일을 중단하면 안된다는 것이다(Jim Coplien) 12345// Moneypublic static Money dollar(int amount) &#123; return new Dollar(amount, "USD");&#125;// Franc도 동일 컴파일 에러를 해결했으니 위로 올리자. 1234567891011121314// Moneypublic Money(int amount, String currency) &#123; this.amount = amount; this.currency = currency;&#125;// Dollarpublic Dollar(int amount, String currency) &#123; super(amount, currency);&#125;// Francpublic Franc(int amount, String currency) &#123; super(amount, currency);&#125; 생성자를 바로 제거하고 싶었으나 아직 Money가 추상 클래스라 불가능하다 times를 먼저 손봐야한다 이런식으로 단계적으로 밟아가는 과정이 답답할수도 있다 중요한 것은 이런식으로 일해야 한다는 것이 아니라, 이런식으로 일할수도 있어야 한다는 것이다 종종걸음 step이 답답하면 조금 보폭을 늘려도 되고, 성큼성큼 걷는것이 불안하면 조금 보폭을 줄이면 된다 TDD란 조종해나가는 과정이다. 올바른 보폭이란 존재하지 않는다 다시 돌아가서 생각을… times() 메서드를 바로 위로 올려보려고 했는데, 생김새가 좀 막막하다 123456789// Dollarpublic Money times(Integer multiplier) &#123; return Money.dollar(amount * multiplier);&#125;// Francpublic Money times(Integer multiplier) &#123; return Money.franc(amount * multiplier);&#125; 잘 모르겠으니 팩토리 메서드를 다시 생성자로 돌려보자 (이렇게 방법이 없을땐 잠시 후퇴해서 보는것도 방법이다) (+ 여기서 currency는 클래스에 있는 값을 바로 쓰도록 한다. 굳이 파라미터로 또 전달할 필요 없다) 123456789// Dollarpublic Money times(Integer multiplier) &#123; return new Dollar(amount * multiplier, currency);&#125;// Francpublic Money times(Integer multiplier) &#123; return new Franc(amount * multiplier, currency);&#125; 이렇게 돌려놓으니 좀 보이는것 같다 Dollar나 Franc으로 생성하느냐는 별로 중요한 것 같지가 않다. 어쩌피 currency가 있는데 굳이 뭐하러. 저걸 Money로 바꿔보자. 12345// Dollarpublic Money times(Integer multiplier) &#123; return new Money(amount * multiplier, "CHF");&#125;// Franc Money를 구현 클래스로 만들어야 하므로, 추상 메서드를 제거한다 1234// Moneypublic Money times(Integer multiplier) &#123; return null;&#125; 이렇게 하고 테스트를 돌렸더니, Money 클래스가 Dollar 클래스가 아니라는둥, Money 클래스가 Franc 클래스가 아니라는 둥의 결과가 출력된다 이 말인 즉, 문제는 equals에 있었던 것이다. 비교해야 될 것은 클래스가 아니라 currency이다 이를 위해 추가적인 테스트가 작성되어야하는데, 현재 빨간막대 상태이다 빨간 막대 상태일때는 테스트를 추가하지 않는 것이 좋다 좀 보수적이긴 하지만, 다시 Dollar와 Franc의 new Money를 new Dollar, new Franc으로 돌린다 그리고 테스트를 작성한다 1234@Testpublic void testDifference() &#123; assertThat(new Money(5, "USD")).isEqualTo(new Dollar(5, "USD"));&#125; 이 테스트를 통과시키기 위해 equals를 변경한다 12345public boolean equals(Object obj) &#123; Money money = (Money) obj; return amount.equals(money.amount) &amp;&amp; currency().equals(money.currency());&#125; 테스트가 통과하니, 다시 times의 new Dollar, new Franc을 new Money로 바꾼다 이 테스트 또한 잘 통과하고, 이제 times의 형태가 같아졌으니 push up 할 수 있다!! 뒤로 돌아가지 않았다면 이처럼 팩토리 메서드 사용을 제거할 수 있다는 사실을 알기 힘들었을 것이다 하위클래스를 없애기 위한 시도 - 나머지 부분 제거 이제 두 클래스에 남은건 생성자밖에 없다 단지 생성자 때문에 하위클래스를 남겨놓을수는 없으니, 제거하는 것이 좋다 유일하게 남아있는 하위 클래스 직접 참조인 정적 팩토리 메서드를 수정하자 1234567public static Money dollar(Integer amount) &#123; return new Money(amount, "USD");&#125;public static Money franc(Integer amount) &#123; return new Money(amount, "CHF");&#125; 이제 완벽하게 제거하…려고 하는데, 생각해보니 아직 직접 참조가 한군데 더 남아있다 1234@Testpublic void testDifference() &#123; assertThat(new Money(5, "USD")).isEqualTo(new Dollar(5, "USD"));&#125; 보니까 이 테스트는 이미 다른 테스트에서 수행하고 있는 작업들이다. 제거하자. 이로 인해 최종적으로 하위 클래스를 전부 제거할 수 있게된다!!]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] TDD로 화폐 개발하기(1)]]></title>
    <url>%2Ftdd%2FTDD%EB%A1%9C-%ED%99%94%ED%8F%90-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0-1%2F</url>
    <content type="text"><![CDATA[이번 예제는 아래의 것들을 설명해주고 있다 처음 테스트를 작성하고, 주기를 완성하는 방법 불길한 예감을 추가해서 테스트를 단단하게 하는 방법 프로덕션 구현을 어떻게 해야할지 모르겠을 때 삼각측량을 이용하는 방법 코드나 테스트가 리팩토링되면서 감춰도 되는 변수를 private 으로 감추는 방법 TDD 주기 만들기 요구사항(작업해야 할 목록)을 나열한다 통화가 다른 두 금액을 더한 금액(주어진 환율에 맞게)을 결과로 얻을 수 있어야한다 어떤 금액을 어떤 수에 곱한 금액을 결과로 얻을 수 있어야한다 요구사항을 보고 할일 목록을 작성한다 $5 + 10CHF = $10(환율이 2:1일 경우) $5 x 2 = $10 Dollar 부작용(side effect?) 등등등 이중 간단한 것 부터 시작한다 복잡한 것은 작게 나눠서 시작하던지, 아예 손을 대지 않는것이 좋다 여기서는 $5 x 2 = $10 부터 시작한다 필요할 테스트를 생각해보고, 작성한다 이때 테스트할 메서드의 완벽한 인터페이스(형태)에 대해 상상해보는 것이 좋다 가능한 최선의 API에서 시작해서 거꾸로 작업하는 것이 애초부터 일을 복잡하고 보기 흉하며 현실적이게 하는 것보다 낫다 123456@Testpublic void testMultiplication() &#123; Dollar five = new Dollar(5); five.times(2); assertThat(five.amount).isEqaulTo(10);&#125; Dollar 클래스, 내부 메서드들이 구현되지 않았기 때문에 컴파일부터 실패하므로, 이를 해결한다 Dollar 클래스 생성 생성자 생성 times(int) 생성 amount 필드 생성 1int amount; 컴파일만 될 수 있게 최소한의 구현만 해서, expected/actual 을 반환하는 형태가 되도록 빨리 만든다 현재 상태에서는 exptected : 10, actual : 0 을 반환하며 테스트가 실패하지만, 이것도 진척이다(정확히 만족시켜야 할 상황을 알게되기 때문이다) 이제 스텁구현(끔찍한 죄악!)을 통해 테스트를 만족시킨다 1int amount = 10; 테스트가 통과하니, 이제 리팩토링(중복 제거)해야한다 10이라는 숫자는 사실 초기값과 곱하고자 하는 수가 같이 들어가있는, 중복 데이터이다 이를 분리한다 1int amount = 5 * 2; 그리고 뭐… 이렇게 저렇게해서 아래와 같이 진행한다 12345678// constructorDollar(int amount) &#123; this.amount = amount;&#125;void times(int multiplier) &#123; amount *= multiplier;&#125; 단계들이 굉장히 작다고 느껴질 수 있는데, TDD의 핵심은 작은 단계를 밟아야 한다는 것이 아니라, 이런 작은 단계를 밟을 능력을 갖추어야 한다는 것이다 작은 단계로 작업하는 방법을 배우면, 저절로 적절한 크기의 단계로 작업할 수 있게 된다 하지만 큰 단계로만 작업했다면, 더 작은 단계가 적절한 경우에 대해 결코 알지 못하게 된다 위 과정에서 볼 수 있는 일반적인 TDD의 주기는 아래와 같다 테스트를 작성한다 메서드 형태가 어떤식으로 나타나길 원하는지 생각해본다 실행 가능하게 만든다 다른 무엇보다도 중요한 것은 빨리 초록 막대를 보는 것이다 여기서 어떠한 죄악을 저질러도 상관없다 만약 깔끔하고 단순한 해법이 명백히 보인다면 그것을 입력한다 굳이 돌아갈 필요는 없다 올바르게 만든다 이제 시스템이 작동하므로(초록 막대!) 그 전에 저질렀던 죄악을 수습해야 한다 죄악을 수습하는 과정은 대부분 중복 제거이다 우리의 목적은 동작하는 깔끔한 코드를 얻는 것이다 하지만 이는 최고의 프로그래머들도 도달하기 힘든 목표이고, 우리같은 일반적인 사람들은 거의 불가능한 일이다 그래서 분할하여 정복(Divide and Conquer)를 사용하는 것이다 일단 작동하는 부분을 먼저 해결하고, 깔끔한 코드 부분을 해결하는 것이다 아키텍쳐 주도 개발과 정 반대다 뭔가 이상한 것 같은데? 뭔가 기존 코드에 사이드 이펙트가 있는 것 같으니, 빠르게 테스트를 추가해서 확인해보자!! 12345678@Testpublic void testMultiplication() &#123; Dollar five = new Dollar(5); five.times(2); assertThat(five.amount).isEqaulTo(10); five.times(3); assertThat(five.amount).isEqaulTo(15);&#125; 테스트가 실패한다. 알다시피 times 수행때마다 내부 amount가 변경되기 떄문이다 times() 메서드에서 매번 새로운 객체를 반환하게 하면 이 문제를 해결가능할 것 같다 근데 이렇게 변경하려니, 프로덕션 코드와 테스트 코드가 둘 다 변경되어야 한다 뭔가 죄를 저지른듯한 기분이지만, 괜찮다 어떤 구현이 올바른지에 대한 우리 추측이 완벽하지 못한 것과 마찬가지로, 올바른 인터페이스에 대한 추측 역시 절대 완벽하지 못하기 때문이다. 괜찮다. 테스트를 변경하자 123456789@Testpublic void testMultiplication() &#123; Dollar five = new Dollar(5); Dollar product = five.times(2); assertThat(product.amount).isEqaulTo(10); product = five.times(3); assertThat(product.amount).isEqaulTo(15);&#125; 그리고 올바르다고 생각되는 코드를 넣고, 테스트를 돌린다 (지금은 운이 좋아 명백히 올바른 코드가 떠올랐지만, 바로 떠오르지 않을떄는 스텁으로 구현한다) 123Dollar times(int multiplier) &#123; return new Dollar(amount * multiplier);&#125; 이처럼 느낌(부작용에 대한 혐오감)을 테스트로 변환하는 것은 TDD의 일반적인 주제이다 이런 작업을 오래 할수록 느낌(미적 판단)을 테스트로 담아내는 것에 점점 익숙해지게 된다 삼각측량을 이용한 VO 구현 구현해놓고 보니, new Dollar(5)와 new Dollar(5)가 같아야 할 것 같다 테스트를 작성한다 1234@Testpublic void testEquality() &#123; assertThat(new Dollar(5)).isEqualTo(new Dollar(5));&#125; 빠르게 통과시켜보자 1234@Overridepublic boolean equals(Object obj) &#123; return true;&#125; 알다시피 이는 끔찍한 죄악이다 얼른 수정해야하는데, 어떻게 구현해야할지 잘 모르겠다(사실은 여기는 너무 간단해서 잘 알지만, 뭔가 복잡한 코드라고 생각해보자) 여기서 삼각측량을 이용해서 답을 도출해낼 수 있다 삼각측량은 어떤 한 점의 좌표와 거리를 삼각형의 성질을 이용하여 알아내는 방법이다 간단하게 말해 2개를 통해 나머지 1개를 알아내는 것 이다 테스트에 삼각측량을 도입해본다 12345@Testpublic void testEquality() &#123; assertThat(new Dollar(5)).isEqualTo(new Dollar(5)); assertThat(new Dollar(5)).isNotEqualTo(new Dollar(6));&#125; amount 5를 가진 Dollar와 amount 6을 가진 Dollar는 같아서는 안된다 amount를 대상으로 비교하면 될 것 같다 1234567@Overridepublic boolean equals(Object obj) &#123; Dollar dollar = (Dollar) obj; return amount == dollar.amount;&#125;// null이나 다른 객체에 대한 검증도 필요하지만 // 당장은 필요하지 않으므로, 간단히 어디에 메모만 해두고 넘어간다 보다시피 2개의 테스트를 이용해 올바른 프로덕션 코드를 알아냈다 사실상 위의 equals 처럼 시작부터 일반적인 해법이 보일 경우 그냥 그 방법대로 구현하면 된다 이 방법은 설계를 어떻게 할지 떠오르지 않을떄 사용해보면 조금 다른 방향에서 생각해볼 기회를 제공해주는 역할을 한다 테스트 리팩토링 Dollar의 동치성을 구현하고 기존 테스트를 보니, 테스트가 그것을 정확히 얘기해주고 있지 않는것 같다 바꿔주자 123456789@Testpublic void testMultiplication() &#123; Dollar five = new Dollar(5); Dollar product = five.times(2); assertThat(product).isEqaulTo(new Dollar(10)); product = five.times(3); assertThat(product).isEqaulTo(new Dollar(15));&#125; 불필요한 임시변수(product)를 인라인 시키는 리팩토링을 실시한다 1234567@Testpublic void testMultiplication() &#123; Dollar five = new Dollar(5); assertThat(five.times(2)).isEqaulTo(new Dollar(10)); assertThat(five.times(3)).isEqaulTo(new Dollar(15));&#125; 이제 amount 변수는 Dollar 에서 밖에 사용하지 않으니 private으로 선언한다 1private int amount; 위 과정에서 우리는 위험한 상황을 만들었다라는 점을 인지해야 한다 Dollar에 대한 동치성 테스트(testEquality)가 실패하면 곱하기 테스트(testMultiplication) 역시 실패하게 된다는 점이다(테스트간 의존) 이것은 TDD를 하면서 적극적으로 관리해야할 위험 요소이다 근데 뭐 어떻게 하라는지는 딱히 알려주고 있진 않네…]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발</tag>
        <tag>TDD 주기</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[aws] AWS 프리티어(free-tier) 과금]]></title>
    <url>%2Faws%2FAWS-%ED%94%84%EB%A6%AC%ED%8B%B0%EC%96%B4-free-tier-%EA%B3%BC%EA%B8%88%2F</url>
    <content type="text"><![CDATA[어제 밤에 갑자기 AWS에서 29달러 정도가 결제되어서 급하게 확인했더니, 프리티어 기간이 끝나서 과금이 되었었다… 아놔… 그래서 그 내용을 정리하고자 한다 프리티어? AWS는 가입시 1년간 특정 리소스들을 무료로 사용할 수 있는 프리 티어 권한을 가질 수 있다 프리티어에서 1년간 무료로 사용가능한 리소스들은 아래와 같다 https://aws.amazon.com/ko/free/?awsf.Free Tier Typeasdasds=categories%2312monthsfree&amp;awsm.page-all-free-tier=2 프리티어인지 확인하는 법 AWS 로그인 - 내 결제 대시보드 로 접속한다 청구서 탭에서 날짜 드롭박스를 보고 언제 가입했는지 알 수 있다. 여기서 1년이 아직 안 지났는지를 체크해보면 된다. 대시보드 홈에서 경고 및 알람 부분에 free tier 관련 내용이 써져있으면 아직 free tier라는 의미이다 https://docs.aws.amazon.com/ko_kr/awsaccountbilling/latest/aboutv2/free-tier-eligibility.html 과금을 해결하는법 프리티어 기간이 지났으면(혹은 프리티어라도 일정 수준 넘으면 과금될 수 있음) 과금이 될 리소스를 정리해야함 위의 1년간 무료로 사용할 수 있는 리소스와 아래 정보들을 조합해서 과금 대상인 리소스들을 다 정리해야함 https://docs.aws.amazon.com/ko_kr/awsaccountbilling/latest/aboutv2/checklistforunwantedcharges.html 내 경우 EC2와 RDS에서 과금이 되고 있었음 RDS 들어가서 해당 데이터베이스를 삭제시켰고, EC2에서 인스턴스를 terminated 시켰음]]></content>
      <categories>
        <category>aws</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>AWS 프리티어</tag>
        <tag>AWS 프리티어 과금</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] 객체지향 스타일]]></title>
    <url>%2Ftdd%2F%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5-%EC%8A%A4%ED%83%80%EC%9D%BC%2F</url>
    <content type="text"><![CDATA[우리는 작성하기 쉬운 코드보다는 유지보수하기 쉬운 코드를 높이 평가한다 가장 직접적인 방식으로 기능을 구현하면 시스템의 유지보수성이 떨어질 수 있고, 그러면 코드를 이해하기 어려워지고 컴포넌트간에 보이지 않는 의존성이 생기게 된다 하지만 알다시피 당면한 관심사와 장기적인 관심사간의 균형을 유지하는 일은 까다로운 문제이다 유지 보수성을 고려한 설계 관심사의 분리 관련된 변경사항이 전부 코드의 어느 한군데에 들어있다면, 뭔가 변경할 때 코드의 여러곳을 변경하지 않아도 된다 하나의 클래스가 여러 관심사를 가지고 있다면, 다른 클래스에서도 해당 관심사들을 가지고 있을 확률이 매우 높다 이는 각각 특정 관심사로 분리되어야 하고, 분리된 관심사들이 여러곳에서 재사용 되어야 한다 그러면 특정 변경사항을 적용하기 위해 코드의 여러곳을 수정하는 상황을 막을 수 있다 우리는 변경사항(요구사항)을 예측할 수 없으므로, 같은 관심사들을 하나의 클래스로 모으는 연습을 계속해서 해야한다 e.g. 인터넷 표준 프로토콜로 전달된 메시지를 푸는 코드는 그 메시지를 해석하는 코드와 똑같은 이유로 변경되어서는 안되므로, 두 개념을 각기 다른 패키지로 나워야 한다 더 높은 수준의 추상화 흐름을 직접 제어하기보다는 클래스들을 조합하는 식으로 프로그램을 작성하면 더 많은 일을 해낼 수 있다 이는 사람들이 식당에서 음식을 주문할 때 세세한 조리법을 설명하는 것이 아니라 메뉴를 보고 음식을 주문하는 것과 같다 ports &amp; adapter pattern http://getoutsidedoor.com/2018/09/03/ports-adapters-architecture/ https://dzone.com/articles/hexagonal-architecture-for-java 기술적인 부분을 인터페이스로 분리해서 변경을 용이하게 한다 정도로만 이해했고, 실제 예제를 보지 못해서 정확히 이해하지 못하겠다 캡슐화와 정보은닉 캡슐화와 정보은닉은 비슷해보이지만 설계의 품질을 나타낼때는 별개의 개념이다 캡술화 https://javacan.tistory.com/entry/EncapsulationExcerprtFromJavaBook 해당 객체의 API를 통해서만 객체의 행위를 좌우할 수 있다 외부에서 객체의 상태를 직접 접근해 변경하거나, 무언가 행위를 하는것이 불가능하다는 의미이다 예상치 못한 의존성이 없음을 보장해줌으로써, 한 객체에 대한 변경이 시스템의 다른 부분에 영향을 덜 주게끔 통제할 수 있다 잘못 캡슐화된 코드를 활용할 경우 해당 코드를 어디서 참조하는지 찾아보고, 잠재적 영향력을 추적하는데 굉장히 많은 시간을 보내게 된다 정보 은닉 해당 객체의 기능을 구현하는 방법을 추상화된 API 너머로 감춘다 e.g. 정렬이라는 메서드가 있으면, 내부에서 어떻게 정렬을 수행하는지 외부에서는 알 필요가 없게끔 한다 하지만 대부분의 객체지향 언어에서 제공하는 별칭(aliasing)을 통해 캡슐화를 위반할 수 있다 관련이 없는 두 클래스를 묶거나, 특정 클래스를 wraping한 클래스를 만들고, 그 클래스를 통해 자식클래스에 행위들을 수행해버릴 수 있다 1234567class A &#123; private B b; public void doManything(B b) &#123; // aliasing b.doSomething(); // 캡슐화 위반 &#125;&#125; A를 통해서 B를 수정할 수 있게 되므로, 이는 캡술화를 위반한 것이다 위와 같이 doManything를 호출 한 후 b의 값이 변경되어 버리는 현상이 일어나서는 안된다 이러한 상황들을 방지하기 위해 아래와 같은 관례를 따라야 한다고 말한다 변경 불가능한 값 타입을 정의하고 값 타입은 내부 값이 수정 불가능한 final이고, 값에 특정한 행위를 수행해도 새로운 객체로 반환하기 때문에 위와 같은 상황에서 안전하다 전역변수와 싱글턴은 자제하며 컬렉션과 변경 가능한 값을 객체간에 전달할때는 그것을 복사해야한다 객체를 복사한다면 위와 같은 상황에 안전할 것이다 저런식으로 협력객체의 특정 이 같은 결정이 중요한 이유는 어떤 객체를 얼마나 쉽게 쓸수 있는지에 영향을 주고, 시스템 내부 품질에 기여하기 때문이다 단일 책임 원칙 모든 객체는 반드시 단 한가지 명확히 규정된 책임을 지녀야 한다 그리고 그러한 객체들이 재사용되어야 한다 한 객체의 역할을 설명할때는 접속사(와,나 등)를 사용하지 않고도 해당 객체의 역할을 설명할 수 있어야 한다 접속사로 구분되는 애들이 각각 객체로 나뉠 수 있을것이다? 객체 이웃의 유형 객체가 단일 책임을 지녔고, 명료한 API를 통해 이웃 객체와 통신한다면, 서로 무슨 얘끼를 할까? 한 객체가 지닐 수 있는 관계는 다음과 같다]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발로 배우는 객체지향 설계와 실천</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] TDD 주기의 유지]]></title>
    <url>%2Ftdd%2FTDD-%EC%A3%BC%EA%B8%B0%EC%9D%98-%EC%9C%A0%EC%A7%80%2F</url>
    <content type="text"><![CDATA[TDD 프로세스를 시작하고 나면, 그 프로세스를 매끄럽게 유지해야 한다 그 상세한 방법 시스템을 구축할 때 테스트를 작성하는 방법 테스트를 이용해 내외적인 품질 문제에 일찍 피드백을 받는 방법 테스트가 계속 변화를 뒷받침하고, 이후 개발에 걸림돌이 되지 않게 하는 방법 각 기능을 인수 테스트로 시작하라 인수 테스트는 우리가 작성하려는 기능이 아직 시스템에서 갖추지 못했다는 사실을 보여주고, 그 기능이 완성되기까지 진행 상황을 반영한다 인수테스트를 작성할때는 기반 기술(데이터베이스나 웹 서버 같은)의 용어가 아닌 응용 도메인에서 나온 용어만 이용한다. 이렇게하면 시스템에서 해야 할 일이 뭔지 이해하는데 보탬이 되고, 구현에 관한 초기 가정에 얽매이지도 않을 뿐더러 테스트가 기술적인 세부사항으로 복잡해지지도 않는다 이뿐 아니라 시스템의 기술 기반 구조가 바뀌었을 때도 인수 테스트를 보호할 수 있다 코딩을 시작하기 전에 테스트를 작성하면 달성하고자 하는 바가 명확해진다 실패하는 테스트덕에 요구사항을 충족하는데 필요한만큼의 기능을 구현하는데 집중할 수 있어 기능을 완성할 확률이 높아진다 테스트로 시작하게 되면 사용자 관점에서 시스템을 바라보게 되어 구현자 관점에서 기능을 짐작하지 않고 사용자가 필요로 하는 것을 이해하게 된다 반면 단위 테스트는 객체 집합을 격리된 상태에서 시험하므로, 그 클래스가 시스템의 나머지 부분과 조화롭게 동작할지에 대해서는 아무것도 담보하지 않는다 인수테스트는 단위 테스트를 거친 객체를 대상으로 통합 테스트를 수행할 뿐 아니라, 프로젝트를 앞으로 나아가게 한다 회귀를 포작하는 테스트와 진행상황을 측정하는 테스트를 분리하라 새 인수 테스트는 진행중인 작업을 나타내고, 기능이 준비될 때 까지는 통과하지 않을 것이다 인수 테스트를 통과하면 해당 테스트는 완료된 기능을 나타내고, 다시는 실패해서는 안된다 (실패는 이전 상태로 회귀했고, 기존 코드를 망가뜨렸음을 의미한다) 인수 테스트를 빨간색에서 녹색으로 바꾸는 활동으로 우리는 진행상태를 측정할 수 있다 정기적인 인수테스트 통과 주기는 중첩된 피드백 고리를 구동하는 엔진에 해당한다 즉 우리는 진행중인 테스트와 완료된 테스트(회귀 테스트)를 항상 분리해야 한다 만약 요구사항이 바뀐다면 거기에 영향을 받은 인수테스트를 회귀 테스트 그룹에서 빼내어서 진행중인 테스트 그룹으로 옮긴 후 새 요구사항을 반영토록 수정한 다음, 해당 테스트를 다시 통과하게끔 시스템을 변경해야 한다 테스트를 가장 간단한 성공 케이스로 시작하라 간단한을 지나치게 단순한으로 해석해서는 안된다 지나치게 단순한 케이스는 시스템에 가치를 별반 더하지 않으며, 더 중요한 점은 아이디어의 유효성에 관해 충분한 피드백을 전해주지 않는다는 것이다 그래서 가장 간단한 성공 케이스로 테스트를 시작해보는 것이 좋다 해당 테스트가 동작하면 솔루션의 실제 구조에 관해 더 좋은 생각이 떠오를테고, 그 과정에서 발견한 발생 가능한 실패를 처리하는 것과 이후의 성공 케이스 사이에서 우선순위를 가늠해볼 수 있다. 처리해야 할 실패 케이스, 리팩터링, 기타 작업들을 메모장 깉은 곳에 기록해두면 테스트 작성에 도움이 많이 된다 기능 구현을 할 때 실패 케이스에만 집중하면 의욕을 짐작하는데 좋지않다. 오류처리메나 신경쓰다 보면 아무것도 성취한 바가 없는 듯이 느껴지기 때문이다 읽고 싶어 할 테스트를 작성하라 각 테스트는 시스템이나 객체에서 수행할 행위로 가능한 한 명확하게 표현하는 것이 좋다 테스트를 작성하는 동안에는 테스트가 실행되지 않거나 컴파일 되지 않는다는 사실을 무시하고 테스트 내용에만 집중해서 작성하는 것이 좋다 테스트가 잘 읽히면, 이제 컴파일이나 런타임 오류를 해결한다 테스트가 명확한 오류메세지를 보이면서 예상대로 실패하면, 기반이 되는 코드를 충분히 구현했음을 의미한다 이제 테스트가 통과되도록 만들면 된다 테스트가 실패하는 것을 지켜보라]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발로 배우는 객체지향 설계와 실천</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] TDD 도구들]]></title>
    <url>%2Ftdd%2FTDD-%EB%8F%84%EA%B5%AC%EB%93%A4%2F</url>
    <content type="text"><![CDATA[JUnit 가장 많이 쓰이는 java 테스트 프레임워크이다. 그 중에서도 JUnit4 버전이 가장 많이 사용된다.(현재는 JUnit5 버전까지 나와있다) JUnit은 기본적으로 리플렉션을 통해 클래스 구조를 파악한 후, 해당 클래스에서 테스트를 나타내는 것을 모두 실행한다. 테스트 케이스 JUnit은 @Test라는 어노테이션이 지정된 메서드는 모두 테스트 케이스로 취급한다. 테스트 메서드는 값을 반환하거나 매개변수를 받아서는 안 된다. 12345678910111213public class SomeTest &#123; private Some some = new Some(); @Test public void testSomething1() &#123; // do something &#125; @Test public void testSomething2() &#123; // do something &#125;&#125; 하나 특별한점은, JUnit은 각 테스트를 실행할 때 마다 해당 테스트 클래스의 새 인스턴스를 생성하여 호출한다는 점이다. 위의 테스트들을 전부 실행하면 2개의 SomeTest 인스턴스가 생성되고, 각자 @Test 메서드들을 실행하게 된다. 이런식으로 매번 새 인스턴스를 생성하면 각 테스트간의 격리성을 확보할 수 있다. 테스트 객체의 필드가 각 테스트에 앞서 대체되기 때문이다. 이는 테스트에서 테스트 객체 필드의 내용을 맘껏 바꿀 수 있다는 의미이다. testSomething1에서 some의 값을 변경해도 testSomething2에는 영향을 주지 않는다 assertion 기본적으로 테스트들은, 각 테스트를 수행하고 그 결과를 assertion(단정)하는 식으로 작성된다. 여기서 JUnit에서 제공하는 assertion 메서드들을 사용할 수 있다. 123456789101112131415public class StoreTest &#123; private Store store = new Store(); @Test public class 결제수단을_체크한다() &#123; assertTrue(store.canPay(PaymentMethod.Money)); assertFalse(store.canPay(PaymeneMethod.CreditCard)); &#125; @Test public class 특정_상품을_찾는다() &#123; assertNotNull(store.find("치킨")); assertNull(store.find("가죽자켓")); &#125;&#125; 보다시피 canPay의 결과가 true/false가 나올것이다 라고 단정(assertion) 했고, find의 결과가 null이 아니라고 단정(assertion) 했다. 예외 예상하기 @Test 어노테이션은 선택적 매개변수로 expected 라는 것을 지원한다. 이 매개변수는 테스트 케이스에서 던져질 예외를 선언한다. 선언한 예외가 발생하면 테스트가 성공한다. 1234@Test(expected = IllegalArgumentException.class)public void 검색어를_입력하지_않는다() &#123; store.find(null);&#125; find 메서드는 검색어로 null을 받으면 IllegalArgument 예외를 리턴하게 작성되어 있다. 그러므로 위의 메서드는 성공하게 된다. 테스트 픽스쳐 테스트가 시작할 때 존재하는 고정된 상태를 의미한다. 테스트를 수행하기 전에 필요한 특정 상태들을 의미한다. 특정 협력객체나, 특정 데이터들이 있을 수 있다. JUnit은 각 테스트마다 인트턴스를 새로 생성하므로 간단히 인스턴스 변수 선언과 동시에 초기화하거나 생성자 같은것을 써서 픽스쳐를 초기화하면 되긴하지만, JUnit에서 제공하는 특정 어노테이션들을 사용하면 좀 더 명시적으로 픽스쳐 초기화가 가능하다. 12345678910111213private Store store = new Store();@Beforepublic void setUp() &#123; store.addProduct("chicken"); store.addProduct("pizza"); store.addProduct("beer");&#125;@Afterpublic void tearDown() &#123; // 별로 할게 없음..&#125; @Before 메서드가 모든 테스트 실행전에 실행되므로, 모든 테스트는 3가지 상품이 들어간 상태에서(동일한 상태에서) 테스트를 수행할 수 있게 된다. 이런식으로 모든 테스트 메서드에서 필요한 상태를 초기화하는데 사용하면 유용하다. @After 메서드는 테스트 메서드가 끝난 후 수행되는데, 사실상 여기서 수행할 작업이 많지는 않다. 생성된 픽스쳐를 정리하는 작업 같은것도 전부 JVM 가비지 컬렉터에서 잘 수행해주기 때문이다. 테스트 러너 JUnit이 클래스를 대상으로 리플렉션을 수행해 테스트를 찾아 해당 테스트를 실행하는 방식은 테스트 러너(test runner) 에서 제어한다. 테스트 클래스에서 사용하는 러너는 @RunWith 어노테이션으로 설정할 수 있다. 1234@RunWith(SpringJUnit4ClassRunner.class)public class SomeTest &#123;&#125; JUnit4의 현재 default runner는 BlockJUnit4ClassRunner 라고 한다. 햄크레스트 매처와 assertThat 햄크레스트는 매칭 조건을 선언적으로 작성하는 프레임워크이다. matches라는 boolean을 반환하는 메서드를 가진 Matcher 인터페이스를 구현하는 많은 클래스들을 제공한다. 이 클래스들을 사용하면 기존의 단순한 assertion 구문들을 좀 더 다양하게 사용 가능하다. 123Matcher&lt;String&gt; containsBananas = new StringContains("bananas");assertTrue(containsBananas.matches(str)); 햄크레스트는 코드의 가독성을 높이고자 Matcher를 생성하는 부분을 static factory 메서드로 제공한다. 123456import static org.hamcrest.CoreMatchers.*;@Testpublic void banana_match() &#123; assertTrue(containsString("bananas").matches(str));&#125; 하지만 실제로는 위와 같은 방법보단, self-describing 특성을 가진 assertThat을 주로 사용한다. 12assertThat(str, containsString("bananas"));assertThat(str, not(containsString("bananas"))); assertThat은 Matcher를 직접 인자로 받을 수 있다. 보다시피 str은 &quot;bananas&quot; 문자열을 포함하고 있다 의 형태로 작성됨으로써 테스트 코드가 더 잘 읽힌다. (참고로 햄크레스트는 위에서 사용된 not Matcher 처럼 다른 매처를 조합할 수 있는 유용한 기능을 제공한다) 이러한 Matcher는 몇가지 조건만 만족하면 사용자가 쉽게 직접 정의해서 사용할 수 있다. AssertJ assertThat과 햄크레스트를 적절히 조합하여 테스트를 작성하는 것만해도 충분하지만, 좀 더 풍부한 구문을 제공하는 AssertJ 라는 라이브러리도 있다. 12345678910111213import static org.assertj.core.api.Assertions.assertThat;public void 오브젝트() &#123; assertThat(object).isNotNull(); assertThat(object).isSameAs(otherObject); assertThat(object).isEqualTo(otherObject);&#125;public void 컬렉션() &#123; assertThat(list).isSorted(); assertThat(list).hasSize(4);&#125; 보다시피 좀 더 직관적이고, 풍부한 메서드들을 제공한다. 게다가 assertion과 햄크레스트를 직접 static import 하지 않아도 되는 편리함도 있으니, 사용해보는 것도 나쁘지 않다. JMock2 JMock을 사용하면 mock 객체를 JUnit 같은 테스트 프레임워크에 붙여서 사용할 수 있도록 해준다. JMock의 핵심 개념은 모조 객체와 목 객체, 예상 구문이다. 아래는 JMock을 이용한 행위 검증의 예제이다. 1234567891011121314151617@RunWith(JMock.class)public class AuctionMessageTranslatorTest &#123; private final Mockery context = new JUnit4Mockery(); // 1 private final AuctionEventListener listener = context.mock(AuctionEventListener.class); private final AuctionMessageTranslator translator = new AuctionMessageTranslator(listener); @Test public void notifiesAuctionCloseWhenCloseMessageReceived() &#123; Message message = new Message(); message.setBody("SOLVersion : 1.1; Event: Close;"); context.checking(new Expectations() &#123;&#123; oneOf(listener).auctionClosed(); &#125;&#125;); translator.processMessage(UNUSED_CHAT, message); &#125;&#125; 여기서 모조 객체라는 개념이 나오는데, 현재 이해하기로는 목 객체들을 담는 그릇 정도로 이해된다… JMock runner가 읽는 대상들? JUnit이 JMock 테스트 러너를 사용하게 된다. 이 러너는 테스트가 끝나는 시점에 모든 모조 객체를 자동으로 호출해 모든 목 객체가 예상대로 호출되었는지 검사한다. 모조 객체를 생성한다. JMock 러너가 검사할 목 객체들이 담긴다. AuctionEventListener의 mock 객체를 생성한다. 자동으로 context에 담긴다. SUT에 mock 객체를 주입한다. SUT는 그 사실을 모르고, 알 필요도 없다. 모조객체에서 검사할 내용(행위 검증)을 작성한다. listener 목객체가 auctionClosed 메서드를 정확히 1번 호출할 것을 예상하고 있다. SUT가 행위를 수행한다. 테스트가 끝나면 JMock runner는 모조객체(현재는 context)에 명시된 행위들을 검증한다 개인적 느낌인데, 작성하기가 조금 어려운 부분이 있는 것 같다. mockito JMock보다 좀 더 강력한 기능을 제공하는 mockito 라는 라이브러리가 있다. 아래는 사용법에 대해 작성된 글이다. https://github.com/mockito/mockito/wiki/Mockito-features-in-Korean 간단히 내가 느끼는 mockito의 장점은 아래 정도이다. mockito는 JMock 처럼 모조객체, 목 객체에 대한 구분이 없다. JMock에도 있는지 모르겠으나(당연히 있곘지…) stub을 매우 간단하게 지원한다. 행위 검증도 매우 간단하게 지원한다. 나는 개인적으로 이 라이브러리가 좀 더 작성하기 편리하고, 명시적인 것 같다. 참고로 mockito 보다 더 강력한 기능을 제공하는 PowerMock 이라는 애도 있다.(private 메서드 테스트, static 메서드 주입 등 까지 제공한다)]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발로 배우는 객체지향 설계와 실천</tag>
        <tag>JUnit</tag>
        <tag>assertThat</tag>
        <tag>hamcrest</tag>
        <tag>assertj</tag>
        <tag>jmock</tag>
        <tag>mockito</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] TDD 주기의 시작]]></title>
    <url>%2Ftdd%2FTDD-%EC%A3%BC%EA%B8%B0%EC%9D%98-%EC%8B%9C%EC%9E%91%2F</url>
    <content type="text"><![CDATA[되돌아보면 우리는… 개발을 다 진행해놓고 마지막에 통합하면서 피드백을 받았던 것 같다. 이 마지막 순간에 변경되는 부분도 굉장히 많고, 서로가 잘못 이해한 부분도 많이 발생한다. 최악의 경우는 작업을 뒤집거나, 누군가가 포기하고 가야하는 상황도 발생한다. 그러므로 앞서 얘기했듯이, 빠른 피드백을 받을 수 있게끔 시스템을 구성하는 작업(중첩된 피드백 고리)은 매우 중요하다. 위에서도 언급했듯이, 이런 시스템의 구성을 나중으로 미루는 것은 매우 위험하다 저 정도 예시면 양반이지, 배포할 수 없어서 프로젝트가 취소되거나 마지막에 테스트를 추가했는데도 불구하고 오류율이 너무 높아 시스템이 폐기되기도 한다(저자의 경험) 피드백은 아주 근본적인 도구이며, 올바른 방향으로 나아가고 있는지 최대한 미리 파악하고자 한다 그 중에서도 외부와의 피드백 고리를 형성하는 것은 중첩된 피드백 고리를 구성하는 첫번쨰 단추이고, 가장 중요하다. 이 말인 즉 자동화된 빌드・배포・테스트 주기 전체를 가장 처음부터 구현해야 함을 의미한다. 근데 사실 말이 쉽지, 이렇게 구성하기에는 할일도 매우 많고, 어렵다. 그러면 우리는 무엇부터 시작해야 할까? 우선 동작하는 골격을 대상으로 테스트하라 먼저 동작하는 골격을 만들어야 한다 전 구간을 대상으로 자동 빌드・배포・테스트를 할 수 있는 실제 기능을 가장 얇게 구현한 조각을 말한다 첫 기능을 구현할 수 있을 정도의 자동화, 주요 컴포넌트, 통신 메커니즘이 포함될 것이다 이 구조를 이용해 유의미한 첫 기능에 대한 인수 테스트를 작성한다 이후로는 시스템의 나머지 부분을 대상으로 테스트 주도 개발을 진행할 수 있게 모든 것이 제자리에 놓일 것이다 동작하는 골격을 만드는 과정에 배포 단계를 포함한 이유는 아래와 같다 배포 단계는 오류가 발생하기 쉬운 활동이므로 실제 환경에 배포해야 할 때 까지 스크립트를 철저히 검증해야 하기 때문이다 배포 과정을 자동화 하는 것 만큼 프로세스를 이해하는데 도움이 되는 것은 없다 배포 단계에서 개발팀이 조직의 다른 부문과 접촉하기도 하며, 실제로 어떻게 운영되는지도 배워야하기 때문이다 참고로 동작하는 골격을 만들때는 골격의 구조에만 집중하고, 테스트가 최대한 표현력을 갖추게끔 테스트를 정리하는 일에 대해서는 크게 신경쓰지 않는다 동작하는 골격과 그것을 보조하는 기반 구조는 테스트 주도 개발을 시작하는 방법을 도와주기 위해 존재하는 것이기 때문이다 동작하는 골격의 외형 결정 어떤 전체 구조에 관한 구상 없이는 빌드・배포・테스트 주기를 자동화 할 수 없으므로, 계획한 첫 출시를 달성하는데 필요할 주요 시스템 컴포넌트와 그러한 컴포넌트의 상호 작용 방식에 대한 대략적인 그림은 필요하다. 이는 화이트보드에 몇분만에 그릴 수 있다 공개된 장소에 이런 구조를 그려놓으면 코드 작성 시 팀이 업무 방향을 참고하는데 도움이 된다(마파 문디) 당연하지만, 이 같은 초기 구조를 설계하려면 시스템의 목적을 어느 정도 이해해야 한다 클라이언트의 요구사항(기능적 요구사항, 비기능적 요구사항)을 고수준 관점에서 바라보고 의사결정에 참고해야 한다 그렇지 않으면 위험을 무릅쓰고 하는 일들이 죄다 의미가 없어진다 참고로 이를 과도한 사전 설계와 혼동하지 않아야 한다 실제 피드백을 토대로 배우고 개선해나가는 과정을 시작할 수 있게(+TDD 주기를 시작할 수 있게)하는데 필요한 최소한의 의사결정만을 내리는 것이 좋다 (현재 생각하고 있는 바가 틀릴 가능성이 있으므로, 시스템이 성장해가면서 세부 사항을 파악해나간다) 피드백 소스 구축 아래의 글로 피드백의 중요성을 재고하고 있다 어플리케이션 설계에 대해 내린 의사결정이나, 어플리케이션 설계가 근거하는 가정의 옳고 그름에 대해서는 아무것도 보장할 수 없다 그저 최선을 다할 뿐이며 현재 밟고 있는 절차에 피드백을 적용해 최대한 빨리 의사결정이나 가정을 검증하는데 의지할 뿐이다 아래는 위의 테스트 주도 주기가 주는 피드백에 대해 그려놓은 그림이다 보다시피 각 과정은 서로에게 피드백이 된다 이러한 시스템을 구축함으로써 우리의 기능이 요구사항에 얼마나 부합하는지, 우리 시스템 구현은 어떤지 빠르게 평가할 수 있고, 빠르게 반영할 수 있다. 최종적으로 우리는 안전해지고, 완전한 소프트웨어를 얻게 될 것이다!!! 또 하나의 가장 큰 헤택은 뭘 배우늗 거기에 맞춰 시스템을 변경할 수 있으리라는 점이다 뭐든 테스트를 먼저 작성한다는 것은 철저한 회귀 테스트 모음을 갖게 된다는 것을 의미하기 때문이다 물론 어떤 테스트도 완벽하지 않지만, 견고한 테스트 모음이 있으면 중대한 변경을 안전하게 할 수 있다는 사실은 확실하다 처음에는 이러한 순서대로 점진적인 개발을 하는 과정이 불안정하고, 활동량도 굉장히 많을 것이지만 자동화가 구축되고 나면 반복적인 과정으로 안정화된다 나중에 통합 절차를 수행하는 프로젝트는 침착한 분위기에서 시작하지만, 처음으로 시스템 통합을 시도하게 되는 후반부에 힘들어지곤 한다 통합을 나중에 하는 방식의 경우 어떤 일이 일어날지 예측하기 불가능한데, 팀에서 엄청나게 많은 각 부분을 제한된 시간에 짜맞춰야 하고 실패한 부분을 고치는 일도 감안해야 하기 때문이다 그래서 경험이 풍부한 이해관계자는 프로젝트 초반부에 생기는 불안정성에 고약하게 반응한다 후반후에 들어가면 상황이 더 악화되리라 예상하기 때문이다 비단 통합 뿐만이 아니다. 우리는 각종 테스트들을 먼저 작성함으로써 혼란을 미리 잡고가게 되는 효과를 가지게 된다. 가장 중요한 것은 방향 감각을 확보하는 것과 가정을 테스트할 구체적인 구현을 갖춰야 한다는 것이다 동작하는 골격은 프로젝트 초기에 각종 쟁점을 드러내지만, 프로젝트 초기라면 아직까지 그러한 쟁점을 해결할 시간과 예산, 의지가 있을 때이다]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발로 배우는 객체지향 설계와 실천</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] commit message convention]]></title>
    <url>%2Fgit%2Fcommit-message-convention%2F</url>
    <content type="text"><![CDATA[커밋 메세지 컨벤션 https://meetup.toast.com/posts/106 https://doublesprogramming.tistory.com/256 feat : 새로운 기능 추가 fix : 버그 수정 docs : 문서 수정 style : 코드 포맷팅, 세미콜론 누락, 코드 변경이 없는 경우 refactor : 코드 리펙토링 test : 테스트 코드, 리펙토링 테스트 코드 추가 chore : 빌드 업무 수정, 패키지 매니저 수정]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>commit</tag>
        <tag>commit convention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] 객체를 활용한 테스트 주도 개발]]></title>
    <url>%2Ftdd%2F%EA%B0%9D%EC%B2%B4%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%85%8C%EC%8A%A4%ED%8A%B8-%EC%A3%BC%EB%8F%84-%EA%B0%9C%EB%B0%9C%2F</url>
    <content type="text"><![CDATA[객체의 설계에 대해 설명하고 있다 객체망 객체 지향 설계는 객체 자체보다 객체간의 의사소통에 더 집중한다 중요한 것은 메시지 전달이며, 위대하고 성장 가능한 시스템을 만들때의 핵심은 모듈간의 의사소통에 있지, 모듈의 내부 특성이나 작동 방식에 있지 않다 시스템은 객체를 생성해 서로 메시지를 주고받을 수 있게 조립하는 과정을 거쳐 만들어진다. 시스템의 행위는 객체의 조합(객체의 선택과 연결 방식)을 통해 나타나는 특성이다. 이런식으로 시스템을 구축하면 방법(how)이 아니라 목적(what)에 집중할 수 있어서, 시스템에 포함된 객체의 구성을 변경해 시스템 작동 방식을 쉽게 바꿀 수 있다. 값과 객체 시스템을 설계할 때는 값(value)과 객체(object)를 구분하는 것이 중요하다. 값은 변하지 않는 양이나 크기를 나타내며, 객체는 식별자를 가지고 시간이 지남에 따라 상태가 변할수도 있는 애들을 가리킨다. (DDD의 Value Object와 Reference Object) 대부분의 객체지향 언어에서는 이 두 개념을 모두 클래스라는 동일한 언어 구성물로 구현한다는 점에서 혼동의 여지가 있다 값(value) 양이 고정된 불변 인스턴스 개인 식별자가 없으므로 두 값 인스턴스의 상태가 같다면 사실상 동일한 셈이다 그러므로 두 값의 식별자를 비교하는 것은 적절하지 않다 string1 == string2 보단 string1.equals(string2) 를 쓰라고 하는 이유이다 객체(object) 변경 가능한 상태를 이용해 시간의 추이에 따른 객체의 행위를 나타낸다 두 객체 인스턴스의 상태가 정확히 동일하더라도 별개의 식별자를 가진다 즉, 식별자로 비교해야 한다 메시지를 따르라 객체를 설계할 때 다른 객체와 쉽게 관계를 맺을 수 있게 객체를 설계해야 한다 객체가 의사소통 패턴을 따르고, 객체간의 의존성이 명시적이어야 한다 의사소통 패턴은 다른 객체와 상호 작용하는 방법을 관장하는 각종 규칙으로 구성되어 있다 객체의 역할, 객체에서 전달 가능한 메세지, 전달 가능한 시점 등 의사소통 구조는 처음 객체를 배울때 느끼는 정적인 분류에서 개념적으로 굉장히 발전한 단계에 해당한다 객체간 메시지 주고받는 것에 더 집중한 방식이라고 생각된다 역할, 책임, 협력자 객체는 역할을 하나 이상 구현한 것이며, 책임은 어떤 과업을 수행하거나 정보를 알아야 할 의무를 말한다 협력은 객체나 역할(또는 둘 다)의 상호 작용에 해당한다 CRC 카드(또는 UML)를 이용해가며 객체를 모델링 해보는 것도 좋은 방법이다 묻지 말고 말하라(★) 서로간에 메시지를 전달하는 객체가 있다면, 서로 무슨 이야기를 할까? 객체를 호출할 땐 이웃 객체가 하는 역할 측면에서 해당 객체가 무엇을 원하는지 기술하고, 호출된 객체가 전달받은 바를 어떻게 실현할 지 결정한다 디미터의 법칙(Law of Demeter), 묻지 말고 말하라(Tell, Don’t Ask) 객체는 그것이 내부적으로 보유하고 있거나 메시지를 통해 확보한 정보만 가지고 의사결정을 내려야한다 객체가 다른 객체를 탐색해 뭔가를 일어나게 하면 안된다 호출자는 해당 객체의 내부 구조나 또는 그 너머에 존재하는 시스템의 구조에 대해 알 필요가 없다(알아서도 안된다) 이 스타일을 일관되게 따른다면 코드가 좀 더 유연해진다 같은 역할을 수행하는 객체를 손쉽게 교체할 수 있기 때문이다 12345678// 외부에서 물어서 직접 판단하지 말고master.getModelisable() .getDockablePanel() // 이러한 것을 .getCustomizer() // '열차 전복'이라고 한다 .getSavetem().setEnabled(Boolean.FALSE.booleanValue());// 해당 객체가 판단하게끔 하라 master.allowSavingOfCustomisations(); 이렇게 작성함으로써 추가적으로 얻는 이점은 아래와 같다 master를 이용하는 쪽에서 메서드를 해당 객체의 내부 구조까지 몰라도 된다 기존에는 연이어 호출하는 모든 객체의 타입까지 알고 있었다 만약 설계가 변경되어도, 이 코드에 미치는 영향이 작다 기존처럼 사용되는 곳이 많았다고 하면, 설계 변경이 미치는 영향이 매우 컸을것이다 명시적인 이름을 부여하였기 때문에 코드를 이해하기가 더 쉬워진다 그래도 가끔은 물어라 물론 모든 것만을 말하지만은 않는다 값과 컬렉션으로부터 정보를 가져오거나 팩토리를 이용해 새 객체를 생성할때(?)는 묻는다 검색이나 필터링을 할 때를 생각해보면 된다 그러나 이렇게 묻는 과정에서도 열차 전복은 피하게끔 작성해야 한다 123456789// 이렇게 내부 구조를 노출해서는 안된다 if(carriage.getSeats().getPercentReserved() &lt; persentReservedBarrier)&#123; request.reserveSeatsIn(carriage);&#125;// 정말로 내가 원하는 답을 주게끔 질문을 해야한다 if(carriage.hasSeatsAvailableWithin(persentReservedBarrier))&#123; request.reserveSeatsIn(carriage);&#125; 이렇게 작성하면 추가적으로, 이해하기 쉽고 테스트하기 쉬워진다는 장점을 얻을 수 있다 참고로 이런식의 질의(getter 포함) 메서드는 되도록 적게 쓰려고 하는 것이 좋다 질의가 객체 바깥으로 새어 나가서 시스템이 더 경직될 수 있기 때문이다 호출하는 객체의 의도를 서술하는 질의를 작성하려고 애써야한다 협력 객체의 단위 테스트 위의 디미터의 법칙을 지키다보면, assertion을 하나도 쓸 곳이 없어지는 것 같은 현상을 맞이하게 된다 (각 객체가 서로 명령을 전달하고, 상태를 질의하는 수단을 노출하지 않기 때문에) 호출되었을때 주위에 하나 또는 그 이상의 이웃객체에 메시지를 보내는 객체의 메서드가 있다고 한다면, 해당 메서드가 올바르게 수행되었는지 어떻게 테스트 할 수 있을까(내부 상태를 드러내지 않고)? 협력자가 잘 호출되었는지 mock으로 검증하라? SUT의 이웃(협력자)를 stub으로 대체하면 된다 실제로 SUT를 테스트할때는 협력자까지 같이 테스트 할 필요는 없기 때문이다. 협력자에 대한 테스트들은 협력자들이 직접 수행하게끔 하고, 우리는 stub에 예상 구문(expectation)을 작성하여 SUT의 테스트만 집중하게끔 해야한다(맞나?) 필요한 mock(stub) 객체 생성 mock(stub)을 포함한 실제 객체 생성 협력객체가 어떻게 호출될것인지를 기술(행위 검증을 하라는 의미인가?) SUT에서 협력객체가 포함된 메서드를 호출 예상되는 메서드 호출이 모두 일어났는지 확인 모든 테스트 의도를 명확하게 해서 테스트를 거친 기능과 보조 역할을 담당하는 기반 구조, 객체 구조를 분리하는 것?]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발로 배우는 객체지향 설계와 실천</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[tdd] 인수테스트, 단위테스트, 통합테스트, 전 구간 테스트]]></title>
    <url>%2Ftdd%2F%EC%9D%B8%EC%88%98%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%8B%A8%EC%9C%84%ED%85%8C%EC%8A%A4%ED%8A%B8-%ED%86%B5%ED%95%A9%ED%85%8C%EC%8A%A4%ED%8A%B8-%EC%A0%84-%EA%B5%AC%EA%B0%84-%ED%85%8C%EC%8A%A4%ED%8A%B8%2F</url>
    <content type="text"><![CDATA[테스트 주도 개발로 배우는 객체지향 설계와 실천 이라는 책과 Acceptance Test vs Integration Test 블로그의 글을 합쳐서 작성한 각 테스트들의 종류와 특징이다. 유닛 테스트(Unit Test) 가장 작은 단위의 테스트이다 보통 메서드 레벨이다 A라는 함수가 실행되면 B라는 결과가 나온다 정도로 테스트한다 즉각적인 피드백이 나온다는 것이 훌륭한 장점이다 꼭 메모리 내에서만 실행되는 테스트여야 한다는 법칙은 없다 데이터베이스, 네트워크 엑세스, 파일 시스템 등을 사용하여도 단위테스트의 레벨일 수 있다 테스트하기 어려운 부분은 stub을 사용하여 테스트한다 비용이 크지 않다면 stub보다는 실제 객체를 사용하는 것이 좋다 아무래도 정교한 목 객체가 실제 객체보다 정확하지는 않기 때문이다 모든 것은 비용 관점에서 생각해야 한다 하나의 메서드들이 잘 동작한다는 것은 보장할 수 있지만, 그들이 결합되었을때도 잘 작동한다는 것은 보장할 수 없다 전 구간 테스트(End-To-End Test) 해당 시스템과 해당 시스템을 구축하고 배포하는 프로세스를 모두 시험하는 것을 말한다 용어를 사용하는 곳마다 조금씩 차이가 있다 내부 기능들까지(클래스의 메서드들까지) 테스트 할 필요는 없다 이는 단위테스트의 영역이다 단점은 테스트를 만들기가 힘들고, 만든 테스트를 신뢰하기도 힘들다는 것이다 통합 테스트(Integration Test) 기본적으로 여러개를 통합해서 테스트 할때 사용하는데, 뭔가 정확한 용어가 정의된 느낌은 아니다 책에서는 변경할 수 없는 부분(외부 라이브러리 등)까지 묶어서 같이 테스트 할 떄 사용한다고 말하고 있다 사용하는 곳에서 어떻게 사용하는 가에 따라 다른 것 같다 그래서 난 사용하지 않을 것이다 인수 테스트(Acceptance Test) 단위 테스트, 전 구간 테스트, 통합 테스트와는 약간 scope 가 다르다 위의 3가지는 초점이 기술 쪽이라면, 인수 테스트는 초점이 비즈니스 쪽이다 구현하고자 하는 기능(비즈니스 레벨)에 대한 테스트이다 대체적으로 전 구간 테스트를 사용하여 기능을 테스트한다 그래서 이를 동일하게 얘기하는 곳도 많다 하지만 결과적으론, 위 3가지 모두 인수테스트의 범위에 들어갈 수 있다 보험금액을 계산하는 어플리케이션에서는 보험금액을 계산하는 메서드 단위 테스트가 인수테스트가 될 수 있다]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발로 배우는 객체지향 설계와 실천</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CRC card]]></title>
    <url>%2Fetc%2FCRC-card%2F</url>
    <content type="text"><![CDATA[https://uiandwe.tistory.com/465]]></content>
      <categories>
        <category>etc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[tdd] TDD의 핵심]]></title>
    <url>%2Ftdd%2FTDD%EC%9D%98-%ED%95%B5%EC%8B%AC%2F</url>
    <content type="text"><![CDATA[소프트웨어 개발은 학습의 과정이다 흥미로운 프로젝트(== 가장 큰 이익을 줄 만한 프로젝트)에는 예상치 못한 요소가 상당히 많다 갖가지 중요한 구성 요소가 조합된 시스템은 너무나 복잡해서, 개인이 해당 시스템의 모든 가능성을 이해하기는 어렵다 프로젝트에 관련된 모두는 프로젝트가 진행되면서 배우는 것이 있어야 한다 프로젝트에서 무엇을 달성해야 하는지? 잘못 이해하고 있는 바를 식별 해결하고자 협업해야 한다 피드백은 가장 기본적인 도구다 팀애서 취할 수 있는 가장 좋은 접근법은 경험에 의거한 피드백을 이용해 시스템과 그 용도에 관해 배운 다음, 이렇게 배운 바를 다시 시스템에 적용하는 것이다 그러므로 이러한 피드백을 얻을 수 있는, 반복적인 활동 주기가 필요하다 저자는 모든 개발에 초에서 월 단위에 이르는 중첩된 고리형 시스템의 피드백 주기를 적용했다 짝 프로그래밍, 단위 테스트, 인수테스트, 일별 회의, 반복 주기, 출시 등을 사용하여 고리를 구성한다 하나의 고리마다 산출물이 나오고, 이 산출물이 피드백으로 드러나게 된다 팀에서는 빠르게 오해를 발견하고 수정할 수 있다 이렇듯, 중첩된 피드백 고리는 서로를 강화한다 안쪽 고리는 기술적 세부 사항에 좀 더 집중한다 단위 코드의 역할과 시스템 나머지 부분과의 통합 여부 바깥쪽 고리는 조직과 팀에 좀 더 집중한다 사용자의 요구를 충족하는지, 팀이 효과적으로 운영되고 있는지 프로젝트의 어떠한 측면에 대해서도 피드백을 일찍 받을수록 좋다 TDD가 왜 중요할까? 시스템 규모를 믿을 수 있는 방식으로 키우고, 늘 일어나는 예상치 못한 변화에 대처하고 싶다면 두가지 기술적인 토대가 필요하다 테스트 자동화 시스템 규모와 상관없이 수동테스트를 자주 하는것은 비실용적이다 회기 오류를 잡아줄 꾸준한 테스트가 필요하다 기존 기능을 망가뜨리지 않고 새 기능을 추가할 수 있다 꾸준한 리팩토링 개발자들은 코드를 작성하는 것보다 코드를 읽는데 훨씬 더 시간을 많이 보낸다 그러므로 코드를 가능한 한 단순하게 유지해야 한다 단순함에는 노력이 많이 들어간다 설계 개선하고 단순화하며, 중복을 제거하며, 코드가 명확하게 자신의 역할을 표현하게끔 코드를 사용할때마다 꾸준히 리팩토링 해야한다 하지만 여기서 문제는, 대부분의 개발자들이 테스트 작성을 업무로 보지 않으며, 따분하다고 여기기 까지 한다는 것이다. 테스트 주도 개발은 이러한 상황을 근본적으로 뒤집는다. 작업을 완료한 후 작업 결과를 검증하려고 테스트를 작성하는 것이 아니라(Test Last), 코드를 작성하기 전에 테스트를 먼저 작성한다. 즉, 테스트 작성 자체가 설계 활동이 되는 것이다. 테스트를 먼저 작성함으로써 코드에서 하고 싶은바에 대한 생각을 명확하게 하고, 테스트를 먼저 작성하려는 노력으로 설계 아이디어의 품질에 대한 피드백도 빠르게 얻을 수 있게된다 코드를 테스트하기 쉽게 만들면 좀 더 깔끔하고 모듈화된 코드가 만들어지기 때문이다 TDD 간단 정리 TDD 작성 순서는 아래와 같다 다들 잘 아는 Red-Green-Refactor cycle이다. 테스트를 작성한다 프로덕션 코드가 없으므로 당연히 실패한다(Red) 해당 테스트가 동작하게(Green) 만든다 테스트는 건드리지 않고 프로덕션 코드를 리팩토링 한다 이를 반복한다 테스트를 먼저 작성함으로써 얻는 이점은 아래와 같다 다음 작업에 대한 인수 조건이 명확해진다 작업이 끝나는 시점을 스스로 알아내야 하기 때문이다 느슨하게 결합된 구성 요소를 작성할 수 있게 된다 격리된 상태에서, 더 높은 수준으로, 모두 결합된 상태로 구성 요소를 손쉽게 테스트 할 수 있다 코드가 하는 일에 대한 설명이 더해진다 테스트 코드는 일종의 스펙 문서 역할도 한다 완전한 회귀 스위트가 늘어난다 사실상 2,3,4 번은 Test First Development 보단 단순히 테스트 작성에서 얻을 수 있는 이점들이다. 하지만 언급했다시피, 테스트를 나중에 작성하는 방식은 힘들고, 현실적으로 위의 이점들을 다 가져가지 못할 가능성이 크다. 그리고 테스트를 실행하면 얻는 이점은 아래와 같다 컨텍스트를 선명하게 인지하는 동안 오류를 탐지한다(?) 언제 작업이 충분히 완료되었는지 알게된다 ‘금도긋’ 하듯 과도한 최적화를 하거나 불필요한 기능을 더하지 않게 된다 TDD의 황금률(?) 실패하는 테스트 없이는 새 기능을 작성하지 말라 좀 더 큰 그림 어플리케이션 내에 있는 클래스들을 대상으로 단위 테스트를 작성하는 것으로 TDD를 시작해보고 싶을 수 있으나, 결과적으로 단위 테스트만 있는 프로젝트는 TDD 프로세스가 주는 아주 중요한 혜텍을 놓치는 셈이 된다. 좀 더 큰 그림을 봐야한다. (물론 단위테스트가 없는 프로젝트 보다는 백배 천배 낫다) 기존의 TDD 라이프 사이클에서 앞단에 인수 테스트(Acceptance test) 작성 이라는 부분이 추가 되었다. (인수테스트-단위테스트-통합테스트-전-구간-테스트) 여기서는 전 구간 테스트를 사용하여 인수 테스트를 수행한다 그리고 TDD의 황금률에, 이 인수 테스트까지 같이해서 코드 작성을 시작한다. 실패하는 인수 테스트를 작성한다 이 테스트가 실패한다는 말은 아직까지 해당 기능을 구현하지 않았다는 것을 보여준다 단위 테스트들을 작성해나가며 기능을 완성해나간다 전 구간 테스트이기 때문에, 많은 단위 테스트들을 포함한다 TDD cycle을 사용하여 단위 기능들을 개발해나간다 인수테스트가 통과하면, 작업이 끝난다 이때서야 시스템이 배포될 수 있다 외부 품질과 내부 품질 외부 품질 시스템이 고객과 사용자의 요구를 얼마나 잘 충족하는가(기능, 신뢰성, 가용성, 응답성 등)이다 보통 계약의 일부라, 이를 이해하지 못할 사람은 없다 전 구간 테스트를 사용한다 전 구간 테스트로는 코드를 얼마나 잘 작성했는지는 알 수 없다 내부 품질 시스템이 개발자와 관리자의 요구를 얼마나 잘 충족하는가(이해하기 쉬운가, 변경하기 쉬운가) 이다 외부 품질과 똑같이 중요하지만, 달성하기는 많이 어렵다 단위 테스트를 사용한다 단위 테스트로는 시스템이 전체적으로 동작하는지를 충분히 확신할 수 없다 높은 내부 품질은 어떻게 달성할 수 있을까? 높은 응집과 낮은 결합을 유지해야한다 코드의 동작 방식을 얼마나 쉽게 바꿀 수 있는지를 설명하는 척도이기 때문이다 한 요소의 변경이 다른 요소의 변경에 영향을 미친다면, 그 두 요소는 결합되어 있는 것이다. 이를 최대한 낮게 유지해야 한다 응집도는 해당 요소의 책임이 의미있는 단위를 형성하는지 나타내는 척도이다 날짜와 URL을 파싱하는 기능이 같이 들어간 클래스는 응집도가 낮다고 표현할 수 있다 그리고, 두 가지 기능을 다 잘할 가능성이 낮다 객체에 대한 단위테스트를 많이 해야한다 객체를 생성하고, 객체의 의존성을 제공하며, 객체와 상호 작용하고, 예상대로 동작하는지 검사할 필요가 있다 설계를 잘못하면 단위 테스트를 작성하거나 이해하기 어렵다 예를 들면 클래스가 멀리 떨어져있는 시스템의 일부와 긴밀하게 결합되어 있거나, 암시적인 의존성이 있거나, 불분명한 책임이 너무 많을때 등이 있다 결과적으로 높은 응집과 낮은 결합을 가진 객체가 많아야 높은 내부 품질을 유지할 수 있게 되는데, 테스트 작성이 이를 측정하는 기준점이 될 수 있다. TDD는 테스트를 먼저 작성함으로써, 이러한 설계에 관한 즉각적인 피드백을 바로 얻을 수 있다는 큰 장점이 있다.]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>테스트 주도 개발로 배우는 객체지향 설계와 실천</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] spring batch AsyncTaskExecutor 등록]]></title>
    <url>%2Fspring%2Fspring-batch-AsyncTaskExecutor-%EB%93%B1%EB%A1%9D%2F</url>
    <content type="text"><![CDATA[Job들을 비동기로 실행시키고 싶을 경우 사용한다. 아래와 같이 빈으로 등록한다 123456789101112131415161718192021@RequiredArgsConstructor@Configurationclass JobConfiguration &#123; private final JobRepository jobRepository; @Bean public JobLauncher asyncJobLauncher()&#123; // 메서드명이 빈 이름이 된다 SimpleJobLauncher simpleJobLauncher = new SimpleJobLauncher(); simpleJobLauncher.setJobRepository(jobRepository); simpleJobLauncher.setTaskExecutor(asyncTestExecutor()); return simpleJobLauncher; &#125; @Bean public TaskExecutor asyncTestExecutor()&#123; SimpleAsyncTaskExecutor asyncTaskExecutor = new SimpleAsyncTaskExecutor(); asyncTaskExecutor.setConcurrencyLimit(5); return asyncTaskExecutor; &#125;&#125; 메서드의 이름이 bean의 이름이 된다. 사용하는 곳은 아래와 같다 123456789@Serviceclass SomeService &#123; private final JobLauncher asyncJobLauncher; // 스프링은 타입 scan -&gt; 이름 scan을 한다 private final Job someJob; public void doSomething()&#123; jobLauncher.launch(someJob); &#125;&#125; 스프링은 타입 injection을 먼저 수행하고, 똑같은 타입이 있으면 이름으로 injection을 수행한다. 만약 JobLauncher가 asyncJobLauncher 외에 더 등록되어 있다면 injection에서 오류가 발생할테니 위처럼 이름을 지정해줘야 한다.]]></content>
      <categories>
        <category>spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[tdd] 상태검증과 행위검증, stub과 mock 차이]]></title>
    <url>%2Ftdd%2F%EC%83%81%ED%83%9C%EA%B2%80%EC%A6%9D%EA%B3%BC-%ED%96%89%EC%9C%84%EA%B2%80%EC%A6%9D-stub%EA%B3%BC-mock-%EC%B0%A8%EC%9D%B4%2F</url>
    <content type="text"><![CDATA[SUT(System Under Test) : 주요 객체(primary object) 협력객체(collaborator) : 부차적 객체(secondary objects) 테스트 더블(Test Double) : 테스팅을 목적으로 진짜 객체대신 사용되는 모든 종류의 위장 객체 Dummy, Fake Object, Stub, Mock 상태검증 vs 행위검증 https://minslovey.tistory.com/97 상태검증은 메서드가 수행된 후 SUT나 협력객체의 상태를 살펴봄으로써 올바로 동작했는지를 판단하게 된다 1234SomeClass someClass = new SomeClass();someClass.someMethod();assertThat(someMethod.someStatus()).isEqualTo(true); someStatus값이 true인지 상태 검사 행위검증은 상태검증과는 다르게 SUT가 협력객체의 특정 메서드가 호출되었지 등의 행위를 검사함으로써 올바로 동작했는지 판단하게 된다 123SomeClass someClass = new SomeClass();verify(someClass).someMethod(); someClass의 someMethod가 실행되었는지 행위 검사 stub vs mock http://testing.jabberstory.net/ 많은 테스트 더블들이 있지만, 테스트 더블들의 역할이 딱딱 나뉘어져 있지도 않고, 서로가 서로의 특성을 조금씩 포함하므로, 대표적으로 stub과 mock만을 구분한다. stub 호출이되면 미리 준비된 답변으로 응답하는 것 테스트시에 프로그램된 것 외에는 응답하지 않는다 협력객체의 특정 부분이 테스트하기 힘들 경우 stub을 사용하면 수월하게 테스트할 수 있다 일반적으로 우리가 mock으로 잘못 알고있다 mock 다른 테스트더블과는 다르게 행위검증 사용을 추구한다 행위를 기록하는 식의 로직이 들어가있겠지… SUT가 실제 협력객체와 대화하고 있다고 믿게해야하므로, 모든 테스트 더블들의 동작이나 형태는 같다. 하지만 여기서 mock은, 행위검증을 추구한다는 것 자체가 다른 테스트 더블과의 큰 차이점이다. classicist, mockist classicist들은 가능하면 항상 진짜 객체를 사용하고, 진짜 객체를 사용하기 만만치 않으면 더블을 사용한다 그리고 항상 상태검증을 사용하려고 한다 mockist들은 관심있는 행위를 가진 모든 객체에 모의객체를 사용하려고 한다 그리고 항상 행위검증을 사용하려고 한다 향 간단한 협력과 간단하지 않은 협력에서의 둘의 선택 간단한 협력 classicist라면 실제 객체를 사용해 상태검증을 할 것이다 mockist라면 mock을 쓰고 행위검증을 할 것이다 간단하지 않은 협력 mockist라면 당연히 mock을 쓰고 행위검증을 사용한다. 간단하지 않은 협력에서 mock의 장점이 부각(?)되기 때문이다 classicist라면 상황에 맞춰 가장 쉬운 방법을 사용하려 한다. 테스트 더블이 필요하다면 테스트 더블을 사용할 것이다. 언제 상태검증? 언제 행위검증? 현재까지 이해한 바로 작성해보면, 행위검증의 경우 특정 메서드의 호출과 같은 것을 검증하기 때문에, 구현에 굉장히 의존적이게 된다 이 말인 즉 프로덕션 코드가 변경되면 테스트코드가 변경될 확률이 높아진다는 것을 의미한다 테스트는 그것을 작성하면서 설계에 피드백을 받을 수 있다는 장점이 있는데(테스트가 어렵다면 설계에 문제가 있는것은 아닌지 의심해보는 과정) 행위검증의 경우 상태검증보다 테스트 작성이 쉬워서 이런 피드백을 받을 기회(?)가 많이 없어지게 된다 강력한 행위검증 라이브러리(powermock)같은 것을 쓰게 되면 이런 부분을 놓치게 될 가능성이 더 크다 예를 들어 powermock의 경우 private 메서드도 테스트할 수 있고, static 클래스도 주입할 수 있는 등 강력한데, 이러한 특징때문에 우리는 설계가 잘못되었다는 의심을 하지 않고 지나갈 확률이 커진다 상태검증의 경우 상태를 검증하기 위해 상태를 노출하는 메서드가 많이 추가될 수 있다 mockist들은 이것을 큰 요소로 여긴다 이러한 이유로 대체적이면 상태검증을 사용하는것이 좋다 하지만 상태검증을 하기 힘든 경우들이 종종 있다 예를 들면 알람 같은 것이다 이런 테스트의 경우, 통합 테스트를 고려해보는 것도 좋다 (알람을 쏘고 로그를 읽어와서 상태검증을 할 수도 있다) 근데 또 이게 단점이 많은게, 전체적인 테스트 시간이 늘어나고(피드백을 받는 시간이 늘어난다), 문제가 발생했을때 찾기가 쉽지 않아진다 SUT가 아닌 다른곳에 의존성이 생길수 있다 이러한 이유 때문에 행위검증을 선택하는 경우도 종종 있다 구현에 의존적이라는 행위검증의 특징을 안고 가는것이 좀 별로긴 하지만, mockito같은 라이브러리를 적절히 사용하고, 테스트를 잘 구성하면 이러한 단점도 어느정도 커버할 수 있다고 한다]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>TDD</tag>
        <tag>상태검증</tag>
        <tag>행위검증</tag>
        <tag>stub</tag>
        <tag>mock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] interface에 static method]]></title>
    <url>%2Fjava%2Finterface%EC%97%90-static-method%2F</url>
    <content type="text"><![CDATA[interface에 선언한 static method는 일반적으로 우리가 정의하는 메서드와는 다르다. body가 있어야 한다 implements 한곳에서 override가 불가능하다 1234567interface SomeInterface &#123; public static String doSomething()&#123; // blah blah &#125; public String doNothing(); // have to override&#125; 마치 java8의 default method와 약간 비슷하다. 하지만 static method는 override가 불가능하다는 것이 특징이다.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>interface static method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[linux] port로 process 찾기]]></title>
    <url>%2Flinux%2Fport%EB%A1%9C-process-%EC%B0%BE%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[https://stackoverflow.com/questions/3855127/find-and-kill-process-locking-port-3000-on-mac mac, centos7 에서 아래의 명령어로 찾을 수 있다고 함 1netstat -vanp tcp | grep 9999 뒤에서 4번째 컬럼이 process 번호이다]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[db] mysql group_concat]]></title>
    <url>%2Fdb%2Fmysql-group-concat%2F</url>
    <content type="text"><![CDATA[그룹화된 컬럼을 쭉 나열할 때 쓸 수 있는 함수이다 1select team, group_concat(id) from user group by team; 1234A_team | 1,3,5B_team | 2,4,6C_team | 7,8,9... 기본 구분자는 ,이고, 변경 가능하다 1select team, group_concat(id separator '|') from user group by team; 앞뒤로 문자를 붙일수도 있다 12345-- 1__, 2__, 3__select team, group_concat(id, '__') from user group by team;-- __1__, __2__, __3__ select team, group_concat('__', id, '__') from user group by team;]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>group_concat separator</tag>
        <tag>group_concat 구분자</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[임시변수 내용 직접 삽입]]></title>
    <url>%2Frefactoring%2F%EC%9E%84%EC%8B%9C%EB%B3%80%EC%88%98-%EB%82%B4%EC%9A%A9-%EC%A7%81%EC%A0%91-%EC%82%BD%EC%9E%85%2F</url>
    <content type="text"><![CDATA[참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[임시변수를 메서드 호출로 전환]]></title>
    <url>%2Frefactoring%2F%EC%9E%84%EC%8B%9C%EB%B3%80%EC%88%98%EB%A5%BC-%EB%A9%94%EC%84%9C%EB%93%9C-%ED%98%B8%EC%B6%9C%EB%A1%9C-%EC%A0%84%ED%99%98%2F</url>
    <content type="text"><![CDATA[수식의 결과를 저장하는 임시변수가 있을 땐, 그 수식을 빼내어 메서드로 만든 후, 임시변수 참조 부분을 전부 수식으로 교체하자 새로 만든 메서드는 다른 메서드에서도 호출 가능하다 특징 메서드 추출을 적용하기 전에 적용해야 한다 지역변수가 많을수록 메서드 추출이 힘들어지기 때문이다 임시변수를 메서드 호출로 수정하면 클래스 안 모든 메서드가 그 정보에 접근할 수 있다 적용하는 가장 간단한 상황은 임시변수에 값이 한번만 대입되고 대입문을 이루는 수식에 문제가 없을 때이다 방법 값이 한번만 대입되는 임시변수를 찾는다 값이 여러변 대입되는 임시변수가 있으면 임시변수 분리를 적용한다 그 임시변수를 final로 선언한다 정말로 임시변수들이 값을 한번만 대입받는지 시험해볼수 있다 대입문 우변을 빼내어 메서드로 만든다 처음에는 private 으로 만들고, 나중에 더 여러곳에서 사용하게 되면 접근제한을 완화한다 추출 메서드에서 문제가 없는지(객체의 값을 변경한다거나) 확인한다 임시변수를 대상으로 임시변수 내용 직접 삽입을 적용한다 예시 임시변수의 값을 루프를 돌면서 변경하는 경우가 많다 이럴떈 먼저 루프 자체를 메서드로 변경하고, 메서드 수행 결과를 임시변수에 담아 사용한다 이 임시변수를 전부 메서드 호출로 변경한다 여기서 성능이 느려질수도 있지만, 대체로 문제없다 리팩토링을 잘 할수록 더욱 강력한 최적화가 가능하기 때문이다 특정 로직의 메서드를 추출하려고 했는데 사용하는 임시변수가 많을 경우, 각 임시변수들에 대해 이 기법을 적용해나가면서 점진적으로 메서드를 분리해낼 수 있다 1234567891011int getPrice()&#123; int originalPrice = quantity * itemPrice; // 구매하고 싶은 개수 * 상품 가격 double discountFactor = 1.0; if(originalPrice &gt; 100000) discountFactor = 0.8; else if(originalPrice &gt; 50000) discountFactor = 0.9; return originalPrice * discountFactor;&#125; 이 상태에서 discountFactor 구하는 부분을 분리하려면 originalPrice가 전달되어야 하므로, originalPrice 먼저 분리하고, 이 임시변수를 전부 메서드 호출로 변경한다 1234567891011121314int getPrice()&#123; double discountFactor = 1.0; if(getOriginalPrice() &gt; 100000) discountFactor = 0.8; else if(getOriginalPrice() &gt; 50000) discountFactor = 0.9; return getOriginalPrice() * discountFactor;&#125;int getOriginalPrice()&#123; return quantity * itemPrice;&#125; 이후 discountFactor 구하는 부분을 손쉽게 분리할 수 있다 12345678910111213141516int getPrice()&#123; return getOriginalPrice() * getDiscountFactor();&#125;int getDiscountFactor()&#123; if(getOriginalPrice() &gt; 100000) return 0.8; else if(getOriginalPrice() &gt; 50000) return 0.9; else return 1.0;&#125;int getOriginalPrice()&#123; return quantity * itemPrice;&#125; 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>replace temp with query</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] string array to character array]]></title>
    <url>%2Fjava%2Fstring-array-to-character-array%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[java] java list array 변환]]></title>
    <url>%2Fjava%2Fjava-list-array-%EB%B3%80%ED%99%98%2F</url>
    <content type="text"><![CDATA[list -&gt; array 1list.toArray(new String[0]); toArray의 인자로 변환하고 싶은 array 타입의 변수를 성생해주면 된다. java 1.6 이전에는 인자로 생성하고 싶은 array 개수만큼 사이즈를 주는게 좋았으나, 1.6 이후로는 0으로 주나 new String[list.size()] 하나 동일하다고 한다 stream -&gt; array 1Stream.toArray(String[]::new)]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[intellij 기능 간단 정리]]></title>
    <url>%2Fetc%2Fintellij-%EA%B8%B0%EB%8A%A5-%EA%B0%84%EB%8B%A8-%EC%A0%95%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[maven, gradle group id = 그룹 아이디, 예를 들면 스프링 artifact id = 모듈 이름, 예를 들면 스프링 시큐리티, 스프링 MVC 액션 검색 : meta + shift + a 새로 만들기 : meta + n 현재포커스 실행 : ctrl + shift + r 이전포커스 실행 : ctrl + r 라이브 템플릿 메인메서드 : psvm System.out.println : sout 라인 복제하기 : meta + d 라인 삭제하기 : meta + delete 문자열 라인 합치기 : ctrl + shift + j 라인 단위로 옮기기 : 문법 상관하면서 : meta + shift + up/down 문법 상관없이 : option + shift + up/down element 단위로 옮기기(html, xml 속성, 메서드 매개변수 순서 등등) : meta + option + shift + left/right 파라미터 즉시보기 : meta + p 코드 구현부 즉시보기 : option + space docs 보기 : f1 단어별 이동 : alt + 좌우(선택 : +shift) 라인 첫/끝 : fn + 좌우(선택 : +shift) page up/down : fn + 위아래 포커스 범위(선택) 한 단계씩 늘리기 : alt + 위아래 포커스 앞/뒤 : meta + [/] 멀티포커스 : alt + alt + 위/아래(리눅스는 ctrl) 오류라인 자동 포커스 : f2 리팩토링 변수추출 똑같은 값들을 하나의 변수로 추출하는 과정 추출하고자 하는 값을 선택한 뒤, command + option + v 파라미터 추출 command + option + p 변수가 아니라 파라미터로 추출된다 extract via overloading method를 사용하면 추출된 메서드를 메서드 추출 추출하고 싶은 만큼 코드를 선택한 다음 command + option + m 이너클래스 추출 이너클래스가 여러군데서 사용될 떄 외부클래스로 추출할 수 있다 f6 번을 누르면 어떻게 이동시킬지 선택할 수 있는 부분이 나오고, 여기서 이동할 패키지를 지정해주면 깔끔하게 클래스가 이동된다 이름 일괄 변경 shift + f6 변수 이름 외에, 메서드 이름, 클래스 이름 모두 적용 가능 타입 일괄변경 파라미터 리턴 타입에 마우스대고 cmd + shift + 6 반환하는 값에 대해서는 자동 변환시키거나 직접 설정가능 사용하지 않는 import 제거 ctrl + option + o command shitf a + optimize import 부분을 off -&gt; on으로 바꾸면 자동으로 사용하지 않는 import를 정리해준다 파일을 열떄마다 자동으로 사라지게 해준다 이 기능을 사용하면 import문을 * 으로 변경하는 경우가 많은데, action -&gt; import with * 의 개수를 999로 설정하면 해결된다 정렬되지 않은 코드 정렬 command + option + l]]></content>
      <categories>
        <category>etc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[메서드 내용 직접 삽입]]></title>
    <url>%2Frefactoring%2F%EB%A9%94%EC%84%9C%EB%93%9C-%EB%82%B4%EC%9A%A9-%EC%A7%81%EC%A0%91-%EC%82%BD%EC%9E%85%2F</url>
    <content type="text"><![CDATA[메서드 기능이 너무 단순해서 메서드명만 봐도 너무 뻔할 땐, 그 메서드의 기능을 호출하는 메서드에 넣어버리고 그 메서드는 삭제하자 특징 메서드명에 모든 기능이 반영될 정도로 메서드 기능이 지나치게 단순하면, 그 메서드는 없애야한다 단순 위임 기능을 하는 메서드들 때문에 코드가 복잡해질 수 있다 리팩토링의 핵심은 한눈에 파악할 수 있는 직관적인 메서드를 만드는 것과 메서드를 간결하게 만드는 것이긴 하지만! 잘못 쪼개진 메서드에도 적용할 수 있다 잘못 쪼개진 메서드의 내용을 다시 큰 메서드에 직접 삽입한 후, 다시 작은 메서드로 추출한다 방법 메서드가 재정의 되어있지 않는지 확인하자 당연한 소리지만 하위클래스에서 재정의하고 있는 메서드를 삭제하면 안된다 그 메서드를 호출하는 부분을 모두 찾는다 각 호출 부분을 메서드 내용으로 교체한다 테스트를 실시하고, 메서드 정의를 삭제한다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[메서드 추출]]></title>
    <url>%2Frefactoring%2F%EB%A9%94%EC%84%9C%EB%93%9C-%EC%B6%94%EC%B6%9C%2F</url>
    <content type="text"><![CDATA[어떤 코드를 그룹으로 묶어도 되겠다고 판단되면, 그 코드를 뺴내어 목적을 잘 나타내는 직관적 이름의 메서드로 만든다 특징 매우 자주 사용됨 메서드가 너무 길거나 코드에 주석을 달아야만 의도를 이해할 수 있을때 사용 메서드가 적절히 잘게 쪼개져있으면 다른 메서드에서 사용하기 수월함 효과를 보려면 메서드의 이름도 잘 지어야 한다 그리고 쪼개는 것도 잘 해야겠지… 명확하게 메서드명과 메서드 내용의 의미적 차이가 중요하다 명료하기만 하면 메서드 길이가 메서드명보다 짧아도된다 방법 목적에 부합하는 새 이름의 메서드를 생성한다 메서드명은 원리가 아니라 기능을 나타내야 한다. 기존 메서드에서 빼낸 코드를 새로 생성한 메서드로 복사한다 뺴낸 코드에서 기존 메서드의 모든 지역변수 참조를 찾는다 새로 생성한 메서드의 지역변수나 매개변수로 활용할 것이다 추출한 메서드에서 사용되는 임시변수에 대해 처리한다 원본 메서드에서 뺴낸 코드 부분을 새로 생성한 메서드 호출로 수정한다 지역변수 처리 추출한 메서드에서만 사용되는 값일 경우 해당 메서드로 임시변수를 분리 원본 메서드, 추출한 메서드 양쪽에서 다 사용되는 지역변수인데 추출한 메서드에서는 읽히기만 할 경우 추출한 메서드에 매개변수로 전달 원본 메서드, 추출한 메서드 양쪽에서 다 사용되는 지역변수이고 추출한 메서드에서 값이 변경될 경우 변경되는 지역변수가 1개일 경우 리턴해주는 형식으로 작성 가능 12345int a = 0;// do something to `a`print(a); 는 아래처럼 변경 가능 12int a = doSomething(0); // 필요한 값을 매개변수로 전달해줄 수 있음 print(a); 2개 이상의 변수가 변경되는 경우, 메서드가 하나의 값을 반환하도록 더욱 분리해주는 것이 좋다. 가능하면 하나의 값만 반환하는 것이 좋다고 한다. 임시변수가 너무 많으면 임시 변수를 메서드 호출로 전환 같은 것을 사용해서 임시변수의 수를 줄이는 것이 좋다. 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>extract methodc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[코드의 구린내와 해결법]]></title>
    <url>%2Frefactoring%2F%EC%BD%94%EB%93%9C%EC%9D%98-%EA%B5%AC%EB%A6%B0%EB%82%B4%EC%99%80-%ED%95%B4%EA%B2%B0%EB%B2%95%2F</url>
    <content type="text"><![CDATA[리팩토링의 기본 한 클래스에서 다른 클래스로 상태와 기능을 옮기는 것은 리팩토링의 기본이다 지금은 합리적이라고 판단되는 설계도 나중에는 그렇지 않을 수 있다 문제는 이게 아니라, 이 상황에도 아무런 대처도 하지 않는 것이다 중복 코드(Duplicated Code) 최대의 구린내는 누가 뭐라해도 중복 코드이다 코드가 중복되면 한쪽만 수정하고, 다른 한쪽은 수정하지 않는 위험천만한 실수를 저지르기 쉽다 한 클래스의 두 메서드안에 같은 코드가 들어있는 경우 메서드 추출을 적용해서 겹치는 코드를 별도의 메서드로 분리하고, 그 메서드를 두 곳에서 호출하면 된다 한 클래스의 두 하위클래스에 같은 코드가 들어있는 경우 메서드 추출을 적용해서 중복을 없앤 후, 메서드 상향을 적용하면 된다. 두 메서드의 코드가 똑같지 않고 비슷하다면? 메서드 추출을 적용해서 같은 부분과 다른 부분을 분리해야 한다 경우에 따라 템플릿 메서드 형성을 적용해야 할 수도 있다 두 메서드의 기능이 같은데 알고리즘만 다르다면? 두 알고리즘 중 더 간단한 것을 택해서 알고리즘 전환을 적용하면 된다 중복코드가 메서드 가운데에 있는 경우 주변 메서드 추출을 적용한다 서로 상관없는 두 클래스안에 중복 코드가 있을 경우 중복코드를 클래스 추출이나 모듈 추출을 적용해서 클래스나 모듈로 떼어낸 후, 그것을 호출한다 두 클래스 중 한쪽에서 다른쪽을 호출할수도 있고, 제 3의 클래스로 추출하고 양쪽에서 호출할수도 있다 장황한 메서드 메서드가 길수록 이해하기 어렵기 때문에 메서드들은 작은 단위로 쪼개주는 것이 좋다 (최적의 상태로 장수하는 객체 프로그램들을 보면 공통적으로 메서드 길이가 짧다) 하지만 하나의 덩어리가 여러개로 나뉘어졌기 때문에, 읽으려면 화면간 전환이 생기기 되고, 이로 인해 사람의 머릿속에 오버헤드가 생기게 된다 그러므로 메서드의 기능을 한눈에 알 수 있는 메서드명을 사용하여 그 메서드 안의 코드를 분석하지 않아도 되게끔 해야한다 메서드명은 기능 수행 방식이 아니라 목적(기능 그 자체)를 나타내는 이름으로 정해야한다 메서드 호출이 원래 코드보다 길어지는 한이 있더라도, 메서드명은 그 코드의 의도를 잘 반영하는 것으로 정해야 한다 메서드의 크기를 줄이려면 십중팔구는 메서드 추출 기법을 적용해야 한다 메서드에서 하나로 묶으면 좋을 만한 부분들을 찾아서 메서드로 만든다 주석을 참고하는 것도 방법이다 주석으로 처리된 코드 구간을 메서드로 만든다 주석에 설명된 기능을 참고해서 메서드명을 짓는다 메서드에 매개변수와 임시변수가 많을 경우 임시 변수를 메서드 호출로 전환이나 임시변수를 메서드 체인으로 전환을 사용하면 대부분의 임시변수는 제거된다 길게 열거된 매개변수는 매개변수 세트를 객체로 전환과 객체를 통째로 전달을 적용하면 간결해진다 위의 기법을 적용했음에도 불구하고 여전히 임시변수가 너무 많을 때는 메서드를 메서드 객체로 전환을 적용한다 조건문과 루프도 메서드로 빼야 한다 조건문을 추출하려면 조건문 쪼개기를 적용한다 루프를 추출하려면 루프를 컬렉션 클로저 메서드로 전환을 적용한 후, 그 클로저 메서드 호출과 클로저 자체에 메서드 추출을 적용하면 된다 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>중복 코드</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] java8 날짜/시간 api]]></title>
    <url>%2Fjava%2Fjava8-%EB%82%A0%EC%A7%9C-%EC%8B%9C%EA%B0%84-api%2F</url>
    <content type="text"><![CDATA[UTC : https://ko.wikipedia.org/wiki/협정_세계시 ISO 8601, 표기법 : https://ohgyun.com/416 기존 자바 시간 API의 문제 java의 기존 Date 에 문제가 많았음 불변 객체가 아니다 상수를 남용한다(e.g. 월 파라미터에 1~12가 아닌 값이 들어가도 문제없음. 상수이기 때문이다) 월이 상수 0부터 시작한다 그래서 Calendar가 나왔는데, 여전히 문제가 있음 대부분의 날짜 유틸이 Date 위주임 Date로 변하기 위한 중간객체 수준밖에 안됨(생성 비용도 비쌈) 0 부터 시작하는 월 은 변경되지 않음 좋은 API는 오용하기 어려워야 하고, 문서가 없어도 쉽게 사용할 수 있어야 한다 그러나 Java의 기본 API는 문서를 열심히 보기 전까지는 제대로 사용하기 어렵다 그래서 jodaTime을 대부분 많이 사용했는데, 자바 8 부터 jodaTime의 유용한 기능들을 java.time 패키지에 넣어서 새로 배포함 이와 관련된 자세한 내용은 아래의 글에 나와있다 https://d2.naver.com/helloworld/645609 java 8 시간 API 상세하게 설명되어 있는 블로그 https://perfectacle.github.io/2018/09/26/java8-date-time/ LocalDate, LocalTime, LocalDateTime Timezone을 가지지 않는 시간 여기서 불변객체라 함은 setter 등으로 변경할 수 없는 객체를 말한다 LocalDate는 날짜를 표현하는 불변객체 LocalTime은 시간을 표현하는 불변객체 LocalDateTime은 둘을 합쳐서 표현 of static method로 연,월,일,시간 등을 받아서 생성할 수 있음 now 메서드로 현재 시간 생성 가능 parse로 문자열을 그대로 받아 사용 가능. 여기에는 DateTimeFormatter 인스턴스를 전달할 수도 있음 ZonedDateTime http://www.daleseo.com/java8-zoned-date-time/ LocalDateTime에 타임존이나 시차가 추가되었다고 보면 된다 ZonedId나 ZonedOffset 값을 주면 타임존이나 시차를 적용할 수 있고, 값을 주지 않을 경우 로컬의 기본 타임존 값을 사용한다. ZonedId 이 부모 클래스, ZonedOffset, ZonedRegion 이 하위 클래스임 시차를 쓰는 곳은 많지않고, ZonedOffset 같은 경우 Summer time 등을 처리하지 못하므로 ZonedId를 쓰는 것이 좋다 ZoneId seoulZone = ZoneId.of(&quot;Seould/Asia&quot;) LocalDate, LocalDateTime, Instant를 ZonedDateTime으로 변환할 수 있다 12345678LocalDate localDate = LocalDate.now();ZonedDateTime zd1 = localDate.atStartOfDay(seoulZone);LocalDateTime localDateTime = LocalDateTime.now();ZonedDateTime zd2 = localDateTime.atZone(seoulZone);Instant instant = Instant.now();ZonedDateTime zd3 = instant.atZone(seoulZone); 이것보다는 OffsetDateTime이 더 선호된다고 함 Instant 컴퓨터가 알아보기 쉽게 표현하기 위한 형태이다 유닉스 에포크 시간(1970년 1월 1일 0시 0분 0초 UTC)를 기준으로 특정 지점까지를 초로 표현한 것이다. 나노초(10억분의 1)까지 표현 가능하다 ZonedId를 이용해서 LocalDateTime을 Instant로 바꿀 수 있다 1Instant instant = LocalDateTime.now().toInstant(seoulZone); Period, Duration instant 자르기 https://stackoverflow.com/questions/27813691/how-to-compare-two-instant-based-on-the-date-not-time]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java date</tag>
        <tag>LocalDate</tag>
        <tag>LocalTime</tag>
        <tag>LocalDateTime</tag>
        <tag>ZonedDateTime</tag>
        <tag>Instant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] Spring Batch]]></title>
    <url>%2Fspring%2FSpring-Batch%2F</url>
    <content type="text"><![CDATA[스프링 배치 설명 https://jojoldu.tistory.com/324?category=635883 https://jojoldu.tistory.com/325?category=635883 https://jojoldu.tistory.com/326?category=635883 배치서비스의 특징 대용량 데이터 배치 어플리케이션은 대량의 데이터를 가져오거나, 전달하거나, 계산하는 등의 처리를 할 수 ​​있어야 합니다. 자동화 배치 어플리케이션은 심각한 문제 해결을 제외하고는 사용자 개입 없이 실행되어야 합니다. 견고성 배치 어플리케이션은 잘못된 데이터를 충돌/중단 없이 처리할 수 있어야 합니다. 신뢰성 배치 어플리케이션은 무엇이 잘못되었는지를 추적할 수 있어야 합니다. (로깅, 알림) 성능 배치 어플리케이션은 지정한 시간 안에 처리를 완료하거나 동시에 실행되는 다른 어플리케이션을 방해하지 않도록 수행되어야합니다. Spring Batch는 Acceuture의 배치 프레임워크를 추상화 한것이다. Spring의 3대 요소인 DI, AOP, 서비스 추상화를 다 사용가능하다. Spring Quartz는 스케줄러이고, Spring Batch는 대용량 처리 배치이다. 둘은 완전히 다르다. 보통 Quartz + Batch를 조합해서 사용한다. Quartz가 Batch를 실행시키는 구조이다. Spring Batch 를 사용하고 싶으면 스프링 부트 어플리케이션 루트에 @EnableBatchProcessing 어노테이션을 써줘야함 Job은 하나의 배치 작업 단위를 뜻함 JOB은 @Configuration에 등록하고, 서비스가 뜨면 실행된다 1234567@Beanpublic Job simpleJob()&#123; return jobBuilderFactory.get("simple-job") .start(simpleStep1(null)) .next(simpleStep2(null)) .build();&#125; 등록한 Batch Job들을 관리하려면 Spring Batch에서 정의한 메타 테이블들을 사용해야 한다 배치와 관련된 전반적인 데이터를 관리한다 spring batch와 같이 다운되는 schema-XXX.sql 파일에 스키마가 들어있다 Job 안에 여러 Step이 존재함 Step이 실제 배치작업(비즈니스 로직)이 들어있는 곳이다 말 그대로 진짜 스텝이다. job이 실행해야 할 step을 작성하는 것이다. job 을 큰 작업, step 을 세부 작업이라고 생각하면 안된다 ‘job은 일괄 등록 요청, 요청 개수 만큼 step 생성’ 은 잘못된 생각이다 Step 안에 Tasklet 또는 Reader &amp; Processor &amp; Writer 묶음이 존재함 진짜 비즈니스 로직을 작성하는 곳 둘은 같은 레벨이므로 하나만 실행가능 Reader &amp; Processing 후에 Tasklet 불가능 Job, Step, Tasklet(ItemReader, ItemWriter 등) 은 전부 스프링 빈이다 지정한 JOB만 실행되도록 하는 방법 application.yml에 spring.batch.job.names: ${job.name:NONE} 를 추가하고, 외부 파라미터로 넘어오는 job.name의 값에 맞춰 배치를 실행한다. 전달되지 않으면 NONE에 의해 아무 배치도 실행하지 않는다. parameter에 --job.name=stepNextJob 의 형태로 입력하면 된다 Spring Batch Metadata Table JOB은 파라미터에 따라 BATCH_JOB_INSTACNE에 저장됨 JOB들의 실행을 관리하는 BATCH_JOB_EXECUTION JOB들의 실제 로직들인 STEP이 저장된 BATCH_STEP_EXECUTION BATCH_JOB_INSTANCE 등록한 JOB들에 대한 인스턴스들인데, 외부에서 전달한 파라미터에 따라 생성된다 파라미터가 같으면 중복해서 생성되지 않으며, 파라미터가 다를 때만 생성된다 job이나 step에서 파라미터를 꼭 받아야 하는것은 아니다 이 파라미터들은 BATCH_JON_PARAMETERS에 저장된다. 근데 왜 BATCH_JOB_INSTANCE와 관계를 가지지 않고 BATCH_JOB_EXECUTION과 관계를 가질까? BATCH_JOB_EXECUTION JOB_INSTACNE의 실행 내역을 가짐(성공, 실패) 그러므로 JOB_INSTANCE와 1:N 관계임 똑같은 파라미터에 대해서는 2번 이상 실행하면 에러가 발생한다 정확히 말해 COMPLETED가 이미 있으면 에러가 발생하는 것이다 FAILD가 있으면 다시 실행해도 에러가 발생하지 않는다 BATCH_JOB_PARAMETERS EXECUTION에 사용한 파라미터들을 저장 VALUE OBJECT COLLECTION Table 형태로 사용함 BatchStatus, ExitStatus JOB_EXECUTION, STEP_EXECUTION 테이블을 보면 status, exit_code라는 것이 있다. status는 job이나 step의 실행 결과를 기록할 때 사용하는 Enum이고, exit_code는 job이나 step의 실행 후 상태를 나타내는 일반 string이다. 기본적으로 exit_code는 status와 같도록 설정되어 있으나, 커스텀한 exit_code가 있으면 추가할 수 있는 구조이다. 스프링 배치 사용 step flow 제어 Job 내에 Step 등록 시 호출 Flow를 제어 가능하다(순서, 조건별 분기 등등) 1234567891011121314151617@Beanpublic Job stepNextConditionalJob() &#123; return jobBuilderFactory.get("stepNextConditionalJob") .start(conditionalJobStep1()) .on("FAILED") // FAILED 일 경우 .to(conditionalJobStep3()) // step3으로 이동한다. .on("*") // step3의 결과 관계 없이 .end() // step3으로 이동하면 Flow가 종료한다. .from(conditionalJobStep1()) // step1로부터 .on("*") // FAILED 외에 모든 경우 .to(conditionalJobStep2()) // step2로 이동한다. .next(conditionalJobStep3()) // step2가 정상 종료되면 step3으로 이동한다. .on("*") // step3의 결과 관계 없이 .end() // step3으로 이동하면 Flow가 종료한다. .end() // Job 종료 .build();&#125; 처음 시작할떄는 무조건 start ExitStatus를 catch하고 싶다면 on 사용하고, 뒤에 연결할 step에 to 사용 from은 이벤트 리스너. 인자로 받는 step의 이벤트를 listen함 위의 상황에선 step1의 이벤트를 이미 캐치하고 있으므로, 추가로 이벤트를 캐치하려면 from을 사용해야했음 on + end 뒤에만 붙일 수 있는데, 왜 그런건지… step을 종료시키기 위해서 붙여야하는 패턴인건지? step에서 contribution.setExitStatus 를 세팅해줘야만 job에서 catch 가능 custom ExitStatus를 세팅하려면 StepExecutionListenerSupport 를 상속받은 클래스를 추가로 등록해야 한다 decide step들의 flow 속에서 분기만 담당하는 애들 ExitStatus 세팅하는 부분을 분리할 수 있다 Scope, jobParameter @JobScope, @StepScope 를 선언하면 빈 생성시점이 해당 scope가 실행되는 시점까지 지연된다 Scope를 이렇게 뒤로 미루면서 얻는 장점이 여러가지가 있다 비즈니스 로직 처리단계에서 Job Parameter 할당받을 수 있음 병렬처리 가능(step당 각자 tasklet을 가지므로) jobParameter @JobScope, @StepScope 에서 파라미터(외부/내부)를 받을 수 있다 step에 @JobScope를 선언하고 parameter를 매개변수로 받는 방법과, tasklet, itemReader 등에 @StepScope를 선언하고 parameter를 매개변수로 받는 방법이 있다. jobParameter가 필요한 곳에서 사용하도록 해야할듯 예를들어 step 단위에서 parameter가 필요하다면 @JobScope에서 파라미터를 받아야 할 것이고, tasklet 단위에서 parameter가 필요하다면 @StepScope에서 파라미터를 받아야 할 것이다 jobParameter는 @JobScope(step), @StepScope(chunk) 빈을 생성할때만 사용할수있으며, step과 chunk의 최상위에는 job이 있다. 즉, job을 생성할때 던진 파라미터를 이용해서 scope bean에서 job parameter로 사용하는 것이다(아마도) 이 scope 빈을 @XXXScope로 생성하지 않고 일반 싱글톤으로 생성하면 job Parameter를 찾을 수 없다 jobParameter는 job으로 부터 오는것이니까 job을 생성(실행)할 때 던진 parameter를 가지고 @JobScope, @StepScope 빈을 생성하며 parameter를 주는 것이다 그러므로 job 코드에는 parameter로 null이 들어가게 된다(… 아마도?) 12345JobParameters jobParameters = new JobParametersBuilder() .addString("input.file.name", fileName) .addLong("time", System.currentTimeMillis()) .toJobParameters();jobLauncher.run(job, jobParameters); jobLauncher.run을 따라가보면(SimpleJobLauncher 기준) jobExecution이 있으면 오류가 발생하고, jobExecution이 없으면 jobExecution을 생성한다 그리고 해당 jobExecution은 async로 실행시킨다(아마도) chunk chunk 지향 처리란 itemReader, itemProcerssor, itemWriter로 이어지는 형태를 말함 tasklet은 내부에 모든 로직이 다 있다 reader에서 job을 읽어오고, process에서 처리하고 writer로 쓴다 reader, process는 1건씩 처리되고, chunkSize만큼 processing이 완료되면 writer로 한방에 쓴다? chunk는 배치에서 한번에 트랜잭션으로 처리할 단위이다 pageSize와는 다르다. pageSize는 한번에 조회하는 단위이다. pageSize와 chunkSize를 같게해야 성능상 좋다 reader reader로 읽을 수 있는 데이터는 데이터베이스만이 아니라 입력 데이터, 파일, jms 등등 여러가지이다 cursor, paging 장점 spring batch에 사용할 단위들을 bean으로 생성할 수 있고, scope가 Job, Step scope 단위라는 점 그러므로 파라미터를 원하는 타이밍에 자유자재로 받을 수 있고, 병렬처리에도 좋다 chunk]]></content>
      <categories>
        <category>spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[tdd] 테알못 신입은 어떻게 테스트를 시작했을까? 리뷰]]></title>
    <url>%2Ftdd%2F%ED%85%8C%EC%95%8C%EB%AA%BB-%EC%8B%A0%EC%9E%85%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%85%8C%EC%8A%A4%ED%8A%B8%EB%A5%BC-%EC%8B%9C%EC%9E%91%ED%96%88%EC%9D%84%EA%B9%8C-%EB%A6%AC%EB%B7%B0%2F</url>
    <content type="text"><![CDATA[https://www.youtube.com/watch?v=1bTIMHsUeIk 방법에 대한 이야기 RED에서 시작할 수도 있고, GREEN 에서 시작할 수 있다. 여기서 RED는 버그가 있어서 RED인 것이고, TDD의 RED는 아직 코드가 없기 떄문에 RED이다 테스트 라스트 방식 이미 구현되어 있는 코드에 테스트를 붙이는 방식 테스트하기 쉬운 코드로 시작한다 순수함수로 만들어져 있다 외부 의존성이 전혀 없다 인풋과 아웃풋이 명확한 단순한 기능들(유틸리티 같은) 프로덕션 코드는 아무것도 수정하지 않고, 함수에 제대로된 인풋에 제대로된 아웃풋이 나오는지 테스트하는 것을 먼저 만든다 리팩토링을 한다 테스트 코드는 수정하지 않고 프로덕션 코드를 수정한다 이제 테스트하기 어려운 코드를 리팩토링 한다 중요도가 높은 비즈니스 로직 버그가 발견된 부분 결합이 낮고 논리는 복잡한 부분 여기서 쉽게 테스트할 수 있는 부분을 분리해서 테스트한다 테스트하기 어려운 코드에서 테스트 가능한 코드를 찾는다 이를 분리한다 테스트코드를 추가하고 GREEN이 되도록 만든다 리팩토링 한다 1~4를 계속 반복한다 예시 전달받은 정보를 validate 한 뒤 api를 요청하고, 요청이 끝나면 모달창을 닫는다 api 요청은 테스트하기 너무 어렵지만, 위의 validate는 테스트하기 쉽다 그 부분을 함수로 빼고 테스트를 만든다. 여기서 테스트가 실패하는데, 이는 외부 의존성이 남아있었기 떄문이다. 이를 파라미터로 받는 형태로 바꿔서 독립적인 함수로 변형시킬 수 있다 이제 스팩을 추가하고(나와야 하는 결과들) 해당 테스트가 성공하도록 하고, 리팩토링 하면 된다 TDD Test LAST와 달리 이미 구현되어 있는 코드가 없다 신규 요구사항에 대한 개발을 말한다 요구사항에 맞춘 테스트를 먼저 작성한다 이미 구현되어 있는 코드가 없으므로 무조건 RED 부터 시작한다 테스트에 맞춰 기능을 개발한다 GREEN 이 뜨면 테스트는 손대지 않고 프로덕션 코드를 리팩토링한다 이를 반복한다 경험에 대한 이야기 좋은점 불안감소 스펙문서기능 본인이 만든 함수의 세부사항을 다 기억하는 사람은 없다 함수를 다시 읽는것보단 테스트를 읽는것이 훨씬 빠르다(스펙의 역할을 한다) 디자인 개선 효과 하나의 함수가 여러가지 역할을 수행하고 있는지 체크할 수 있다 학습 동기부여 디자인 개선을 하다보면 내가 얼마나 디자인을 못하게 되는지 알게 된다 그래서 설계에 계속 관심을 가지게 된다 이게 학습이다! 개발 생산성 향상 통합테스트는 손해보는 시간 테스트 안해서 아낀 시간(테스트 작성 시간) &lt; 테스트 안해서 나온 버그 고치는 시간 비즈니스 로직의 허점을 미리 발견할 수 있다 스펙에 대해서 많이 생각하게 되기 떄문이다 코딩하기 전에 발견할수 있어서 아끼는 비용이 많다 집중력 향상 TDD는 죽었다에 대한 켄트백의 말 나는 내가 필요하게 될 거라고 알고 있는 기능을 추가하려는 성향이 있다. 하지만 하나의 빨간 테스트를 녹색으로 만드는 것을 딱 충분한 만큼 구현하도록 도와준다. 집중을 유지할 새로운 방법을 찾아야한다. 실제로도 테스트(스펙)을 정해놓으면 내가 구현해야 할 기능에 집중할 수 있다 실수 테스트 자체가 목적이 되어서(커버리지를 높이자!) 테스트를 막 찍어내는 상황 불필요한 테스트 비즈니스와 관련된 버그를 낼 가능성이 낮거나 없음 테스트를 유지함으로써 얻는 이익 &lt; 테스트 유지와 관리에 드는 비용 테스트가 단언하고 있는 내용이 사용자에게 중요한 가치를 주는것이 아닐 때 이때 div가 몇개 나오게 되고, div 안에는 p가 몇개고… 같은 테스트 이런것은 사용자의 관심이 아니다 이런것까지 굳이 테스트 할 필요는 없다 필요하지만 검증방식이 잘못된 테스트 DOM 구조에 의존한 테스트 div의 첫번째 자식에 이러이러한 테스트가 있다 등 사용자에게 주는 가치가 낮고, 테스트를 유지하는 비용이 더 큰 경우 특정 위치를 찝어서 검사하는 형태보단, 의존관계를 줄이는 방식으로 테스트를 개선한다(DOM구조 의존 제거) 검증력이 떨어지는 테스트 테스트가 원하는 결과가 나올수 있는 상황이 여러가지 일때 우리가 원하지도 않는 형태에서 우리가 원하는 결과 발생 e.g. expect 값이 undefined 테스트 제목과 검증의 불일치 e.g. 전체요소에 대해 테스트하는 척해놓고 엘리먼트 1번만 테스트 하기 테스트를 앞서가는 프로덕션 코드 프로덕션 코드에서 테스트에서 테스트하는 대상보다 이상의 행위를 할 때 프로덕션에 테스트되지 않은 코드가 추가되는 것이 허용되는 것이다 첫번째 요소에 대해 테스트했다면, 프로덕션에서는 첫번째 요소만 렌더링 했어야 했다 고민 픽스쳐(테스트를 위해 만들어진 데이터) 생성을 어떻게 해야할까? 함수만 계속해서 만들어지게 된다 무조건 함수들을 분리 하는게 맞는건가? 이게 가독성을 더 해치는건 아닌가? 추상화 수준이 낮아서 그렇다 높은 응집, 낮은 결합 을 충족해야 한다 함수로 분리해서 낮은 결합은 달성했지만, 높은 응집은 달성하지 못함 그냥 니 리팩토링 수준이 낮아서다 라고 얘기하는 중]]></content>
      <categories>
        <category>tdd</category>
      </categories>
      <tags>
        <tag>TDD</tag>
        <tag>OKKYCON TDD 2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 한 컬럼에 여러 엔티티 저장하기(feat.@Any)]]></title>
    <url>%2Fjpa%2F%ED%95%9C-%EC%BB%AC%EB%9F%BC%EC%97%90-%EC%97%AC%EB%9F%AC-%EC%97%94%ED%8B%B0%ED%8B%B0-%EC%A0%80%EC%9E%A5%ED%95%98%EA%B8%B0-feat-Any%2F</url>
    <content type="text"><![CDATA[여러 테이블에 공용으로 저장하는 테이블을 만들어야 한다고 가정해보자. 여러 엔티티라고 해서 리스트를 얘기하는 것이 아니라, 다형성을 얘기하는 것이다. http://docs.jboss.org/hibernate/orm/5.2/userguide/html_single/Hibernate_User_Guide.html#mapping-column-any https://www.concretepage.com/hibernate/hibernate-any-manytoany-and-anymetadef-annotation-example]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>@ManyToOne multiple entities</tag>
        <tag>column polymorphic</tag>
        <tag>@Any</tag>
        <tag>@AnyMetaDef</tag>
        <tag>@AnyToMany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] JPA 성능 최적화]]></title>
    <url>%2Fjpa%2FJPA-%EC%84%B1%EB%8A%A5-%EC%B5%9C%EC%A0%81%ED%99%94%2F</url>
    <content type="text"><![CDATA[N+1 문제 성능상 가장 주의해야 하는것이 이 N+1 문제이다. 아래와 같은 엔티티가 있다고 가정한다. 1234567891011121314151617@Entityclass Member&#123; @Id @GeneratedValue private Long id; @OneToMany(mappedBy = "member", fetch = FetchType.EAGER) private List&lt;Order&gt; orders = new ArrayList&lt;&gt;();&#125;@Entityclass Order&#123; @Id @GeneratedValue private Long id; @ManyToOne private Member member;&#125; (참고로 Order에서 Member를 EAGER로 설정해도 동일하게 N+1은 발생한다) 즉시 로딩 123List&lt;Member&gt; members = em.createQuery("SELECT m FROM Member", Member.class) .getResultList(); 예전에도 언급했지만, JPA는 fetchType을 전혀 신경쓰지 않고 충실하게 JPQL에 맞춰 SQL을 생성한다. 따라서 Member를 전체 조회하는 쿼리가 먼저 실행되고, Member의 개수만큼 Order를 조회하게 될 것이다(…) 123456SELECT * FROM Member; -- if result is 5SELECT * FROM Order_ WHERE MEMBER_ID = 1;SELECT * FROM Order_ WHERE MEMBER_ID = 2;SELECT * FROM Order_ WHERE MEMBER_ID = 3;SELECT * FROM Order_ WHERE MEMBER_ID = 4;SELECT * FROM Order_ WHERE MEMBER_ID = 5; 이처럼 처음 실행한 SQL의 결과 수만큼 추가로 SQL을 실행하는 것을 N+1 문제라고 한다. 지연 로딩 즉시로딩을 지연로딩으로 바꿔도 N+1에서 자유로울수는 없다. 즉시로딩이 아니라서 Member 조회와 동시에 Member 건수만큼 Order를 조회해오진 않겠지만, Member에서 Order를 사용하는 시점에는 똑같이 불러오게 된다. 123for(Member member : membres)&#123; System.out.println(member.getOrders().size());&#125; members 개수만큼 order 조회 SQL이 실행될 것이다. 즉, 이것도 결국 N+1 문제다. 해결법 페치 조인 가장 일반적인 방법이다. SQL 조인을 이용해서 연관된 엔티티를 함께 조회하므로 N+1 문제가 발생하지 않는다. JPQL은 아래와 같다. 1SELECT m FROM Member m JOIN FETCH m.orders JOIN으로 같이 조회해서 Member 엔티티의 orders 속성에 초기화 하였기 때문에 더이상 N+1이 발생하지 않는다. 참고로 위 예제는 일대다 조인이므로 결과가 늘어날 수 있다. DISTINCT를 써줘야한다. 하이버네이트 @BatchSize 하이버네이트가 제공하는 org.hibernate.annotations.BatchSize 어노테이션을 이용하면 연관된 엔티티를 조회할 때 지정된 size 만큼 SQL의 IN절을 사용해서 조회한다. 123456789@Entityclass Member&#123; @Id @GeneratedValue private Long id; @org.hibernate.annotations.BatchSize(size = 5) @OneToMany(mappedBy = "member", fetch = FetchType.EAGER) private List&lt;Order&gt; orders = new ArrayList&lt;&gt;();&#125; 즉시로딩이므로 Member를 조회하는 시점에 Order를 같이 조회한다. @BatchSize가 있으므로 Member의 건수만큼 추가 SQL을 날리지 않고, 조회한 Member 의 id들을 모아서 SQL IN 절을 날린다. 1234SELECT * FROMORDER_ WHERE MEMBER_ID IN( ?, ?, ?, ?, ?) size는 IN절에 올수있는 최대 인자 개수를 말한다. 만약 Member의 개수가 10개라면 위의 IN절이 2번 실행될것이다. 그리고 만약 지연로딩이라면 지연로딩된 엔티티 최초 사용시점에 5건을 미리 로딩해두고, 6번째 엔티티 사용 시점에 다음 SQL을 추가로 실행한다. hibernate.default_batch_fetch_size 속성을 사용하면 애플리케이션 전체에 기본으로 @BatchSize를 적용할 수 있다. 12&gt; &lt;property name="hibernate.default_batch_fetch_size" value="5" /&gt;&gt; 하이버네이트 @Fetch(FetchMode.SUBSELECT) 연관된 데이터를 조회할 때 서브쿼리를 사용해서 N+1 문제를 해결한다 123456789@Entityclass Member&#123; @Id @GeneratedValue private Long id; @org.hibernate.annotations.Fetch(FetchMode.SUBSELECT) @OneToMany(mappedBy = "member", fetch = FetchType.EAGER) private List&lt;Order&gt; orders = new ArrayList&lt;&gt;();&#125; 아래와 같이 실행된다. 123456SELECT * FROM Member;SELECT * FROM Order_ WHERE MEMBER_ID IN( SELECT ID FROM Member ) 즉시로딩으로 설정하면 조회시점에, 지연로딩으로 설정하면 지연로딩된 엔티티를 사용하는 시점에 위의 쿼리가 실행된다. 모두 지연로딩으로 설정하고 성능 최적화가 필요한 곳에는 JPQL 페치 조인을 사용하는 것이 추천되는 전략이다. 읽기 전용 쿼리의 성능 최적화 JPA의 영속성 컨텍스트는 변경 감지를 위해 스냅샷 인스턴스를 보관하는 특징이 있다. 하지만 단순 조회 화면에서는 조회한 엔티티를 다시 조회할 필요도 없고, 수정할 필요도 없어서 이때는 스냅샷 인스턴스를 위한 메모리가 낭비된다. 이럴 경우 아래의 방법으로 메모리 사용량을 최적화할 수 있다. 스칼라 타입으로 조회 엔티티가 아닌 스칼라 타입으로 모든 필드를 조회하는 것이다. 알다시피 스칼라 타입은 영속성 컨텍스트가 관리하지 않는다. 1SELECT m.id, m.name, m.age FROM Member m 읽기 전용 쿼리 힌트 사용 하이버네이트 전용 힌트인 org.hibernate.readOnly를 사용하면 엔티티를 읽기 전용으로 조회할 수 있다. 읽기 전용이므로 영속성 컨텍스트가 스냅샷을 저장하지 않으므로 메모리 사용량을 최적화 할 수 있다. 123Member member = em.createQuery("SELECT m FROM Member m", Member.class) .setHint("org.hibernate.readOnly", true) .getSingleResult(); 스냅샷이 없으므로 member의 값을 수정해도 update 쿼리가 발생하지 않는다. 스냅샷만 저장하지 않는 것이지, 1차 캐시에는 그대로 저장한다 똑같은 식별자로 2번 조회했을 경우 반환되는 엔티티의 주소가 같다 읽기 전용 트랜잭션 사용 스프링 프레임워크를 사용하면 트랜잭션을 읽기 전용 모드로 설정할 수 있다. 1@Transactional(readOnly = true) 트랜잭션을 읽기 전용으로 설정하면 스프링 프레임워크가 하이버네이트 세션의 플러시 모드를 MANUAL로 설정한다. 이렇게하면 강제로 플러시 호출을 하지 않는 한 플러시가 일어나지 않는다. 엔티티의 플러시 모드는 AUTO, COMMIT 모드만 있다. MANUAL 모드는 하이버네이트 세션에 있는 플러시모드이다. 이는 강제로 플러시를 호출하지 않으면 절대 플러시가 일어나지 않는 특징을 가지고 있다. 하이버네이트 세션은 JPA 엔티티 매니저를 하이버네이트로 구현한 구현체이다. 플러시를 수행하지 않으므로 플러시할 떄 일어나는 스냅샷 비교와 같은 무거운 로직들이 실행되지 않으므로 성능이 향상된다. (그래도 스냅샷은 그대로 저장하는 듯. 단지 플러시만 일어나지 않는 것 같다.) 물론 트랜잭션을 시작했으므로 트랜잭션 시작, 수행, 커밋의 과정은 이루어진다. 트랜잭션 밖에서 읽기 트랜잭션 없이 영속성 컨텍스트만 가지고 엔티티를 조회하는 것을 의미한다. JPA에서 엔티티를 변경하려면 트랜잭션이 필수이므로, 조회가 목적일 때만 사용해야 한다. JPA는 기본적으로 아래의 2가지 특성이 있다 트랜잭션이 커밋될 때 영속성 컨텍스트를 플러시한다 영속성 컨텍스트만 있으면 트랜잭션 없이 읽기가 가능하다 스프링을 사용하지 않을 때 123456789EntityManager em = emf.createEntityManger();// EntityTransaction tx = em.getTransaction();try &#123; // tx.begin(); Some some = em.find(Some.class, 1L); SomeChild someChild = some.getSomeChildren().get(0);&#125; 트랜잭션을 시작하는 부분 없이 엔티티매니저만 가져와서 조회를 수행해도 정상 동작하고, 보다시피 lazy 로딩까지도 동작한다. 참고로 여기서 스프링이 @PersistenceContext를 통해 가져온 EntityManager를 사용할 경우 예외가 발생한다(org.hibernate.LazyInitializationException) 위처럼 메서드내에서 생성된 entityManager에만 유효하다(@PersistenceContext는 공유되는 애라서 그런가…) 공유되는 애인게 왜 문제가 되지? 스프링을 사용할 때 스프링을 사용할떄는 OSIV를 사용하거나, 직접 EntityManager를 생성시켜주는 방법을 사용해야 한다 기본적으로 @Transactional 을 통해 트랜잭션이 생성될 때 영속성 컨텍스트를 생성하는 구조이기 떄문이다 트랜잭션을 시작하지 않을거라면 @Transactional 어노테이션을 제거해줘야하는데, 그러면 스프링 입장에서 언제 영속성 컨텍스트를 생성해줘야 할지 알지 못하게 된다 그러므로 OSIV를 사용하지 않을거면 EntityManagerFactory를 가져온 뒤 직접 엔티티 매니저를 생성해줘야만 트랜잭션 없이 읽기가 가능하다 이 과정 없이 트랜잭션 없이 읽기를 사용하려면(@Transactional 어노테이션을 제거해서) OSIV를 켜줘야한다]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>N+1</tag>
        <tag>fetch join</tag>
        <tag>@BatchSize</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] JPA 예외]]></title>
    <url>%2Fjpa%2FJPA-%EC%98%88%EC%99%B8%2F</url>
    <content type="text"><![CDATA[JPA의 표준 예외들은 javax.persistence.PersistenceException의 자식 클래스이다. 그리고 이 클래스는 RuntimeException의 자식이다. 즉, JPA의 예외는 모두 언체크 예외이다. JPA 표준 예외 JPA 표준 예외는 크게 아래의 2가지로 나뉜다. 난 실행단계에서 아래 2개를 어떻게 구분해야할지 잘 모르겠다 트랜잭션 롤백을 표시하는 예외 심각한 예외이므로 복구해선 안되는 예외들이다. 이 예외가 발생하면 트랜잭션을 강제로 커밋해도 커밋되지 안혹 javax.persistence.RollbackException이 발생한다. 예외 설명 javax.persistence.EntityExistsException EntityManager.persist(..) 호출 시 같은 엔티티가 있으면 발생 javax.persistence.EntityNotFoundException EntityManager.getReference(..) 호출하고 실제 사용 시 엔티티가 존재하지 않으면 발생. refresh(..), lock(..) 에서도 발생 javax.persistence.OptimisticLockException 낙관적 락 충돌시 javax.persistence.PessimisticLockException 비관적 락 충돌시 javax.persistence.RollbackException EntityTransaction.commit() 실패 시 발생. 롤백이 표시되어 있는 트랜잭션 커밋시에도 발생 javax.persistence.TransactionRequiredException 트랜잭션이 필요할 떄 트랜잭션이 없으면 발생. 트랜잭션 없이 엔티티를 변경할 떄 주로 발생 트랜잭션 롤백을 표시하지 않는 예외 심각한 예외가 아니다. 개발자가 트랜잭션을 커밋할지 롤백할지 판단하면 된다. 예외 설명 javax.persistence.NoResultException Query.getSingleResult() 호출 시 결과가 하나도 없을 때 발생 javax.persistence.NonUniqueResultException Query.getSingleResult() 호출시 결과가 둘 이상일 떄 발생 javax.persistence.LockTimeoutException 비관적 락에서 시간 초과시 발생한다 javax.persistence.QueryTimeException 쿼리 실행 시간 초과시 발생 스프링 프레임워크의 JPA 예외 변환 서비스 계층에서 데이터 접근 기술에 직접 의존하는 것은 좋은 설계가 아니다. 이것은 예외도 마찬가지다. 서비스 계층에서 위 JPA 예외에 직접 의존하면 결국 JPA에 의존하게 되는것이다. 스프링 프레임워크는 이런 문제를 해결하기 위해 JPA 예외를 추상화해서 제공한다. blah blah… 스프링 프레임워크에 JPA 예외 변환기 적용 위처럼 JPA예외를 스프링 예외로 변경해서 받으려면 PersistenceExceptionTranslationPostProcessor를 스프링 빈으로 등록하면 된다. 이것은 @Repository 어노테이션을 사용한곳에 예외 변환 AOP를 적용해서 JPA 예외를 스프링 추상화 예외로 변환해준다. 1234@Beanpublic PersistenceExceptionTranslationPostProcessor exceptionTranslation()&#123; return new PersistenceExceptionTranslationPostProcessor();&#125; 아래 예외는 변환되어서 던져질 것이다. 123456789101112@Repositorypublic class NoResultExceptionTestRepository&#123; @PersistenceContext private EntityManager em; public Member findMember()&#123; // 조회결과 없음 return em.createQuery("SELECT m FROM Member WHERE m.id = :id", Member.class) .setParameter("id", 999) .getSingleResult(); &#125;&#125; 원래라면 NoResultException이 발생해야 하지만, 등록한 AOP 인터셉터가 동작해서 EmptyResultDataAccessException으로 변환해서 반환한다. 만약 예외를 변환하지 않고 그대로 반환하고 싶다면 반환할 JPA 예외나 JPA 예외의 부모 클래스를 직접 명시하면 된다. 12345678@Repositorypublic class NoResultExceptionTestRepository&#123; public Member findMember() throws javax.persistence.NoResultException&#123; return em.createQuery("SELECT m FROM Member WHERE m.id = :id", Member.class) .setParameter("id", 999) .getSingleResult(); &#125;&#125; 예외의 최상위 클래스인 Exception을 throws 하면 예외를 아예 변환하지 않을 것이다. 트랜잭션 롤백 시 주의사항 트랜잭션을 롤백하는 것은 데이터베이스의 반영사항만 롤백하는 것이지, 수정한 자바 객체까지 원상태로 복구해주지는 않는다. 이 말인 즉 트랜잭션이 롤백되었다고 영속성 컨텍스트도 롤백되는 것은 아니라는 것을 의미하는데, 기본 전략인 트랜잭션당 영속성 컨텍스트 ㅈ]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>jpa exception</tag>
        <tag>PersistenceException</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 부모 테이블을 공유하는 자식 테이블(feat.@PrimaryKeyJoinColumn)]]></title>
    <url>%2Fjpa%2F%EC%97%94%ED%8B%B0%ED%8B%B0-%EC%83%81%EC%86%8D%EC%9D%98-%ED%95%9C%EA%B3%84%2F</url>
    <content type="text"><![CDATA[원하는 형태 여러 자식 테이블들의 1개 로우가 부모 테이블 로우 1개를 바라보는 형태 1234Item : row 1개, pk=1 Album : row 1개, pk1 Movie : row 1개, pk1 ... 결과적으로, 상속으론 저렇게 할수없다. 상속을 사용하면 완벽한 식별관계를 형성해서 자식(상속에서) 로우에 맞춰 부모 로우도 같이 생성하기 때문이다. 하긴 객체지향에서도 자식 생성시 부모 인스턴스 생성하니까 당연한 소리인가… 구현 @OneToOne Eager를 사용하면 위의 형태를 구성할 수 있다. 기본적인 @OneToOne 을 사용하면 위의 관계를 외래키를 이용해서 유지하게 되는데, 만약 정말 위처럼 완벽한 식별관계의 형태로 유지하고 싶다면 @PrimaryKeyJoinColumn을 사용하면 된다. 12 결과적으로 JPA 엔티티 상속의 기능은 딱히 특별할것이 없다… 는 결론입니다. 자식 테이블의 여러 로우들이 부모 테이블 로우 1개를 바라보는 방식 이건 잘못 생각한 것이다. 여러 로우에서 1개의 로우를 바라보려면 외래키로 연결되어야 한다 @ManyToOne Eager 를 사용해야 할 듯 여러 자식 테이블들의 여러 로우들이 부모 테이블 로우 1개를 바라보는 방식 이 또한 잘못 생각한 것 @ManyToOne Eager 를 사용해야 할 것이고, @DiscriminatorValue가 추가적으로 필요할 것이다]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>JPA Inheritance</tag>
        <tag>@PrimaryKeyJoinColumn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[리팩토링 중요 개념]]></title>
    <url>%2Frefactoring%2F%EB%A6%AC%ED%8C%A9%ED%86%A0%EB%A7%81-%EC%A4%91%EC%9A%94-%EA%B0%9C%EB%85%90%2F</url>
    <content type="text"><![CDATA[리팩토링 겉으로 드러나는 기능은 그대로 둔 채, 알아보기 쉽고 수정하기 간편하게 소프트웨어 내부를 수정하는 작업 코드 구조는 한번 엉망이 되면 갈수록 더 엉망이 된다. 그러므로 주기적인 리팩토링이 필요하다. 어떤 코드를 실행할때 컴퓨터는 몇바퀴 헤매더라도 그다지 문제될 것 없지만, 기껏해야 한시간정도면 이해하고도 남을 코드를 다른 개발자가 신속히 이해하지 못해서 한 부분을 수정하는데 일주일 이상 걸린다면 그건 큰 문제다. 설계가 깔끔하지 않으면 개발 초기 잠시 동안은 진행이 빠를지 몰라도, 얼마 못가서 개발 속도가 떨어진다 리팩토링은 작정해서 하는것이 아니라, 뭔가 다른걸 해야겠는데 리팩토링을 실시하면 그 작업이 쉬워지기 때문에 하는 것이다 리팩토링은 언제 필요할까? 같은 작업을 3번째 반복하게 됐을 때 기능을 추가할 때 설계가 지저분해서 어떤 기능을 추가하기 힘들떄는, 주저하지 않고 리팩토링을 실시한다 버그를 수정할 떄 코드의 기능을 파악하다 이해하기 힘들면, 이해하기 쉽게 만드려고 리팩토링을 실시한다 돌아가는 원리를 쉽게 파악 가능하게 되고, 버그 찾기도 쉬워진다 테스트 메서드 작성은 필수이다(자체 테스트 가능해야한다) 리팩토링은 프로그램을 조금씩 단계적으로 수행하므로 실수를 발견해도 고치기 쉽다 좋은 코드는 그것이 무슨 기능을 하는지 분명히 드러나야 한다 리팩토링의 각 단계는 간단해야 실수할 가능성이 줄어든다 리팩토링 떄문에 성능이 안좋아질 수 있다. 하지만 그건 최적화 단계에서 수정하면 된다 코드의 기능을 파악하는데 시간이 많이 걸리는데, 어쩌피 지나야 할 과정이다 주기적 리팩토링을 통해 코드를 깔끔하게 해놓으면, published 인터페이스 인터페이스의 장점은 사용하는 곳을 수정하지 않고도 내부 구조를 수정할 수 있다는 점이다 하지만 대다수의 리팩토링 기법은 인터페이스를 건드린다 이 인터페이스를 사용하는 모든 코드에 접근할 수 있다면, 이 리팩토링 기법은 문제되지 않는다 하지만 사용하는 모든 코드에 접근하지 못하는 상황이 있다. 외부에서 사용하기 위한 API를 작성하는 등의 행위를 예로 들 수 있다. 이런 인터페이스를 published(배포된) 인터페이스라고 한다 published 인터페이스를 수정하는 경우, 사용하는 부분이 모든 변경될 때 까지 기존 인터페이스와 새 인터페이스를 모두 유지해야 한다 기존 인터페이스에서 새로운 인터페이스를 호출하는 형태로 작성해야 한다 기존 인터페이스는 @Deprecated 같은것을 사용해서 사용하지 않게끔 알려야한다 최소한 일정 기간동안 사용하지 않는 메서드를 유지시켜야 하므로 불편하다 방법은 published 메서드를 최대한 만들지 않는 것이다 java api 같은 외부 API를 만드는 상황을 빼고도 과하게 published 메서드를 만드는 경우가 있다 이럴 경우는 코드 소유권 정책을 수정해서 인터페이스 수정을 촉진하도록 하는 것이 낫다 인터페이스를 수정하는 사람이 인터페이스를 사용하는 곳의 코드까지 고칠 수 있게 하는것이다 설계가 수정되었을 때 설계에 오류가 있거나, 설계에 대한 결정이 바뀌었을 경우, 수정하기 힘든 민감한 부분일 경우라도 대부분 리팩토링으로 해결된다 새로 추가되는 코드들에 새 구조(바뀐 설계)를 사용하도록 개발한다 기존의 코드에 대한 기능추가나 버그 발견 등이 발생했을 경우 기존 코드를 새 구조를 사용하게 바꾼다 리팩토링 한다고 all stop 하는 상황을 막을 수 있다 리팩토링하면 안되는 상황 코드가 제대로 돌아가지 않는다면, 그냥 새로 작성하는것이 낫다 코드는 제대로 돌아가는 것이 우선이고, 리팩토링은 나중 일이다 납기가 임박했을 때도 리팩토링은 삼가야한다 납기가 임박한 경우가 아니면 시간이 없다는 핑계로 리팩토링을 미루면 안된다 리팩토링은 미완료 대출금이다 대출이자는 복잡한 코드로 유지보수하기, 확장의 어려움 이다 언제나 시간에 쫓긴다면, 그건 리팩토링해야 한다는 신호이다 처음부터 완벽한 설계를 할 필요는 없다 애초에 처음부터 완벽한 설계란 너무 어렵고, 빈틈이 많을 가능성이 있다 그러므로 완벽한 솔루션을 찾을 필요없이, 적당한 솔루션을 찾으면 된다 구축해나가면서 문제를 더 잘 이해하게 되며, 처음 구축한 솔루션이 최상의 솔루션이 아니었다는걸 알게 된다 이러면 처음 제시한 솔루션이 잘못되었더라도, 그 솔루션을 수정하는데 비용이 들지 않게된다 간단하게 설계하고, 일단 구현한 다음, 리팩토링으로 다듬으면 된다 소프트웨어는 실물의 기계와 달리 유연하기 때문이다 시스템 전체를 유연하게 설계할 필요는 없다 잠재적 가능성을 생각하여 유연한 설계를 하는 것은 매우 복잡하고 힘들다 애초에 유연하게 복잡한 설계가 필요한 상황은 거의 없다 단순한 솔루션을 구현해놓고, 나중에 그것을 리팩토링 하려면 얼마만큼의 수고가 들지 생각해본다 별로 어려움이 없을 것이라고 판단되면, 단순하게 솔루션을 구현하면 된다 리팩토링을 쉽게 할 수 있는 감각을 익히게 되면, 유연한 솔루션은 떠오르지도 않는다 때가 되면 어련히 리팩토링하게 되리란 확신이 있기 때문이다 리팩토링을 실시하면 소프트웨어 속도는 느려지지만, 성능을 더 간단하게 조절할 수 있다 소프트웨어 성능을 올리려면 먼저 소프트웨어를 튜닝 가능하게 만들어놔야 한다 성능 감소의 이유를 추측에 의존하지 말고 프로파일러를 통해 측정하도록 해야한다 프로그램을 잘 쪼개어야 한다 프로파일러로 성능을 감소시키는 작은 부분을 발견할 수 있게 되고, 이 부분에 집중하여 성능 개선을 할 수 있다 성능을 분석할 때 더욱 정밀한 분석이 가능해진다 켄트백의 모자 두개 기능을 추가할 땐 코드를 수정하지 말고 기능만 추가해야 한다 테스트를 추가하고 테스트가 잘 되는지만 보면 됨 리팩토링할때는 코드를 추가하지 말고 구조 개선만 해야 한다 테스트를 수정하지 않고 코드 구조만 수정해야 함 둘을 같이 하면 실패할 확률이 높다(경험…) 랄프 존슨의 우선 창 밖이 보이게 뿌연 유리창부터 닦는 일 리팩토링을 실시하면 낯선 코드를 쉽게 이해할 수 있다 그렇게 첫 리팩토링을 마치고 코드가 깔끔해지면, 기존에 안보이던 설계가 보인다 켄트백의 리팩토링의 효용성 프로그램이 지닌 가치는 현재의 기능과 미래의 기능이라는 2개의 가치이다 프로그램의 현재기능은 그저 일부에 불과하다 과거의 판단이 현재의 기준으로 불합리하다는 사실을 발견했으면, 수정해야 한다 프로그램이 수정하기 힘들어지는 4가지 상황 코드를 알아보기 힘들 때 중복된 로직이 들어있을 떄 추가기능을 넣어야해서 실행중인 코드를 변경해야 할 때 조건문 구조가 복잡할 떄 프로그램은 코드를 알아보기 쉽고, 모든 로직이 한곳에 있으며, 기존 기능을 건드릴 필요 없이 조건문 구조가 최대한 간결하게끔 작성해야 한다. 메서드는 대체로 자신이 사용하는 데이터와 같은 객체에 들어있어야 한다 임시변수는 최대한 제거하는 것이 좋다 변경되지 않는 변수는 매개변수로 전달할 수 있다 변경되는 변수가 하나뿐이라면 리턴값으로 사용할 수 있다 위 두 원칙을 적용하면서 임시변수를 계속 제거해보자 switch문의 인자로는 자신의 데이터를 사용해야 한다(타객체 데이터 X) 질의 메서드를 따로 만들어서 편리하게끔 할 수 있음 참고 : 마틴 파울러, 『리팩토링』, 김지원 옮김, 한빛미디어(2012)]]></content>
      <categories>
        <category>refactoring</category>
      </categories>
      <tags>
        <tag>리팩토링</tag>
        <tag>refactoring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] jpa entity equals, hashcode]]></title>
    <url>%2Fjpa%2Fjpa-entity-equals-hashcode%2F</url>
    <content type="text"><![CDATA[JPA는 영속성 컨텍스트의 키로 엔티티의 식별자를 사용한다. 식별자가 primitive 타입일 경우 ==, reference 타입일 경우 equals를 사용한다(아마도 당연히) 기본적인 wrapper 타입의 경우 equals가 잘 구현되어 있기 때문에 상관없으나 @EmbeddedId의 경우 equals, hashCode를 구현해주지 않으면 두 엔티티가 같다는 것을 보장해줄 수 없다. 123456789101112131415@Entityclass Member&#123; @EmbeddedId private MemberPK id; public Member(MemberPK id)&#123; this.id = id; &#125;&#125;@Embeddableclass MemberPK&#123; private Integer memberId; private String name;&#125; 12345Member member1 = new Member(new MemberPK(1, "joont"));Member member2 = new Member(new MemberPK(1, "joont")); em.persist(member1);em.persist(member2); MemberPK에 대해 eqauls, hashCode를 구현했다면 두 엔티티가 같다고 보장되므로 Member가 하나만 들어가지만, equals, hashCode를 구현하지 않았다면 똑같은 식별자를 가졌음에도 불구하고 영속성 컨텍스트에 두번 들어가게 된다. 그러므로 @EmbeddedId에 대해서는 꼭 equals, hashCode를 구현해줘야 한다. 아래는 또 다른 사용 사례에 관한 것이다. https://jojoldu.tistory.com/134]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>jpa equals hashcode</tag>
        <tag>equals</tag>
        <tag>hashcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OAS 파일 나누기]]></title>
    <url>%2FopenAPI%2FOAS-%ED%8C%8C%EC%9D%BC-%EB%82%98%EB%88%84%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[참고로 이 방식은 OAS 3.0 이상에서만 된다. 방식은 간단하다. # 앞에 파일 경로만 써주면 된다. 1234567891011121314151617181920212223242526272829# api.ymlitems/&#123;itemId&#125;: get: operationId: getItem summary: 아이템 조회 parameters: - $ref: 'items.yml#/parameters/itemId' responses: '200': description: OK content: application/json: schema: $ref: 'item.yml#/schemas/Item'# item.ymlparameters: itemId: in: path name: itemId required: true schema: type: integerschemas: Item: title: Item type: object properties: # ... 파일명 뒤에 #만 붙여주면 바로 접근 가능하다. item.yml은 api.yml과 같은 위치에 있다(상대경로로 접근했음) 폴더로 나누고 definitions/item.yml#schemas/Item 의 형태로 선언해도 된다. 보다시피 item.yml 파일에 최상위 레벨이 없다. 없어도 되기 때문이다. 단일파일에 작성할 때 components 아래 작성했던 부분을 파일로 나눴다고 생각하면 된다.]]></content>
      <categories>
        <category>openAPI</category>
      </categories>
      <tags>
        <tag>OAS file split</tag>
        <tag>swagger file split</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iterm2 꾸미기]]></title>
    <url>%2Fetc%2Fiterm2-%EA%BE%B8%EB%AF%B8%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[http://heetop.blogspot.com/2017/10/oh-my-zsh_12.html zsh, oh my zsh 까지 적용했다]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>iterm zsh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 엔티티 리스너]]></title>
    <url>%2Fjpa%2F%EC%97%94%ED%8B%B0%ED%8B%B0-%EB%A6%AC%EC%8A%A4%EB%84%88%2F</url>
    <content type="text"><![CDATA[엔티티의 생명주기에 따른 이벤트를 처리하고 싶을 수 있다. (삽입 시점, 업데이트 시점, 삭제 시점에 로그남기기 등) 그렇다고 직접 로직을 수행하는 곳을 찾아가 일일히 추가 로직을 처리하는 것은 너무 비효율적이다. JPA 리스너 기능을 사용하면 엔티티의 생명주기에 따른 이벤트를 처리할 수 있다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 엔티티 값을 변환해서 저장하기(@Converter)]]></title>
    <url>%2Fjpa%2F%EC%97%94%ED%8B%B0%ED%8B%B0-%EA%B0%92%EC%9D%84-%EB%B3%80%ED%99%98%ED%95%B4%EC%84%9C-%EC%A0%80%EC%9E%A5%ED%95%98%EA%B8%B0-Converter%2F</url>
    <content type="text"><![CDATA[@Converter를 사용하면 엔티티의 데이터를 변환해서 데이터베이스에 저장할 수 있다. 예를 들면 엔티티에는 boolean, 데이터베이스에는 YN 값을 저장하고 싶을 경우 정도가 있겠다. (기본적으로 boolean 타입으로 지정하면 0,1로 저장된다) 기본 사용법 123456789101112131415161718192021@Entityclass Member&#123; @Id @GeneratedValue private Integer id; @Convert(converter=BooleanToYNConverter.class) private boolean useYn;&#125;@Converterclass BooleanToYNConverter implements AttributeConverter&lt;Boolean, String&gt;&#123; @Override public String convertToDatabaseColumn(Boolean attribute)&#123; return (attribute != null &amp;&amp; attribute) ? "Y" : "N"; &#125; @Override public Boolean convertToEntityAttribute(String dbData)&#123; return "Y".eqauls(dbData); &#125;&#125; 보다시피 간단하다. AttributeConverter를 구현하면서 제네릭으로 엔티티 컬럼 타입, 데이터베이스 컬럼 타입을 주고, 아래 2개의 메서드를 오버라이드 해주면 된다(엔티티-&gt;데이터베이스, 데이터베이스-&gt;엔티티) 클래스 레벨 설정 클래스 레벨에도 설정할 수 있다. 단 이때는 attrbuteName 속성을 사용해서 어떤 필드에 컨버터를 적용할 지 명시해야한다. 12345@Entity@Converter(converter = BooleanToYNConverter.class, attributeName = "useYn")class Member&#123; // ...&#125; 글로벌 설정 모든 Boolean 타입에 설정하고 싶을 경우 아래와 같이 직접 컨버터에 명시해주면 된다. 1234@Converter(autoApply = true)class BooleanToYNConverter implements AttributeConverter&lt;Boolean, String&gt;&#123; // ...&#125; 이렿게 하면 엔티티에 따로 컨버터를 지정해주지 안항도 자동으로 컨버팅 된다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>@Converter</tag>
        <tag>AttributeConverter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 컬렉션과 부가기능]]></title>
    <url>%2Fjpa%2F%EC%BB%AC%EB%A0%89%EC%85%98%EA%B3%BC-%EB%B6%80%EA%B0%80%EA%B8%B0%EB%8A%A5%2F</url>
    <content type="text"><![CDATA[JPA는 자바에서 기본으로 제공하는 Collection, List, Set, Map 컬렉션을 지원하고, 아래와 같은 상황에서 컬렉션을 사용할 수 있다. @OneToMany, @ManyToMany 를 사용해서 일대다나 다대다 관계를 매핑할 때 @ElementCollection 을 사용해서 값 타입을 하나 이상 보관할 때 하이버네이트는 엔티티를 영속상태로 만들 때 컬렉션 필드를 하이버네이트에서 준비한 컬렉션으로 감싸서 사용한다. 이는 하이버네이트가 컬렉션을 효율적으로 관기하기 위함이다. 하이버네이트는 본 컬렉션을 감싸고 있는 내장 컬렉션을 생성한 뒤, 이 내장 컬렉션을 사용하도록 참조를 변경한다. Collection, List 중복을 허용하는 컬렉션이다. 하이버네이트에서 PersistentBag으로 래핑된다. 사용할 때는 ArrayList로 초기화하면 된다. 12345@OneToMany(mappedBy = "parent")Collection&lt;Child&gt; children = new ArrayList&lt;&gt;();// or@OneToMany(mappedBy = "parent")List&lt;Child&gt; children = new ArrayList&lt;&gt;(); 중복을 허용하는 특성때문에 겍체를 추가할때 아무 조건검사가 필요없으므로, 지연로딩이 발생하지 않는다. 하지만 엔티티가 있는지 체크하거나 삭제할 경우 eqauls로 비교해야 하므로 지연로딩이 발생한다. 1234children.add(child); // no action children.contains(child); // Lazy loading occurs because of using equalschildren.remove(child); // Lazy loading occurs because of using equals Set 중복을 허용하지 않는 컬렉션이다. 하이버네이트에서 PersistentSet으로 래핑된다. 사용할 때는 HashSet으로 초기화하면 된다. 12@OneToMany(mappedBy = "parent")Set&lt;Child&gt; children = new HashSet&lt;&gt;(); 중복을 허용하지 않으므로 객체를 추가할 때 마다 equals 메서드로 같은 객체가 있는지 비교한다. 즉 add 메서드만 수행해도 지연로딩이 발생한다. 참고로 HashSet은 해시 알고리즘을 사용하므로 equals와 hashCode를 같이 사용한다. 1234children.add(child); // eqauls + hashCodechildren.contains(child); // eqauls + hashCodechildren.remove(child); // eqauls + hashCode List + @OrderColumn 순서가 있는 복수형 컬렉션을 의미하는데, 데이터베이스에 순서값을 저장해서 조회할 때 사용한다는 의미이다. 위처럼 데이터베이스에 순서값을 함꼐 관리하는 테이블에 사용된다. 12345678910111213141516171819@Entityclass Board&#123; @Id @GeneratedValue private Integer id; @OneToMany(mappedBy = "board") @OrderColumn(name = "POSITION") private List&lt;Comment&gt; comments = new ArrayList&lt;&gt;();&#125;@Entityclass Comment&#123; @Id @GeneratedValue private Integer id; @ManyToOne @JoinColumn(name = "BOARD_ID") private Board board;&#125; List의 위치(순서)값을 POSITION이라는 컬럼에 저장하게 되는것이고, 이는 일대다 관계의 특성에 따라 다(N)쪽에 저장하게 된다. 아래는 사용예제이다. 12345678910Board board = new Board("title", "content");em.persist(board);Comment comment1 = new Comment("comment1");comment1.setBoard(board); // POSITION 0em.persist(comment1);Comment comment2 = new Comment("comment1");comment2.setBoard(board); // POSITION 1em.persist(comment2); 어떻게보면 위치값을 알아서 관리해주니 편해보이지만 사실은 실무에서 사용하기에는 단점이 많다. Comment가 POSITION의 값을 알 수 없다. Board에서 관리되기 때문이다. 이러한 특징때문에 위의 명령을 수행하면 comment1, comment2 insert 후에 POSITION 값을 수정하는 update가 2번 추가로 발생한다(ㄷㄷ) 요소가 하나만 변경되도 모든 위치값이 변경된다. 예를들어 첫번쨰 댓글을 삭제하면 그 뒤의 댓글들의 POSITION-- 하는 update가 댓글의 개수만큼 발생한다. 중간에 POSITION 값이 없으면 null이 저장된다. 예를들어 강제로 0,1,2의 POSITION 값을 0,2,3으로 변경하면 1번 위치에 null이 보관된 컬렉션이 반환된다. 이러면 NullPointerException이 발생한다. @OrderBy ORDER BY 절을 이용해서 컬렉션을 정렬하는 방법이다. @OrderColumn 처럼 순서용 컬럼을 매핑하지 않아도 된다. 12345678910111213141516171819202122@Entityclass Member&#123; @Id @GeneratedValue private Integer id; @OneToMany(mappedBy = "board") @OrderBy("createdDate desc") private List&lt;Comment&gt; comments = new ArrayList&lt;&gt;();&#125;@Entityclass Comment&#123; @Id @GeneratedValue private Integer id; @ManyToOne @JoinColumn(name = "BOARD_ID") private Board board; @Temporal(TemporalType.TIMESTAMP) private Date createdDate;&#125; 이렇게 하면 comments를 초기화 할 때 명시해놓은 ORDER BY 구문이 같이 수행되어 순서가 보장된다. 사용하는 컬럼명은 JPQL 때처럼 엔티티의 필드를 대상으로 한다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>Collection</tag>
        <tag>List</tag>
        <tag>Set</tag>
        <tag>@OrderColumn</tag>
        <tag>@OrderBy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] filter, interceptor 차이]]></title>
    <url>%2Fspring%2Ffilter-interceptor-%EC%B0%A8%EC%9D%B4%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[aws] iam]]></title>
    <url>%2Faws%2Fiam%2F</url>
    <content type="text"><![CDATA[http://victorydntmd.tistory.com/67 AWS 리소스에 대해 접근 권한을 부여하는 것이다. 개인에게 직접 부여할 수도 있고, 그룹에 권한을 부여한 뒤 사용자를 그룹에 포함시킬수도 있다. 유저를 생성하면 access key와 secret key가 저장된 csv 파일을 다운받을 수 있는데, 이는 외부에서 AWS 리소스 접근시에 사용되므로 잘 저장해두어야 한다.]]></content>
      <categories>
        <category>aws</category>
      </categories>
      <tags>
        <tag>iam role</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[aws] presigned url]]></title>
    <url>%2Faws%2Fpresigned-url%2F</url>
    <content type="text"><![CDATA[서버에서 직접 multipart를 받아 S3 버킷에 업로드하면 서버쪽에서 multipart 파일을 쥐고 있어야 하는 둥 리소스 낭비가 크므로 S3에 접근할 수 있는 Presigned url을 생성하여 클라이언트가 직접 업로드하게 한다. https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/PresignedUrlUploadObjectJavaSDK.html presigned url 이란 AWS S3 버킷에 바로 파일을 업로드 할 수 있는 URL을 말한다. 생성 및 사용 java sdk는 maven repository에서 간단하게 검색 가능하다 https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk 위 라이브러리를 임포트 하고 아래와 같이 작성하면 된다 12345678910111213141516171819202122232425@Autowired // spring 사용 시 DI 가능하다!private final AmazonS3 s3Client;@Value("$&#123;aws.s3.bucket&#125;")private String bucketName;public String getPresignedUrl()&#123; String fileName = UUID.randomUUID().toString(); Date expiration = new Date(); long expTimeMillis = expiration.getTime(); expTimeMillis += 1000 * 60 * 60; // 1시간 expiration.setTime(expTimeMillis); GeneratePresignedUrlRequest generatePresignedUrlRequest = new GeneratePresignedUrlRequest(bucketName, objectKey) .withMethod(HttpMethod.PUT) .withExpiration(expiration); generatePresignedUrlRequest.addRequestParameter(Headers.S3_CANNED_ACL, CannedAccessControlList.PublicRead.toString()); URL url = s3Client.generatePresignedUrl(generatePresignedUrlRequest); return url.toExternalForm();&#125; 이렇게 생성된 presigned url에 PUT 요청으로 multipart data를 보내면 파일을 S3에 업로드할 수 있게 된다. 업로드는 expiration time 동안 유효하다. 참고로 위의 소스에는 접근하는 S3에 대한 인증정보가 없는데, 이는 AmazonS3에서 사용하는 ProfileCredentialProvider 때문이다. 이 클래스는 저장된 credential을 읽어서 인증하는데, 실행되는 환경에 따라 이 credential 을 찾는 우선순위가 있다. (ec2에서 실행되면 ec2에 지정된 credential, pc에 저장된 credential 등등) 업로드한 파일 접근 업로드 이후 presigned-url에서 query param을 제거하고 GET 요청하면 해당 파일을 보거나(이미지) 다운로드 받을 수 있는데, 그냥 접근하면 access denied가 발생한다. 그러므로 presigned url 생성할 때 public read 권한을 줘야하고, 아래와 같이 하면 된다. http://zstd.github.io/amazon-presigned-url/ 12345678GeneratePresignedUrlRequest generatePresignedUrlRequest = new GeneratePresignedUrlRequest(bucketName, objectKey) .withMethod(HttpMethod.PUT) .withExpiration(expiration);// 이부분generatePresignedUrlRequest.addRequestParameter( Headers.S3_CANNED_ACL, CannedAccessControlList.PublicRead.toString());]]></content>
      <categories>
        <category>aws</category>
      </categories>
      <tags>
        <tag>aws presigned url</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] mysql port 파라미터 안될 때]]></title>
    <url>%2Fdb%2Fmysql-port-%EB%B3%80%EA%B2%BD%ED%95%B4%EC%84%9C-%EC%A0%91%EC%86%8D%2F</url>
    <content type="text"><![CDATA[docker로 3307 포트로 mysql을 띄웠는데 포트 변경해서 접속하는 옵션인 -P 를 아무리 줘도 무시되는 상황이 발생했다. 찾아보니 -h 가 localhost일 경우 소켓을 사용해서 포트 옵션이 무시되므로, 127.0.0.1을 써줘야 한다고 한다 https://serverfault.com/questions/306421/why-does-the-mysql-command-line-tool-ignore-the-port-parameter/306423?newreg=1c57059c005942bfbf0735b86bc570d6 1mysql -u root -h 127.0.0.1 -P 3307]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql port doesn&#39;t work</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 웹 어플리케이션과 영속성 관리]]></title>
    <url>%2Fjpa%2F%EC%9B%B9-%EC%96%B4%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98%EA%B3%BC-%EC%98%81%EC%86%8D%EC%84%B1-%EA%B4%80%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[트랜잭션 범위의 영속성 컨텍스트 스프링 컨테이너의 기본 전략 스프링 컨테이너는 트랜잭션 범위의 영속성 컨텍스트 전략을 기본으로 사용한다. 이 전략은 트랜잭션을 시작할 때 영속성 컨텍스트를 생성하고, 트랜잭션이 끝날 때 영속성 컨텍스트를 종료하는 방법이다. 스프링 트랜잭션 AOP는 @Transactional 어노테이션이 붙은 메서드가 호출될 때 트랜잭션을 시작한다. 메서드가 성공적으로 수행되면 해당 트랜잭션을 커밋하고, 예외가 발생한다면 트랜잭션을 롤백한다. 이 시점에 영속성 컨텍스트에 추가적인 작업을 호출한다. 트랜잭션을 커밋하면(메서드가 성공적으로 수행되면) 영속성 컨텍스트를 플러시해서 변경내용을 반영한 후 데이터베이스 트랜잭션을 커밋한다. 예외가 발생하면 플러시를 호출하지 않고 데이터베이스 트랜잭션을 롤백한다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Controllerclass HelloController&#123; @Autowired private HelloService helloService; @PostMapping("/&#123;teamId&#125;/members") public String addMember(@PathVariable Integer teamId, @RequestBody MemberDTO memberDTO)&#123; Member member = helloService.logic(teamId, memberDTO); // 준영속 상태 // ... return "/member/add_result"; &#125;&#125;@Serviceclass HelloService&#123; @Autowired private MemberRepository memberRepository; @Transactional public void logic(Integer teamId, MemberDTO memberDTO)&#123; Member member = memberRepository.addMember(memberDTO.toEntity()); Team team = teamRepository.findTeam(teamId); team.setMemberCnt(team.getMemberCnt()+1); return member; &#125;&#125;@Repositoryclass MemberRepository&#123; @PersistenceContext EntityManager em; public Member addMember(Member member)&#123; return em.persist(member); &#125;&#125;@Repositoryclass TeamRepository&#123; @PersistenceContext EntityManager em; public Team findTeam(Integer id)&#123; return em.find(Team.class, id); &#125;&#125; logic 메서드가 실행될 때 트랜잭션이 시작된다. logic 메서드가 종료되면 member, team에 대한 변경 내용이 데이터베이스에 플러시되고 트랜잭션이 커밋된다. 예외가 발생하면 변경 내용이 데이터베이스에 플러시되지 않고, 시작한 트랜잭션은 롤백된다. 트랜잭션과 영속성 컨텍스트의 생명주기가 같으므로, 트랜잭션이 끝남과 동시에 영속성 컨텍스트도 종료된다. 즉, HelloController에서 logic의 결과로 받은 Member 엔티티는 준영속 상태이다. 참고로 트랜잭션이 같으면 같은 영속성 컨텍스트를 사용한다. 위의 상황에서 MemberRepository와 TeamRepository는 서로 다른 엔티티 매니저를 주입받았지만 같은 영속성 컨텍스트를 사용한다. 이와 반대로 같은 엔티티 매니저를 사용해도 트랜잭션이 다르면 다른 영속성 컨텍스트를 사용한다(잘 안그려진다 상황이…) 준영속 상태와 지연로딩 위에서 언급했듯이 트랜잭션과 영속성 컨텍스트의 생명주기가 같기 때문에, 트랜잭션이 끝난 뒤의 엔티티는 준영속 상태가 된다. 즉 위의 상황에서 logic 메서드가 끝남과 동시에 영속성 컨텍스트도 종료되었기 때문에 결과로 반환된 Member 엔티티는 준영속 상태가 되는 것이다. 그리고 당연하게도, 준영속 상태인 엔티티에 지연로딩을 수행하게 되면 오류가 발생한다. 1234567891011121314151617181920@Controllerclass HelloController&#123; @Autowired private HelloService helloService; @PostMapping("/&#123;teamId&#125;/members") public String addMember( @PathVariable Integer teamId, @RequestBody MemberDTO memberDTO, ModelMap modelMap)&#123; Member member = helloService.logic(teamId, memberDTO); // ... modelMap.add("member", member); modelMap.add("teamName", member.getTeam().getName()); // lazy loading! but an exception occured return "/member/add_result"; &#125;&#125; 위처럼 지연로딩을 하게 되면 하이버네이트 기준으로 org.hibernate.LazyInitializationException이 발생한다. 이는 영속성 컨텍스트에 들어있지 않은 준영속 상태의 엔티티에 지연로딩을 시도했기 때문에 발생하는 것이다. 트랜잭션이 끝나면서 영속성 컨텍스트도 같이 종료되었기 때문에 반환된 엔티티는 자연스럽게 준영속 상태가 되었고, 이런 현상이 발생한 것이다. 하지만 생각해보면, 영속성 컨텍스트가 트랜잭션과 동시에 종료되고 프레젠테이션 계층까지 전파되지 않는것은 좋은 선택(?)이다. 만약 영속성 컨텍스트를 프레젠테이션 계층까지 열어두었다면 프레젠테이션 계층에서도 변경 감지가 동작하게 되어 위험하고, 각 계층이 가지는 역할자체도 모호해지기 때문이다. 하지만 위에서 봤다시피, 지연로딩이 동작하지 않는다는 점은 꽤나 골치아픈 일이다. 결국 위의 상황을 해결하고 싶으면 아래의 2가지 방법을 사용해야 한다. 뷰에 필요한 엔티티를 미리 로딩해두는 방법 말 그대로 영속성 컨텍스트가 살아있을 때 뷰에 필요한 엔티티들을 미리 다 로딩하거나 초기화해서 반환하는 방법이다. 아래의 3가지 방법이 있다. 글로벌 페치 전략 수정 fetchType을 EAGER로 바꾸는 방법이다. 12345class Member&#123; @ManyToOne(fetch=FetchType.EAGER) @JoinColumn(name = "team_id") private Team team;&#125; Member 조회 시 항상 Team을 같이 로딩해서 가지게 되므로, 준영속 상태가 되어도 지연로딩 문제가 발생하지 않는다. 이미 로딩해서 가지고 있기 때문이다. 하지만 이 방식은 아래와 같은 문제를 가진다. 사용하지 않는 엔티티를 로딩한다 뷰에서 Team이 필요하지 않은 경우도 있을것이다. 하지만 항상 Team을 같이 조회해야 한다. N+1 문제가 발생한다 12String sql = "SELECT m FROM Member m";List&lt;Member&gt; members = em.createQuery(sql, Member.class).getResultList(); 실행되는 SQL은 아래와 같다. 123456SELECT * FROM Member;SELECT * FROM Team WHERE id = ?;SELECT * FROM Team WHERE id = ?;SELECT * FROM Team WHERE id = ?;SELECT * FROM Team WHERE id = ?;.... JPQL을 실행할 때는 글로벌 페치 전략을 참고하지 않고 오직 JPQL만 참고하여 충실히 SQL을 만들기 때문에 발생한 현상이다. 위와 같이 처음 조회한 수만큼 다시 SQL을 사용해서 조회하는 것을 N+1 문제라고 한다. JPQL FetchJoin 봤다시피 fetchType을 EAGER로 바꾸는건 너무 비효율적이다. fetchType을 LAZY로 설정하고 필요할 때만 같이 조회해오도록 하는것이 좋을 것이고, JPQL에서는 FetchJoin이라는 기능으로 이를 지원한다. 사용법은 간단하다. 12String sql = "SELECT m FROM Member m JOIN FETCH m.team";List&lt;Member&gt; members = em.createQuery(sql, Member.class).getResultList(); 간단히 조인 명령어 마지막에 FETCH 만 넣어주면 된다. 이렇게 하면 해당 대상까지 조인으로 함꼐 조회해온 뒤 엔티티에 바인딩해준다. 이 방식이 현실적인 대안이긴 하지만, 화면에 맞춘 리파지토리 메서드가 증가할 수 있다는 단점이 있다. 즉, 아래와 같은 메서드들이 생길 수 있다는 것이다. Member만 조회해오는 repository.findMember 메서드 Member와 연관된 Team 까지 조회해오는 repository.findMemberWithTeam 메서드 이런식으로 계속 메서드가 추가되다보면 레파지토리와 뷰 간의 논리적인 의존관계가 발생하게 된다. 이런 상황에서는 최적화를 조금 포기하고 논리적 의존관계를 최소화하는 방법을 선택하던지(findMember와 findmemberWithTeam 통합), 최적화를 선택하고 논리적 의존관계를 가지고 가던지… 선택해야 한다. 사실상 성능에 미치는 영향이 미비하므로 뷰와 레파지토리의 의존관계가 급격하게 증가하는 것보다는 최적화를 포기하는것이 조금 나은 것 같다. 강제로 초기화 영속성 컨텍스트가 살아있을 때 프리젠테이션 계층이 필요한 엔티티를 강제로 초기화해서 반환하는 방법이다. 1234Member member = em.find(Member.class, 1);member.getTeam().getName(); // 프록시는 실제 사용하는 시점에 초기화된다return member; 강제로 초기화했으므로 준영속 상태에서도 사용할 수 있게된다. 하이버네이트를 사용한다면 initialize 메서드를 사용해요 프록시를 강제로 초기화할 수 있다. 1org.hibernate.Hibernate.initialize(member.getTeam()); 하지만 이것도 결국 생각해보면 프리젠테이션 계층이 은근슬쩍 서비스 계층을 침범하는 상황이다. 프리젠테이션 계층에 필요한 엔티티를 서비스 계층에서 초기화하고 있기 때문이다. 프리젠테이션 계층과 서비스 계층 사이에 FACADE 계층이라는 것을 둬서 이런 논리적 의존관계를 완전히 제거할 수 있다. FACADE 계층이랄게 특별할 건 없고, 그냥 중간에 계층하나를 더 두고 서비스 계층에서 받은 엔티티를 프리젠테이션 계층에서 필요한 형태로 가공해서 내려다주는 역할을 하는 것이다. 지연로딩 때문에 영속성 컨텍스트가 필요하므로, 트랜잭션은 FACADE 계층부터 시작해야한다. 얼핏보면 이렇게 함으로써 논리적 의존관계가 완전히 제거된 듯 보이지만, 실용적인 관점에서 보면 결국 코드를 훨씬 많이 작성하게 되고, 단순히 서비스 계층 호출을 위임하는 코드가 생길 가능성이 많다. OSIV OSIV(Open Session In View)는 영속성 컨텍스트를 뷰 까지 열어준다는 뜻이다. 영속성 컨텍스트가 살아있으면 엔티티는 영속 상태가 유지되므로, 뷰에서도 지연로딩을 사용할 수 있다. 이 기능의 핵심은, 뷰에서도 지연 로딩이 가능하다 이다. 요청 당 트랜잭션(Transaction Per Request) 뷰까지 영속성 컨텍스트를 열어두기 위해 가장 간단한 방법을 사용한다. 요청이 들어오자마자 필터나 인터셉터에서 트랜잭션을 시작하고, 요청이 끝날때 트랜잭션도 끝내는 것이다(트랜잭션과 영속성 컨텍스트의 생명주기가 같기 때문) 이렇게 하면 뷰에서도 지연로딩이 가능하므로 엔티티를 미리 초기화할 필요가 없다. 당연히 FACADE 계층도 필요없어진다. 하지만 트랜잭션이 프레젠테이션 영역까지 열리므로 프레젠테이션 계층이 엔티티를 변경할 수 있는 심각한 문제가 생긴다. 12345678910111213141516@Controllerclass HelloController&#123; @Autowired private HelloService helloService; @GetMapping("/member/&#123;memberId&#125;") public String addMember(@PathVariable Integer memberId, ModelMap modelMap)&#123; Member member = helloService.logic(memberId); member.setName("XXXX"); // 보안상의 이유로 XXXX로 세팅해서 내림 modelMap.add("member", member); return "/member/list"; &#125;&#125; 뷰를 렌터링 한 후 트랜잭션이 커밋될 것이고, 이때 변경 내역이 플러시되면서 회원의 이름이 XXXX로 변경되는 참사가 발생할 것이다. 프레젠테이션 계층은 데이터를 보여주는 계층이다. 이런 행위가 절대 허용되어서는 안된다. 이를 막기 위한 방법들은 아래와 같다. 엔티티를 읽기 전용 인터페이스로 제공 123456789101112131415interface MemberView&#123; // 보여줄 애들의 getter만 선언 public String getName();&#125;@Entityclass Member implements MemberView&#123; // getter 오버라이드&#125;class MemberService&#123; public MemberView getMember(Integer id)&#123; // memberView 반환 return memberRepository.findById(id); &#125;&#125; 엔티티 래핑 1234567891011121314151617class MemberWrapper&#123; private Member member; public MemberWrapper(Member member)&#123; this.member = member; &#125; public String getName()&#123; return member.getName(); &#125;&#125;class MemberService&#123; public MemberWrapper getMember(Integer id)&#123; // memberView 반환 return new MemberWrapper(memberRepository.findById(id)); &#125;&#125; DTO 전통적인 방법으로, 단순히 데이터만 전달하는 객체인 DTO를 만들고 보여줄 엔티티의 값을 세팅해서 내려주는 방법이다. 사실 이 방법은 OSIV의 장점을 못 살리는 방법이다. 강제로 초기화의 조금 다른 방법일 뿐이다. 이러한 문제점들로 인해 요청 당 트랜잭션은 거의 사용하지 않는다. 요즘에는 서비스 레벨까지만 트랜잭션을 내리는 방법을 사용하는데, 이게 스프링 프레임워크에서 제공하는 OSIV가 선택한 방식이다. 스프링 OSIV 아래는 spring-orm.jar에 들어있는 클래스들 중 하나이다. 필요한 위치에 따라 선택해서 사용하면 된다. JPA OEIV(OSIV) 서블릿 필터 org.springframework.org.jpa.support.OpenEntityManagerInViewFilter JPA OEIV(OSIV) 스프링 인터셉터 org.springframework.org.jpa.support.OpenEntityManagerInViewInterceptor 스프링 OSIV는 아래와 같이 OSIV를 사용하기는 하지만 트랜잭션은 비즈니스 계층에서만 사용한다. 클라이언트 요청이 들어오면 서블릿 필터나 스프링 인터셉터에서 영속성 컨텍스트를 생성한다. 트랜잭션은 시작하지 않는다. 서비스 계층에서 트랜잭션을 시작할 때(@Transactional) 1번에서 생성한 영속성 컨텍스트를 찾아와서 트랜잭션을 시작한다. 서비스 게층이 끝나면 영속성 컨텍스트를 플러시하고 트랜잭션을 커밋한다. 그리고 트랜잭션을 끝내지만 영속성 컨텍스트는 종료하지 않는다. 프레젠테이션까지 영속성 컨텍스트가 유지되므로 지연로딩이 가능하다. 서블릿 필터나 스프링 인터셉터로 요청이 돌아오면 영속성 컨텍스트를 종료한다. 이때 플러시는 하지 않는다. 참고로 여기서 강제로 플러시를 호출해도 예외(javax.persistence.TransactionRequiredException이 발생한다) 다시 간단하게 정리하면, 프레젠테이션 계층에서 트랜잭션이 없으므로 엔티티 변경이 불가능하다 하지만 영속성 컨텍스트는 열려있으므로 지연로딩이 가능하다 이런것이 가능한 이유는, 엔티티를 변경하지 않고 단순히 조회만 할 때는 트랜잭션이 없어도 되기 때문이다. 이를 Nontransaction reads라고 한다. (트랜잭션 없이 엔티티를 변경하면 javax.persistence.TransactionRequireException이 발생한다) 상당히 많은 부분이 해결되었지만, 여기에도 여전히 주의사항이 존재한다. 바로 프레젠테이션 계층에서 엔티티를 수정한 직후에 트랜잭션을 시작하는 서비스 계층을 만났을 경우이다. 1234567891011121314151617181920212223242526@Controllerclass HelloController&#123; @Autowired private HelloService helloService; @GetMapping("/member/&#123;memberId&#125;") public String addMember(@PathVariable Integer memberId, ModelMap modelMap)&#123; Member member = helloService.logic(memberId); member.setName("XXXX"); // 보안상의 이유로 XXXX로 세팅해서 내림 helloService.biz(); modelMap.add("member", member); return "/member/list"; &#125;&#125;@Serviceclass HelloService&#123; @Transactional public void biz()&#123; // ... &#125;&#125; 결과는 member의 이름이 XXXX로 바껴버린다. 기존의 트랜잭션 단위로 영속성 컨텍스트를 열던 것을 요청 단위로 영속성 컨텍스트를 여는 것으로 확장했기 때문에 하나의 요청에서 여러 트랜잭션이 있을 경우 영속성 컨텍스트를 공유하게 되고, 위와 같은 현상이 발생하는 것이다. 나는 OSIV를 이용해 엔티티를 프레젠테이션까지 내리는 행위는 딱히 좋지 않다고 생각한다. 위와 같은 문제점도 있고, 엔티티와 프레젠테이션 사이에 논리적인 의존관계가 생기는 것을 막을 수 없게 될 테니까… 전통적인 방법인 필요한 애들만 DTO로 만들어서 내려주는 방법이 가장 괜찮은 것 같긴한데, 이것또한 서비스 영역과 프레젠테이션 영역에 은근한 의존관계가 생긴다는 말을 반박할수는 없는 것 같다. OSIV 주의사항 영속성 컨텍스트가 리퀘스트까지 열린다는 것은, 커넥션을 유지하는 시간이 늘어남을 의미한다 이는 트래픽이 많아지면 OSIV를 사용하면 영속성 컨텍스트를 유지하는 영역이 트랜잭션 이상으로(request까지) 넓어지고, 이는 커넥션을 맺는 시간이 길어짐을 의미한다 (게다가 하나의 요청내에서는 db작업 외에 다른 많은 작업들이 수행될 수 있으므로(네트워크 통신 등), 커넥션을 계속 열어두는 행위는 좋지않다) request 까지 커넥션을 유지하게 됨. 데이터베이스에서 데이터를 가져와야 하기 때문이다 이는 성능상 이슈를 발생시킬 수 있으므로(커넥션 풀 개수 부족), OSIV를 끄고 service에서 DTO로 변환해서 내려주는 방식을 택하는 것이 좋다 위에서 반박할 수 없다고 해놓고…]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>영속성 컨텍스트</tag>
        <tag>OSIV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] MultipleBagException]]></title>
    <url>%2Fjpa%2FMultipleBagException%2F</url>
    <content type="text"><![CDATA[JPQL은 기본적으로 1 -&gt; N 방향으로 조인이 1번 까지밖에 안된다 2번 이상하게 되면 MultipleBagException 이 발생한다. 123// item아래 재고단위, 색상 등이 1:N으로 있다고 가정query.join(item.stockUnits, stockUnit).fetchJoin() .join(item.colors, color).fetchJoin() // MultipleBagException 발생 카테시안 곱이 일어나기 때문이라는데…(너무 많은 로우가 생기므로) @OneToMany 관계를 List가 아닌 Set으로 바꿔주면 위 에러가 발생하지 않는다 https://www.thoughts-on-java.org/hibernate-tips-how-to-avoid-hibernates-multiplebagfetchexception/ 이유는 모르겠다 젠장…]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>queryDSL</tag>
        <tag>JPQL</tag>
        <tag>1:N multiple join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[API documentation으로 좋은 redoc]]></title>
    <url>%2FopenAPI%2FAPI-documentation%EC%9C%BC%EB%A1%9C-%EC%A2%8B%EC%9D%80-redoc%2F</url>
    <content type="text"><![CDATA[https://github.com/Rebilly/ReDoc 작성한 OAS를 보기좋은 document로 만들어준다. https://github.com/Rebilly/ReDoc/blob/master/cli/README.md 1redoc-cli bundle &lt;spec file&gt; 하면 생성된다]]></content>
      <categories>
        <category>openAPI</category>
      </categories>
      <tags>
        <tag>redoc</tag>
        <tag>redoc-cli</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenApi codegen]]></title>
    <url>%2FopenAPI%2FOpenApi-codegen%2F</url>
    <content type="text"><![CDATA[openapi generator https://github.com/OpenAPITools/openapi-generator Open Api Spec을 통해 code를 generate 해주는 라이브러리 server stub, client stub 생성 가능 gradle plugin https://github.com/OpenAPITools/openapi-generator/tree/master/modules/openapi-generator-gradle-plugin openapi generator를 gradle에서 사용하기 위한 플러그인 123dependencies &#123; classpath "org.openapitools:openapi-generator-gradle-plugin:3.3.0"&#125; 1234567891011121314151617apply plugin: 'org.openapi.generator'// 위 repository README의 Configuration 참조 openApiGenerate &#123; generatorName = "kotlin" inputSpec = "$rootDir/specs/petstore-v3.0.yaml".toString() outputDir = "$buildDir/generated".toString() apiPackage = "org.openapi.example.api" invokerPackage = "org.openapi.example.invoker" modelPackage = "org.openapi.example.model" modelFilesConstrainedTo = [ "Error" ] configOptions = [ dateLibrary: "java8" ]&#125; Tip generate 된 코드에 상속 적용하기 allOf를 사용하면 generate된 코드에 상속을 사용할 수 있다 12345678910111213141516ChildDTO: title: ChildDTO description: description allOf: - $ref: '#/components/schemas/ParentDTO' - type: object required: - name - age properties: name: type: string description: 이름 example: ㅋㅋ age: type: integer 아래와 같이 generate 됨 1234class ChildDTO extends ParentDTO&#123; String name; Integer age;&#125;]]></content>
      <categories>
        <category>openAPI</category>
      </categories>
      <tags>
        <tag>openapi generator</tag>
        <tag>openapi codegen gradle plugin</tag>
        <tag>openapi 상속</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OAS 3.0]]></title>
    <url>%2FopenAPI%2FOAS-3-0%2F</url>
    <content type="text"><![CDATA[document 문서 git : https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.1.md swagger : https://swagger.io/docs/specification/basic-structure/ meta data, server 정보 12345678910openapi: 3.0.0info: title: Sample API description: Optional multiline or single-line description in [CommonMark](http://commonmark.org/help/) or HTML. version: 0.1.9servers: - url: http://api.example.com/v1 description: Optional server description, e.g. Main (production) server - url: http://staging-api.example.com description: Optional server description, e.g. Internal staging server for testing data type 기본 타입 https://swagger.io/docs/specification/data-models/data-types/ string, number, integer, boolean, array, object 사용가능 string 은 date와 file을 포함함 array는 items가 필수로 와야함. items 아래 type 혹은 $ref api 정의 path 정의 paths 아래에 정의한다. 123456789101112131415paths: /users: get: operationId: getUsers summary: get user description: get user description post: operationId: createUser summary: create user description: create given user /users/&#123;id&#125;: get: ... patch: deprecated: true path 아래 http method 별로 선언 가능 summary랑 description 차이가 뭐지? operationId는 나중에 code-gen 에서 메서드명으로 사용됨 path templating 할 수 있고, 파라미터에서 받을 수 있음 deprecated 할 수 있음 parameter 정의 https://swagger.io/docs/specification/describing-parameters/ 기본 형태는 아래와 같음 1234567891011paths: /users/&#123;id&#125;: get: parameters: - in: path name: id # 상단의 path명과 같아야함 required: true schema: type: integer minimum: 1 description: The user ID 파라미터 형태, 이름, 필수여부 등을 줄 수 있음. 추가적인 정보는 여기 확인 in에는 path, query, header, cookie 가 올 수 있음 path의 경우 url에 path templating과 이름을 동일하게 해줘야 한다는 점 빼고는 모두 동일하게 사용 가능 schema에서 전달받은 파라미터에 대해 정의함. 타입, 최소값 등을 줄 수 있음. 추가적인 정보는 여기 확인 request body https://swagger.io/docs/specification/describing-request-body/ 12345678910111213141516171819202122paths: /pets: post: summary: Add a new pet requestBody: description: Optional description in *Markdown* required: true content: application/json: schema: $ref: '#/components/schemas/Pet' /pets: post: summary: Add a new pet requestBody: description: Optional description in *Markdown* required: true content: application/json: schema: type: object properties: object 정의 https://swagger.io/docs/specification/components/ components 아래에 정의하고, 재사용을 목적으로 한다. $ref 속성을 이용하고 #으로 참조한다. 1$ref: '#/components/schemas/Item' #는 현 위치를 말한다. schemas 일반 오브젝트(DTO 등) parameters https://swagger.io/docs/specification/describing-parameters/#common-for-various-paths Common Parameters for Various Paths 부분 참조 responses https://swagger.io/docs/specification/describing-responses/ status 별로 선언가능하며 description, object 등을 내려줄 수 있다 enum reuse 1234567891011121314151617181920212223paths: /products: get: parameters: - in: query name: color required: true schema: $ref: '#/components/schemas/Color' responses: '200': description: OK components: schemas: Color: type: string enum: - black - white - red - green - blue]]></content>
      <categories>
        <category>openAPI</category>
      </categories>
      <tags>
        <tag>swagger 3.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] 마스터 데이터, 트랜잭션 데이터]]></title>
    <url>%2Fdb%2F%EB%A7%88%EC%8A%A4%ED%84%B0-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%98-%EB%8D%B0%EC%9D%B4%ED%84%B0%2F</url>
    <content type="text"><![CDATA[https://www.intricity.com/data-warehousing/master-data-vs-transaction-data/ 마스터 데이터 : 사람이나 기관이 관리하는 데이터 트랜잭션 데이터 : 마스터 데이터에 대한 이벤트 데이터 같은 것. 구매, 주문 등]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>transaction data</tag>
        <tag>master data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] best practice]]></title>
    <url>%2Fjpa%2Fbest-practice%2F</url>
    <content type="text"><![CDATA[Spring orm이 제공하는 것 LocalContainerEntityManagerFactory 테이블의 기본키 이름은 테이블이름을 prefix로 가져가는것이 좋다 엔티티의 경우 참조라는 값이 있지만 RDB의 경우는 없기 때문 게다가 테이블간 관계설정할 때 외래키에 테이블의 이름을 쓰는 관례가 많으므로 헷갈리지 않기 위해 기본키도 테이블 이름을 prefix로 가져주는 것이 좋다 @PersistenceContext : 스프링이나 J2EE를 사용하면 컨테이너가 직접 엔티티매니저를 관리하고 제공해준다. 이 어노테이션은 컨테이너가 관리하는 엔티티 매니저를 주입하는 어노테이션이다 @PersistenceUnit : EntityManagerFactory를 주입받고자 할 때 @Transactional은 RuntimeException만 롤백한다 엔티티 설계에서 참조할 수 있는 부분. 편의메서드가 내가 작성한 것 만큼 빡빡하게 조건검사를 하지 않는다 public void setMember(Member member){ this.member = member; member.getOrders().add(this); } public void addOrder(Order order){ orders.add(order); order.setmember(this); } 근데 책에서 설명할때는 bug를 방지하여 조건검사를 어느정도 넣어주는것을 봤는데, 이걸 제거해도 되는것인지? 회원 중복검사를 하는 로직은 service 메서드에서 private으로 구현했다 이미 등록된 회원일 경우 exception을 반환하는 식으로 처리했다 이러한 검증 로직이 있어도 멀티스레드 안정성을 보장하기 위해 UK를 걸어주는 것이 좋다 값이 setter를 통해 무분별하게 사용되지 않도록 메서드로 제한한다 public void addStock(int quantity) { this.stockQuantity += quantity; } public void removeStock(int quantity) { int restStock = this.stockQuantity - quantity; if (restStock &lt; 0) { throw new NotEnoughStockException(“need more stock”); } this.stockQuantity = restStock; } public void cancel() { if (delivery.getStatus() == DeliveryStatus.COMP) { throw new RuntimeException(&quot;이미 배송완료된 상품은 취소가 불가능합니다.&quot;); } this.setStatus(OrderStatus.CANCEL); for (OrderItem orderItem : orderItems) { orderItem.cancel(); } } 관계를 한방에 맺어주는 편의메서드를 제공한다. 이로인해 객체간의 관계도 파악가능하다. public static Order createOrder(Member member, Delivery delivery, OrderItem… orderItems) { Order order = new Order(); order.setMember(member); order.setDelivery(delivery); for (OrderItem orderItem : orderItems) { order.addOrderItem(orderItem); } order.setStatus(OrderStatus.ORDER); order.setOrderDate(new Date()); return order; } 추가적인 정보를 제공해주는 메서드를 만든다 public int getTotalPrice() { int totalPrice = 0; for (OrderItem orderItem : orderItems) { totalPrice += orderItem.getTotalPrice(); } return totalPrice; } 다른 엔티티와 값을 동기화한다(재고 등) public static OrderItem createOrderItem(Item item, int orderPrice, int count) { // 주문생성시 재고를 깎고 OrderItem orderItem = new OrderItem(); orderItem.setItem(item); orderItem.setOrderPrice(orderPrice); orderItem.setCount(count); item.removeStock(count); return orderItem; } public void cancel(){ // 주문취소시 재고를 더한다 getItem().addStock(count); } — 도메인 모델 패턴과 트랜잭션 스크립트 패턴 도메인 모델 패턴은 대부분의 비즈니스 로직이 엔티티에 속해있고, 서비스는 역할을 위임하고 트랜잭션 바인딩 정도로 사용되것이다 트랜잭션 스크립트 패턴은 서비스 계층에서 대부분의 비즈니스 로직을 처리하는 것을 말한다 수정에 대해서는 ‘영속화 + 필요한 메서드 변경’ 을 사용하던 ‘병합’을 사용하던 상관없다고 말하고 있음]]></content>
      <categories>
        <category>jpa</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch]]></title>
    <url>%2Fetc%2Felasticsearch%2F</url>
    <content type="text"><![CDATA[참고자료 https://sanghaklee.gitbooks.io/elk/content/ elk docker로 띄우기 https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html ELk를 마스터하면 어떤 빅데이터를 만나든 쉽게 다룰 수 있다? 로그 스테이시가 어떤 데이터든(csv든 상관없이) 수집해서 엘라스틱 서치에 수집해주고, 키바나는 데이터 비주얼라이션 툴로써 엘라스틱 서치의 데이터를 화면에 보기좋게 보여줌 엘리스틱 서치 : 키워드가 어떤 document에 있다고 저장하는 식이다 인덱스(가장 큰 개념)안에 타입, 타입안에 도큐먼트 REST API로 데이터 CRUD를 수행한다 인덱스 생성은 PUT으로 하고, document 생성은 POST로 한다 json 파일을 직접 넣을 수 있다 값 업데이트 : _update _update시에 script에 객체탐색형식으로 접근해서 값을 수정할 수 있다 bulk insert : _bulk bulk는 2개의 라인으로 구성되어있다 1번 라인은 metadata, 2번 라인은 데이터 데이터 라인에서 기본적인 데이터 타입은 자동으로 잡아서 mappings로 등록해주는 듯 하다 { “index” : { “_index” : “basketball”, “_type” : “record”, “_id” : “1” } } {“team” : “Chicago Bulls”,“name” : “Michael Jordan”, “points” : 30,“rebounds” : 3,“assists” : 4, “submit_date” : “1996-10-11”} { “index” : { “_index” : “basketball”, “_type” : “record”, “_id” : “2” } } {“team” : “Chicago Bulls”,“name” : “Michael Jordan”,“points” : 20,“rebounds” : 5,“assists” : 8, “submit_date” : “1996-10-11”} 이런 데이터를 등록하면 자동으로 mappings에. “mappings” : { “record” : { “properties” : { “assists” : { “type” : “long” }, “name” : { “type” : “text”, “fields” : { “keyword” : { “type” : “keyword”, “ignore_above” : 256 } } }, “points” : { “type” : “long” }, “rebounds” : { “type” : “long” }, “submit_date” : { “type” : “date” }, “team” : { “type” : “text”, “fields” : { “keyword” : { “type” : “keyword”, “ignore_above” : 256 } } } } } 식으로 등록이 되어있었다. 엘라스틱서치에 데이터매핑을 안할수 있긴 하지만 위험하다. (키바나 시각화 등) curl -XGET http://localhost:9200/classes?pretty 하면 mappings가 나오는데, 각 필드별 타입등을 지정해놓은 구간이다 classes/class/_mapping -d @json파일 형태로 지정 가능 이후 bulk insert하면 잘 들어간것을 확인할 수 있다는데, mapping이 있고 없고 차이가 뭔지? curl -XGET http://localhost:9200/{_index}/{_type}/_search?q=points:30 requestBody로도 검색 가능]]></content>
      <categories>
        <category>etc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[jpa] entity callback method]]></title>
    <url>%2Fjpa%2Fentity-callback-method%2F</url>
    <content type="text"><![CDATA[http://www.thejavageek.com/2014/05/23/jpa-lifecycle-callback-methods/ lifecycle call method http://www.thejavageek.com/2014/05/24/jpa-entitylisteners/ 클래스에 적용하는 법 class 단위 범용적인 life cycle을 적용하고 싶다면 EntityListener class 사용 entity에 접근할 수 있어야 하므로 callback method에서 단일파라미터로 엔티티를 받을 수 있다 엔티티리스너 클래스는 public no args 생성자가 있어야 한다 엔티티리스너를 엔티티에 붙이려면 아래와 같이 해야함 @EntityListeners 어노테이션을 엔티티에 선언해줘야 함. 여러개 붙일 수 있음 라이프사이클 이벤트가 발생하면 @EntityListeners에 선언된 애들 순서대로 생성되고 실행된다 이벤트리스너 내의 callback 메서드를 실행하면서 entity를 리스너에 전달한다(콜백 메서드가 있다면)]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>@PostLoad</tag>
        <tag>@PrePersist</tag>
        <tag>@PostPersist</tag>
        <tag>@PreUpdate</tag>
        <tag>@PostUpdate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] Spring Data JPA]]></title>
    <url>%2Fjpa%2FSpring-Data-JPA%2F</url>
    <content type="text"><![CDATA[Spring Data JPA는 스프링에서 JPA를 편리하게 사용할 수 있도록 지원하는 프로젝트다. 데이터 접근 계층을 개발할 때 지루하게 반복되는 CRUD 문제를 세련된 방법으로 해결할 수 있게 해준다. CRUD 처리를 위한 공통 인터페이스 제공 인터페이스만 작성하면 동적으로 구현체를 생성해서 주입해줌 따라서 인터페이스만 작성해도 개발을 완료할 수 있음 설정 maven repository에서 Spring Data Jpa 검색해서 gradle에 추가하면 된다. 1compile group: 'org.springframework.data', name: 'spring-data-jpa', version: '2.1.2.RELEASE' spring boot를 사용할 경우 Spring Boot Data JPA Starter를 사용해주는 것이 좋다. 1compile group: 'org.springframework.boot', name: 'spring-boot-starter-data-jpa', version: '2.1.1.RELEASE' 이후 spring config에 아래와 같이 작성한다. xml config 1&lt;jpa:repositories base-package="com.joon.repository"&gt; java config 123@Configuration@EnableJpaRepositories(basePackages = "com.joont.repository")public class AppConfig()&#123;&#125; 이렇게 설정해두면 Spring Data Jpa는 base package에 있는 레파지토리 인터페이스들을 찾아서 해당 인터페이스를 구현한 클래스들을 동적으로 생성한 다음, 빈으로 등록한다. JpaRepository(공통 인터페이스) JpaRepository는 앞서 언급했던 CRUD 처리를 위한 공통 인터페이스이다. 이 인터페이스를 상속받은 인터페이스만 생성하면 해당 엔티티에 대한 CRUD를 공짜로 사용할 수 있게된다. 123public interface MemberRepository extends JpaRepository&lt;Member, Long&gt;&#123;&#125; 제네릭에는 엔티티 클래스와 엔티티 클래스가 사용하는 식별자 타입을 넣어주면 된다. JpaRepository의 계층구조는 아래와 같다. 보다시피 스프링 데이터 프로젝트가 공통으로 사용하는 인터페이스를 사용하고 JpaRepository에서 JPA에 특화된 기능을 추가로 제공한다. 실제 구현체 공통 인터페이스인 JpaRespository는 org.springframework.data.jpa.repository.support.SimpleJpaRepository 클래스가 구현한다. 123456789101112131415@Repository // 1@Transactional(readOnly = true) // 2public class SimpleJpaRepository&lt;T, ID&gt; implements JpaRepository&lt;T, ID&gt;, JpaSpecificationExecutor&lt;T&gt; &#123; @Transactional // 3 public &lt;S extends T&gt; S save(S entity) &#123; // 4 if (entityInformation.isNew(entity)) &#123; em.persist(entity); return entity; &#125; else &#123; return em.merge(entity); &#125; &#125; // ....&#125; @Repository 적용 JPA 예외를 스프링이 추상화 한 예외로 변환한다. Transactional(readOnly = true) 적용 전체적으로 트랜잭션이 적용되어 있다. 이로인해 서비스에서 트랜잭션을 적용하지 않으면 레파지토리에서 트랜잭션을 시작하게 된다. 공통 인터페이스에는 조회하는 메서드가 많으므로 전체적으로 readOnly=true를 적용해서 약간의 성능향상을 얻는다. Transactional 조회 메서드가 아니라서 readOnly=true가 빠지게 한다. save 보다시피 저장할 엔티티가 새로운 엔티티면 저장하고 이미 있는 엔티티면 병합한다. 여기서 사용되는 entityInformation.isNew는 식별자가 객체일때는 null, primitive 타입일때는 0이면 새로운 객체라고 판단한다. Persistable 인터페이스를 구현한 객체를 빈으로 등록하면 위의 조건을 직접 정의할 수 있다. 사용 메서드 이름으로 쿼리 생성 인터페이스에 선언한 메서드의 이름으로 적절한 JPQL 쿼리를 생성해주는 마법같은(ㅋㅋ) 기능이다. 123public interface MemberRepository extends JpaRepository&lt;Member, Long&gt;&#123; List&lt;Member&gt; findByEmailAndName(String email, String name);&#125; 이렇게만 작성하면 Spring Data JPA가 메서드 이름을 분석해서 JPQL을 생성한다. 위의 메서드를 통해 생성되는 JPQL은 아래와 같다. 1SELECT m FROM Member m WHERE m.email = ?1 AND m.name = ?2 물론 정해진 규칙은 있다. 현재 보니까 생각보다 굉장히 많은 기능을 지원한다. https://docs.spring.io/spring-data/jpa/docs/current/reference/html/#repositories.query-methods.details findByNameAndEmail 의 형태로 작성. By 뒤부터 파싱 (조건은 여기에) findDistinctBy 로 distinct 가능 엔티티 탐색 가능. camel case로 해도 되지만 애매하니 findByAddress_ZipCode 처럼 _로 이어주는게 좋을 듯 findFirst3By, findTop10By, findFirstBy(1건) 로 limit 기능을 사용할 수 있다. findLast3By 같은건 없음 findByAgeOrderByNameDesc 처럼 order by 가능 findBy 말고 countBy, deleteBy도 있음 반환 타입 Spring Data JPA는 유연한 반환 타입을 지원한다. https://docs.spring.io/spring-data/jpa/docs/current/reference/html/#repository-query-return-types 기본적으로 결과가 한건 이상이면 컬렉션 인터페이스를 사용하고, 단건이면 반환 타입을 지정한다. 12List&lt;Member&gt; findByMember(String name); // 컬렉션 Member findByEmail(String email); // 단건 단건의 경우 T 형태와 Optional&lt;T&gt; 형태 2개로 받을 수 있다. 결과가 2건이상 나오면 javax.persistence.NonUniqueResultException 예외가 발생하고, 결과가 0건일 경우 T는 null, Optional&lt;T&gt;는 Optional.empty() 를 리턴한다. 참고로 단건의 경우 내부적으로 query.getSingleResult()를 사용해서 결과가 0건일 경우 javax.persistence.NoResultException이 발생해야하지만, 이는 다루기가 까다로우므로 exception을 발생시키지 않는 방향으로 기능을 제공한다. Named Query 엔티티나 xml에 작성한 Named Query도 찾아갈 수 있다. 1234567891011@Entity@NamedQuery( name = "Member.findByName", query = "select m from Member m where m.name = :name")public class Member&#123;&#125;public interface MemberRepository extends JpaRepository&lt;Member, Long&gt;&#123; List&lt;Member&gt; findByName(@Param("name") String name);&#125; Spring Data JPA는 메서드 이름으로 쿼리를 생성하기 전에 해당 이름의 Named Query를 먼저 찾는다(전략 변경 가능) 도메인 클래스 + .(점) + 메서드 이름으로 먼저 찾고, 없으면 JPQL을 생성한다. 위의 상황에서는 Member.findByName Named Query를 찾게 된다. JPQL 직접 정의 org.springframework.data.jpa.repository.Query 어노테이션을 사용하면 된다. 1234567891011// 위치기반 파라미터public interface MemberRepository extends JpaRepository&lt;Member, Long&gt;&#123; @Query("SELECT m FROM Member m WHERE m.name = ?1") Member findByName(String name);&#125;// 이름기반 파라미터public interface MemberRepository extends JpaRepository&lt;Member, Long&gt;&#123; @Query("SELECT m FROM Member m WHERE m.name = :name") Member findByName(@Param("name") String name);&#125; 아무리봐도 위치기반 파라미터는 개극혐이다. 내 주변사람에는 위치기반 파라미터를 쓰는 사람이 없으면 좋겠다. 네티티브 쿼리도 사용할 수 있다. nativeQuery = true 옵션만 주면 된다. 1234public interface MemberRepository extends JpaRepository&lt;Member, Long&gt;&#123; @Query(value = "SELECT * FROM Member WHERE name = ?0", nativeQuery = true) Member findByName(String name);&#125; 네이티브 쿼리는 위치기반 파라미터가 0부터 시작한다. 벌크 연산 12345public interface MemberRepository extends JpaRepository&lt;Member, Long&gt;&#123; @Modifying @Query("UPDATE Product p SET p.price = p.price * 1.1 WHERE p.stockAmount &lt; :stockAmount") int updatePrice(@Param("stockAmount") String stockAmount);&#125; @Modifying을 명시해줘야 한다. 기존 벌크 연산처럼 영향받은 엔티티의 개수를 반환한다. 알다시피 벌크 연산은 영속성 컨텍스트를 무시한다. 벌크 연산후에 영속성 컨텍스트를 초기화하고 싶으면 clearAutomatically 옵션을 true로 주면 된다. 기본값은 false이다. 123@Modifying(clearAutomatically = true)@Query("~~")// ~~~ 페이징과 정렬 아래의 두 파라미터를 사용하면 쿼리 메서드에 페이징과 정렬 기능을 추가할 수 있다. org.springframework.data.domain.Sort : 정렬기능 org.springframework.data.domain.Pageable : 페이징기능(Sort 포함) 사용법은 간단하다. 위의 두 클래스를 파라미터에 사용하기만 하면 된다. Pageable을 사용하면 반환타입으로 Page를 받을 수 있다. 해당 클래스를 사용하면 페이징과 관련된 다양항 정보들을 얻을 수 있다. 참고로 Page를 반환타입으로 받으면 전체 데이터 건수를 조회하는 count 쿼리가 추가로 날라간다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public interface Page&lt;T&gt; extends Slice&lt;T&gt; &#123; static &lt;T&gt; Page&lt;T&gt; empty() &#123; return empty(Pageable.unpaged()); &#125; static &lt;T&gt; Page&lt;T&gt; empty(Pageable pageable) &#123; return new PageImpl(Collections.emptyList(), pageable, 0L); &#125; int getTotalPages(); long getTotalElements(); &lt;U&gt; Page&lt;U&gt; map(Function&lt;? super T, ? extends U&gt; var1);&#125;public interface Slice&lt;T&gt; extends Streamable&lt;T&gt; &#123; int getNumber(); int getSize(); int getNumberOfElements(); List&lt;T&gt; getContent(); boolean hasContent(); Sort getSort(); boolean isFirst(); boolean isLast(); boolean hasNext(); boolean hasPrevious(); default Pageable getPageable() &#123; return PageRequest.of(this.getNumber(), this.getSize(), this.getSort()); &#125; Pageable nextPageable(); Pageable previousPageable(); &lt;U&gt; Slice&lt;U&gt; map(Function&lt;? super T, ? extends U&gt; var1);&#125; 아래는 간단한 사용 예제이다. 123456789// Pageable은 interface 이므로 구현체인 PageRequest 를 사용해야 한다. // 페이지, limit수, Sort 객체를 주면 된다 PageRequest pageRequest = PageRequest.of(0, 10, new Sort(Direction.DESC, "name"));Page&lt;Member&gt; result = memberRepository.findByNameStartingWith("김", pageRequest);result.getContet(); // 조회된 데이터 result.getTotalPages(); // 전체 페이지 수 result.hasNextPage(); // 다음 페이지 존재 여부 컨트롤러에서 사용 Pageable 객체를 Controller parameter로 직접 받을수도 있다 1234@GetMapping("/members")public String list(Pageable pageable)&#123; // ...&#125; 1/members?page=0&amp;limit=10&amp;sort=name,asc&amp;sort=age,desc pageable 기본값은 page=0, size=20이다. 정렬을 추가하고 싶으면 sort 파라미터를 계속 붙여주면 된다. 페이징 정보가 둘 이상이면 접두사를 사용해서 구분할 수 있다 123456@GetMapping("/members")public String list( @Qualifier("member") Pageable memberPageable, @Qualifier("order")Pageable orderPageable)&#123; // ...&#125; 1/members?member_page=0&amp;order_page=1 사용자 정의 레파지토리 구현 Spring Data JPA로 개발하면 인터페이스만 정의하고 구현체는 만들지 않는데, 다양한 이유로 메서드를 직접 구현해야 할 때도 있다. 그렇다고 레파지토리 자체를 직접 구현하면 공통 인터페이스가 제공하는 모든 기능까지 다 구현해야 한다. Spring Data JPA는 이런 문제를 우회해서 필요한 메서드만 구현할 수 있는 방법을 제공한다. 먼저 사용자가 직접 구현할 메서드를 위한 정의 인터페이스를 작성한다. 123public interface MemberRepositoryCustom&#123; public List&lt;Member&gt; search();&#125; 위의 사용자정의 인터페이스를 구현한 클래스를 작성한다. 이떄 클래스 이름은 레파지토리 인터페이스 이름 + Impl로 지어야한다. 이렇게 해야 Spring Data JPA가 사용자 정의 구현 클래스로 인식한다. 123456public class MemberRepositoryImpl implements MemberRepositoryCustom&#123; @Override public List&lt;Member&gt; search()&#123; // .... &#125;&#125; 이 클래스 이름 규칙은 변경할 수 있다. config에 설정했던 spring data jpa 설정의 속성값인 repository-impl-postfix 값을 지정해주면 된다. 기본값은 Impl 이다. 1@EnableJpaRepositories(basePackages = "com.joont.repository", repositoryImplementationPostfix = "impl") 마지막으로 레파지토리 인터페이스에서 사용자정의 인터페이스를 상속받으면 된다. 123public interface MemberRepository extends JpaRepository&lt;Member, Long&gt;, MemberRepositoryCustom&#123;&#125; spring data jpa + QueryDSL spring은 2가지 방법으로 QueryDSL을 지원하지만, 하나는 기능에 조금 한계가 있어서 다양한 기능을 사용하려면 JPAQuery를 직접 사용하거나 Spring Data JPA가 제공하는 QueryDslRepositorySupport를 사용하면 된다. 아래는 사용 예제이다. 코드를 직접 써야하므로 사용자 정의 레파지토리를 구현해야한다. 1234567891011121314151617181920212223242526public class OrderRepositoryImpl extends QueryDslRepositorySupport implements OrderRepositoryCustom &#123; public OrderRepositoryImpl() &#123; super(Order.class); &#125; @Override public List&lt;Order&gt; search(OrderSearch orderSearch, Pageable pa) &#123; QOrder order = QOrder.order; QMember member = QMember.member; JPQLQuery query = from(order); if (StringUtils.hasText(orderSearch.getMemberName())) &#123; query.leftJoin(order.member, member) .where(member.name.contains(orderSearch.getMemberName())); &#125; if (orderSearch.getOrderStatus() != null) &#123; query.where(order.status.eq(orderSearch.getOrderStatus())); &#125; return query.list(order); &#125;&#125; QueryDslRepositorySupport 클래스를 상속받아서 사용하고 있다. 참고로 생성자에서 QueryDslRepositorySupport에 클래스 정보를 넘겨줘야 한다. 기존에 생성하기 나름 번거로웠던 JPAQuery, JPAUpdateClause, JPADeleteClause 등을 간단하게 생성할 수 있다. 참고로 반환은 전부 인터페이스 타입인 JPQLQuery, UpdateClause, DeleteClause 등을 반환한다. 이 외에도 EntityManager, QueryDSL헬퍼 등을 반환하는 메서드도 구현되어 있다. QueryDSL에 Pageable 적용하기 QueryDslRepositorySupport를 사용하면 Spring Data JPA의 Pageable을 간단하게 적용할 수 있다. 123456789QItem item = QItem.item;JPQLQuery&lt;Item&gt; query = from(item);// making conditionquery.where(condition).distinct();long totalCount = query.fetchCount();List&lt;Item&gt; results = getQuerydsl().applyPagination(pageable, query).fetch();return new PageImpl&lt;&gt;(results, pageable, totalCount); Page를 리턴함으로써 완벽하게 Pageable을 사용할 수 있다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>findBy</tag>
        <tag>@Query</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac virtualbox 설치 실패]]></title>
    <url>%2Fetc%2Fmac-virtualbox-%EC%84%A4%EC%B9%98-%EC%8B%A4%ED%8C%A8%2F</url>
    <content type="text"><![CDATA[https://hongku.tistory.com/64 환경설정 - 보안 및 개인정보 보호 - 일반 - 아래쪽 허용버튼 클릭 - Oracle America 허용]]></content>
      <categories>
        <category>etc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[java] stream example]]></title>
    <url>%2Fjava%2Fstream-example%2F</url>
    <content type="text"><![CDATA[stream 도는 오브젝트에 추가적인 행위를 하기 1234list.stream() .map(mapper::toEntity) .peek(e -&gt; e.setParent(parent)) .forEach(childRepository::save); peek을 이용하여 중간중간 메서드들을 호출 가능하다. Stream을 이용하여 노출순서 정렬하기 123456789101112AtomicInteger i = new AtomicInteger(0);SomeDTOList.stream() .sorted(Comparator.comparingInt(SomeDTO::getDisplayOrder)) .peek(dto -&gt; dto.setDisplayOrder(i.incrementAndGet())) .peek(dto -&gt; &#123; AtomicInteger j = new AtomicInteger(0); dto.getChildren().stream() .sorted(Comparator.comparingInt(SomeChildDTO::getDisplayOrder)) .forEach(c -&gt; c.setDisplayOrder(j.incrementAndGet())); &#125;) .map(itemOptionMapper::toEntity) .forEach(repository::save); 자식들의 속성들까지 들고와 하나의 리스트로 합치기 1234Stream.of(category) .flatMap(c -&gt; c.getChildCategories().stream()) .map(Category::getId) .collect(Collectors.toList())]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[db] 락(lock)]]></title>
    <url>%2Fdb%2F%EB%9D%BD-lock%2F</url>
    <content type="text"><![CDATA[MySQL에서 사용하는 잠금은 크게 MySQL 엔진 레벨과 스토리지 엔진 레벨으로 나눌 수 있다. MySQL 엔진 레벨의 잠금은 모든 스토리제 엔진에 영향을 미치지만, 스트리지 엔진 레벨의 잠금은 스토리지 엔진 간 상호 영향을 미치지 않는다. MySQL 엔진 잠금 글로벌 락 MySQL 서버에 존재하는 모든 테이블에 잠금을 걸게되며, MySQL에서 제공하는 락의 범위중에 가장 크다. 1FLUSH TABLES WITH READ LOCK 명령으로 락을 획득할 수 있고(기존에 실행중인 락이 있으면 기다린다), 모든 테이블 모든 레코드에 변경이 불가능하게 된다. 서버의 미치는 영향이 크기 떄문에 웹 서비스용으로 사용되는 MySQL에서는 사용하지 않는것이 좋다. mysqldump 같은것이 우리가 모르는 사이에 내부적으로 이 명령을 실행하고 백업할 때도 있다. 테이블 락 개별 테이블 단위로 잠금을 거는 방식이며, 명시적 또는 묵시적으로 특정 테이블의 락을 획득할 수 있다. 묵시적 방법 MyISAM이나 Memory DB에서 데이터를 변경하는 쿼리를 실행하면 자동으로 테이블 락이 획득된다. (쿼리가 실행되는 동안 자동으로 획득됬다가 쿼리가 완료되면 자동으로 해제된다.) MyISAM이나 Memory의 경우 이 묵시적 LOCK이 해당된다. InnoDB의 경우 레코드 기반 잠금을 사용하기 때문에 변경 쿼리를 실행해도 테이블 잠금이 발생하진 않지만, 스키마를 변경하는 DDL 쿼리를 수행할 경우 테이블 락을 묵시적으로 사용한다. 명시적 방법 InnoDB도 아래와 같이 명시적으로 선언하여 Table LOCK을 획득할 수 있다. 1LOCK TABLES table_name [READ | WRITE] READ든 WRITE든 걸게 되면 다른 트랜잭션에서 해당 테이블에 변경 작업을 할 수 없게 된다. READ는 일관된 읽기를 위해 락을 거는 것이고, WRITE는 데이터 변경을 위해 락을 거는 것이다. Table READ 락의 경우 다른 트랜잭션에서 READ가 가능하지만 WRITE가 불가능하고, READ 락을 건 트랜젹션이 해당 테이블의 데이터 변경을 원한다면 다시 WRITE 락을 획득해야 한다. 아니면 Table was locked with a READ lock and can't be updated 와 같은 에러가 발생한다. Table WRITE 락의 경우 락을 건 트랜잭션만이 해당 테이블에 접근 가능하고, 다른 트랜잭션은 접근 불가능하다. 여기서 접근이란 read, write를 모두 포함하므로 Table WRITE 락이 걸린 테이블은 다른 트랜잭션에서 조회도 불가능하다. 1UNLOCK TABLES 위의 명령을 통해 트랜잭션에서 획득한 테이블 락을 해제할 수 있다. 유저 락 GET_LOCK 함수를 통해 잠금을 획득할 수 있으며, 단순히 사용자가 지정한 문자열에 대해 락을 획득하고 반납한다. 문자열에 대해 잠금을 획득한다는게 정확히 이해가 안간다… 문자열은 어쩌피 immutable 할텐데 락을 걸어야 할 이유가 있을까? 네임 락 db 객체(테이블 등)의 이름을 변경하는 경우 획득하는 잠금이다. 명시적으로 획득하거나 해제할 수 있는것은 아니고, RENAME TABLE a TO b 처럼 테이블의 이름을 변경하는 경우 자동으로 획득하는 잠금이다. 스토리지 엔진 잠금 MyISAM, MEMOERY 스토리지 엔진 잠금 자체적인 잠금을 가지고 있지 않고 MySQL 엔진에서 제공하는 테이블 락을 그대로 사용한다. InnoDB 스토리지 엔진 잠금 InnoDB의 경우 레코드 기반 잠금 방식을 사용한다. 이로 인해 훨씬 뛰어난 동시성 처리를 제공할 수 있게된다. 잠금 방식 비관적 잠금 변경하고자 하는 레코드에 대해 잠금을 먼저 획득하고 변경 작업을 처리하는 방식 현재 변경하고자 하는 레코드를 다른 트랜잭션에서도 변경할 수 있다는 비관적 가정을 하기 때문에, 먼저 잠금을 획득 높은 동시성 처리에 유리하며, InnoDB가 기본으로 채택하고 있는 방식임 낙관적 잠금 각 트랜잭션이 같은 레코드를 변경할 가능성은 희박할 것이라고 낙관적으로 가정 변경 작업을 먼저 수행하고, 마지막에 잠금 충돌이 있는지 확인 문제가 있었다면 ROLLBACK 처리 잠금 종류 레코드 락(Record Lock) 레코드 자체만을 잠그는 행위를 말한다. 다른 상용 DBMS의 레코드 락과 달리 InnoDB의 경우 인덱스를 참조하여 레코드를 잠근다는 큰 특징이 있다. 갭 락(Gap Lock) 레코드 자체가 아니라 레코드와 바로 인접한 레코드 사이의 간격만을 잠그는 것을 말한다. 레코드와 레코드 사이 간격에 새로운 레코드가 생성되는 것을 제어하기 위함이다. 개념일 뿐 자체적으로 사용되지는 않고, 넥스트락의 일부로 사용된다. 넥스트 키 락(Next Key Lock) 레코드락과 갭 락을 합쳐놓은 형태의 잠금을 말한다. 자동 증가 락(Auto Increment Lock) 인덱스와 잠금 위에서 언급했듯이 InnoDB의 잠금은 레코드를 바로 잠그는 것이 아니라, 인덱스를 사용하여 레코드를 잠근다. 아래와 같은 상황이 있다고 하자. 1234567-- index : ix_firstname(firstname에 대한 index) -- 250건SELECT COUNT(*) FROM employees WHERE first_name = 'Georgi';-- 1건 SELECT COUNT(*) FROM employees WHERE first_name = 'Georgi' AND last_name = 'Klasen'; 이 상황에서 아래와 같은 쿼리를 실행하게 되면, 1UPDATE employees SET hire_date = NOW() WHERE first_name = 'Georgi' AND last_name = 'Klasen'; 업데이트 될 레코드는 1건이지만, 인덱스로 필터할 수 있는 레코드의 개수는 250개가 한계이다. last_name에 대한 인덱스는 없고, first_name에 대한 인덱스만 있기 때문이다. 즉, 최종적으로 first_name = 'Georgi' 에 해당하는 250건의 레코드가 모두 잠기는 현상이 발생한다. 이러한 특징 떄문에 UPDATE나 DELETE 문장을 위한 적절한 인덱스가 준비되어 있어야 한다. 그렇지 않으면 동시성이 상당히 떨어져서 한 세션에서 변경작업을 하는 중에는 다른 세션에서는 그 테이블을 변경하지 못하고 기다려야 하는 상황이 발생할 것이다. 인덱스가 없는 컬럼을 조건으로 변경 작업을 하게 될 경우, 테이블의 모든 레코드에 대해 내부 클러스터드 인덱스를 이용해 락을 걸게된다. 즉 last_name = 'Klasen'과 같은 조건으로 update 문을 실행하게 되면 외부에서 다른 아무 데이터도 수정할 수 없는 상황이 발생하게 되는 것이다. 이러한 특징 때문에 MySQL Client Tool(workbench 등)에서 기본적으로 index가 없는 컬럼으로 변경쿼리를 못 날리게 되어있는 것 같다(safe update 거리면서…) 해결법?(정확하지 않음) 이런 불필요한 레코드 잠금 현상은 InnoDB의 넥스트 키 락 때문에 발생하는 것이다. 넥스트 키 락의 경우 MySQL의 기본 isolation level인 REPETABLE READ에서 디폴트로 사용하는 잠금 방식이다. 여기서 isolaton level을 READ COMMITTED로 바꿔주면 불필요한 잠금 대신 실제 변경하는 레코드만 락을 거는 방식을 사용할 수 있게 된다. 그런데 MySQL 5.1 이상에서는 바이너리 로그가 활성화되면 최소 REPETABLE READ 이상의 격리 수준을 사용하도록 강제되고 있다. 그러므로 바이너리 로그를 사용하지 않아도 되는 상황이면, 바이너리 로그를 사용하지 않도록 설정하고 isolation level을 READ COMMITTED로 바꾸는 방법도 고려해 볼만하다. 참고로 READ COMMITTED로 불필요한 잠금이 없어졌다고 해서 엄청난 성능향상이 있는것은 아니다. 인덱스로 조회된 레코드에 모두 락을 거는 방식은 똑같은데, 이후에 바로 불필요한 부분에 대해서 락을 해제하는 식으로 동작하기 때문이다. 그러므로 최대한 인덱스를 사용헐 수 있게 튜닝해주는 것이 좋다. 참고 : 이성욱, 『Real MySQL』, 위키북스(2012)]]></content>
      <categories>
        <category>db</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[jpa] QueryDSL]]></title>
    <url>%2Fjpa%2FQueryDSL%2F</url>
    <content type="text"><![CDATA[JPQL을 편하게, 동적으로 작성할 수 있도록 JPA에서 공식 지원하는 Creteria 라는것이 있다. 하지만 큰 단점이 있는데, 너무 불편하다는 것이다. 그에 반해 JPA에서 공식 지원하지는 않지만 쿼리를 문자가 아닌 코드로 작성해도 쉽고 간결하며, 모양도 쿼리와 비슷하게 개발할 수 있는 QueryDSL 이라는 것이 있다. QueryDSL은 오픈소스 프로젝트이며, 이름 그대로 데이터를 조회하는데 기능이 특화되어 있다. 최범균님이 번역한 공식 한국어 문서를 제공한다. http://www.querydsl.com/static/querydsl/4.0.1/reference/ko-KR/html_single/ 설정 필요 라이브러리 123456789101112&lt;dependency&gt;&lt;groupId&gt;com.querydsl&lt;/groupId&gt;&lt;artifactId&gt;querydsl-apt&lt;/artifactId&gt;&lt;version&gt;$&#123;querydsl.version&#125;&lt;/version&gt;&lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;com.querydsl&lt;/groupId&gt;&lt;artifactId&gt;querydsl-jpa&lt;/artifactId&gt;&lt;version&gt;$&#123;querydsl.version&#125;&lt;/version&gt;&lt;/dependency&gt; querydsl-jpa : QueryDSL JPA 라이브러리 querydsl-apt : 쿼리 타입(Q)를 생성할 때 사용하는 라이브러리 쿼리 타입 엔티티를 기반으로 생성된 쿼리용 클래스를 말한다. 12345678910111213141516171819202122232425buildscript &#123; repositories &#123; mavenCentral() maven &#123; url "https://plugins.gradle.org/m2/" &#125; &#125; dependencies &#123; classpath('net.ltgt.gradle:gradle-apt-plugin:0.18') &#125;&#125;repositories &#123; mavenCentral()&#125;apply plugin: "net.ltgt.apt"apply plugin: "net.ltgt.apt-idea”compile "org.projectlombok:lombok:$&#123;lombok_version&#125;"annotationProcessor "org.projectlombok:lombok:$&#123;lombok_version&#125;"compile "com.querydsl:querydsl-jpa:$&#123;querydsl_version&#125;"compile "com.querydsl:querydsl-core:$&#123;querydsl_version&#125;"compile "com.querydsl:querydsl-apt:$&#123;querydsl_version&#125;"annotationProcessor "com.querydsl:querydsl-apt:$&#123;querydsl_version&#125;:jpa"annotationProcessor "org.hibernate.javax.persistence:hibernate-jpa-2.1-api:$&#123;hibernate_jpa_api_version&#125;" 빌드하면 지정한 outputDirectory에 지정한 target/generated-sources 위치에 QMember.java 처럼 Q로 시작하는 쿼리 타입들이 생성된다. 사용(4.1.3 버전 기준) 기본 사용법 동적으로 생성할 쿼리는 JPAQuery를 사용하여 만들 수 있는데, 이것보단 JPAQueryFactory를 사용하는게 권장된다고 한다. JPQLQuery 인터페이스가 queryDSL 동적 쿼리 생성의 기준이 되는 인터페이스이고, JPAQuery는 JPQLQuery를 구현한 클래스이다. 근데 왜 이름이 JPAQuery일까? 1234567JPAQueryFactory queryFactory = new JPAQueryFactory(em);QMember member = QMember.member;Member foundMember = queryFactory.selectFrom(member) // select + from .where(customer.username.eq("joont")) .fetchOne(); 대충 위의 형태로 사용할 수 있다. 결과반환 fetch : 조회 대상이 여러건일 경우. 컬렉션 반환 fetchOne : 조회 대상이 1건일 경우(1건 이상일 경우 에러). generic에 지정한 타입으로 반환 fetchFirst : 조회 대상이 1건이든 1건 이상이든 무조건 1건만 반환. 내부에 보면 return limit(1).fetchOne() 으로 되어있음 fetchCount : 개수 조회. long 타입 반환 fetchResults : 조회한 리스트 + 전체 개수를 포함한 QueryResults 반환. count 쿼리가 추가로 실행된다. 프로젝션 프로젝션을 지정한다. 1234List&lt;Member&gt; foundMembers = queryFactory.select(member) .from(member, order) .fetch(); (아직 나오진 않았지만 from 절에 위처럼 쿼리 타입을 연속으로 줄 경우, 두 엔티티가 조인된다.) member와 order가 조인된 상태에서 member 엔티티의 속성만 가져온다. (select를 생략하면 기본적으로 from의 첫번째 엔티티를 프로젝션 대상으로 쓴다) from 쿼리할 대상을 지정한다. 123List&lt;Member&gt; foundMembers = queryFactory.from(member) .fetch(); member 테이블을 전체 조회하게 된다. 프로젝션 지정(select)가 빠졌지만 위와 동일하게 from의 첫번째 엔티티를 사용한다. from과 select를 나누기 보단 selectFrom 절을 쓰는것이 더 낫다. 조인 join, innerJoin, leftJoin, rightJoin 을 지원한다. 개인적으로 from절에 multiple arguments를 주는것보다 이게 더 좋다.(SQL에서도…) 123456QTeam team = QTeam.team;List&lt;Member&gt; foundMembers = queryFactory.selectFrom(member) .innerJoin(member.team, team) .fetch(); join의 첫번쨰 인자로는 join할 대상, 두번쨰 인자로는 join할 대상의 쿼리 타입을 주면 된다. on 절은 자동으로 붙는다. 추가적인 on 절도 사용할 수 있다. 12345List&lt;Member&gt; foundMembers = queryFactory.selectFrom(member) .innerJoin(member.team, team) .on(member.username.eq("joont")) .fetch(); 조건 1234567List&lt;Member&gt; foundMembers = queryFactory.selectFrom(member) .where(member.username.eq("joont")) // 1. 단일 조건 .where(member.username.eq("joont"), member.homeAddress.city.eq("seoul")) // 2. 복수 조건. and로 묶임 .where(member.username.eq("joont").or(member.homeAddress.city.eq("seoul"))) // 3. 복수 조건. and나 or를 직접 명시할 수 있음 .where((member.username.eq("joont").or(member.homeAddress.city.eq("seoul"))).and(member.username.eq("joont").or(member.homeAddress.city.eq("busan")))) .fetch(); (E1 and E2) or (E3 and E4) 같은 형태도 가능하다. 그냥 괄호로 묶어주면 된다. 1234List&lt;Member&gt; foundMembers = queryFactory.selectFrom(member) .where((member.username.eq("joont").or(member.homeAddress.city.eq("seoul"))).and(member.username.eq("joont").or(member.homeAddress.city.eq("busan")))) .fetch(); 두가지 조건이 괄호로 묶이게 되었을때, or 이면 합집합이고 and 이면 교집합이다. 참고로 (E1 and E2) or (E3 and E4) 는 괄호가 생략되고 (E1 or E2) and (E3 or E4) 는 잘 동작한다. 그룹핑 group by도 가능하다. 12345List&lt;String&gt; foundCities = queryFactory.from(member) .select(member.homeAddress.city) .groupBy(member.homeAddress.city) .fetch(); city로 group by 한 뒤 city만 출력하게 된다. having도 가능하다. 집계함수도 쓸 수 있다. 123456List&lt;String&gt; foundItems = queryFactory.select(item.category) // category가 그냥 String이라고 가정 .from(item) .groupBy(item.category) .having(item.price.avg().gt(1000)) // 집계함수 사용 .fetch(); 정렬 1234List&lt;Member&gt; foundMembers = queryFactory.selectFrom(member) .orderBy(member.id.asc(), member.username.desc()) .fetch(); 페이징 시작 인덱스를 지정하는 offset, 조회할 개수를 지정하는 limit, 두개를 인수로 받는 QueryModifiers를 사용하는 restrict를 지원한다. 근데 실제로 페이징 처리를 하려면 전체 데이터 개수를 알고 있어야하므로, fetchResults()를 사용해야 한다. 12345678910QueryResults&lt;Member&gt; result = queryFactory.selectFrom(member) .offset(10) .limit(10) .fetchResults();List&lt;Member&gt; foundMembers = result.getResults(); // 조회된 memberlong total = result.getTotal(); // 전체 개수 long offset = result.getOffset(); // offsetlong limit = result.getLimit(); // limit 다중 결과 반환 다중 프로젝션 할 경우 Tuple 클래스로 받을 수 있다. 1234567List&lt;Tuple&gt; foundMembers = queryFactory.select(member.username, member.homeAddress.city) .from(member) .fetch();System.out.println(founeMembers.get(0));System.out.println(founeMembers.get(1)); 리턴되는 클래스가 class com.querydsl.core.types.QTuple$TupleImpl 인데, 이것보단 아래 빈 생성(bean population)을 쓰는게 더 나아보인다. 빈 생성 자바빈을 말한다(스프링 빈 아님). 출력되는 다중 결과를 빈으로 변경해서 리턴할 수 있다. 1234List&lt;MemberDTO&gt; foundMembers = queryFactory.select(Projections.bean(UserDTO.class, member.username, member.homeAddress.city)) .from(member) .fetch(); 위의 bean 메서드를 호출하면 전달받은 인자와 동일하게 UserDTO의 setter를 호출한다. field 메서드를 사용하면 필드에 직접 접근하고(private도 가능), constructor 메서드를 사용하면 생성자를 사용한다. 지정한 프로젝션과 파라미터 순서가 같은 생성자가 필요하다. 엔티티의 필드명과 빈의 필드명이 다를 경우 아래와 같이 사용할 수 있다. 12queryFactory.select(Projections.bean(UserDTO.class, member.username.as("name"), member.homeAddress.city)) .... Member 엔티티 필드 username을 MemberDTO의 name에 전달하게 된다. 서브쿼리 JPAExpression 을 사용하면 된다. 1234567891011QMember member = QMember.member;QMember subQueryMember = new QMember("subQueryMember"); // 추가로 생성해줘야 함 List&lt;Tuple&gt; foundMembers = queryFactory.select(member.name, member.homeAddress.city) .from(member) .where(member.name.in( JPAExpressions.select(memberForSubquery.name) .from(memberForSubquery) )) .fetch(); 동적 조건 com.querydsl.core.BooleanBuilder 를 사용하면 동적 조건을 생성할 수 있다. 123456789101112BooleanBuilder builder = new BooleanBuilder();if(param.getId() != null)&#123; builder.and(member.id.eq(param.getId()));&#125;if(param.getName() != null)&#123; builder.and(member.name.contains(param.getName()));&#125;List&lt;Member&gt; list = queryFactory.selectFrom(member) .where(booleanBuilder) .fetch(); 수정, 삭제, 배치 쿼리 update JPAUpdateClause 클래스를 통해 실행할 수 있다.(인터페이스는 UpdateClause이다) JPAQueryFactory의 update 메서드를 통해 생성할 수 있다. 12345QCustomer customer = QCustomer.customer;// rename customers named Bob to BobbyqueryFactory.update(customer).where(customer.name.eq("Bob")) .set(customer.name, "Bobby") .execute(); delete JPADeleteClause 클래스를 통해 실행할 수 있다.(인터페이스는 DeleteClause이다) JPAQueryFactory의 delete 메서드를 통해 생성할 수 있다. 12345QCustomer customer = QCustomer.customer;// delete all customersqueryFactory.delete(customer).execute();// delete all customers with a level less than 3queryFactory.delete(customer).where(customer.level.lt(3)).execute();]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>QueryDSL 문법</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Swagger와 OAS의 관계]]></title>
    <url>%2FopenAPI%2FSwagger%EC%99%80-OAS%EC%9D%98-%EA%B4%80%EA%B3%84%2F</url>
    <content type="text"><![CDATA[누가 이 글에 잘못된 것좀 알려주세요 원래는 기업별로 각각 OpenAPI를 제공하는 명세가 있었다. (swagger를 만든 smart bear도 자신들의 OpenAPI specification이 있었다) 2015년에 이 모든 기업들이 OpenAPI initiative 라는 그룹으로 통합(가입?)하었고, 여기서 공통된 spec인 Open API specification을 제공한다. https://github.com/OAI/OpenAPI-Specification 그러므로 swagger도 문서에 보면 2.0일때는 상단에 swagger: &quot;2.0&quot; 이었는데, 3.0부터 openapi: &quot;3.0&quot;으로 변경되었다. (근데 통합인데 왜 3.0부터 가는건지… swagger가 주도하나…)]]></content>
      <categories>
        <category>openAPI</category>
      </categories>
      <tags>
        <tag>swagger</tag>
        <tag>OAS</tag>
        <tag>difference between swagger and OAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] git-crypt]]></title>
    <url>%2Fgit%2Fgit-crypt%2F</url>
    <content type="text"><![CDATA[git-crypt란? git-crypt를 사용하면 특정 파일 또는 폴더를 원격 repository에 올릴때는 encrypt하고, 로컬로 내려받을때는 decrypt 하는 식으로 관리할 수 있다. git-crypt repostiroy : https://github.com/AGWA/git-crypt git-crypt 적용법 : https://s-opensource.org/2017/07/18/introduction-gpg-encryption-git-crypt/ 적용법 12345brew install git-cryptcd /path/to/repositorygit-crypt init 하면 .git/git-crypt가 추가되고, git-crypt 할 수 있게 됨 그리고 repository root 경로애 .gitattributes 파일을 만들고 내부에 git-crypt가 적용될 파일을 적어주면 된다. 12config/production/** filter=git-crypt diff=git-crypt# more... 원하는 패턴이 더 있을 경우 밑에 나열해주면 된다.(.gitignore과 비슷하게 작성한다) 기본적으로 초기에 git-crypt를 설정한 사람은 올리고 내릴때 자동으로 encrypt decrypt가 되지만, 다른 collaborator들은 적용되지 않으므로 추가적인 설정이 필요하다. gpg 적용 상단의 git-crypt 적용법 보고 따라하면 될 듯 추가하고 싶은 collaborator의 pc에서 gpg 생성 project 생성자가 git-crypt add-gpg-user 로 collaborator의 gpg를 추가 pull 받은 collaborator는 git-crypt unlock 으로 간단하게 unlocking 가능 테스트 안해봄 key export import encrypt 할 때 사용한 key를 export하고, collaborator는 해당 key로 unlock 하는 방법 repository의 git-crypt key값을 export 하여 collaborator에게 전달하고, 1git-crypt export-key /path/to/key 전달받은 collaborator는 해당 key 파일을 사용하여 1git-crypt unlock /path/to/key 의 형태로 decrypt 한다. 기타 만약 레파지토리끼리 키를 공유하고 싶으면? 한쪽 레파지토리에서 생성된 .git/git-crypt/keys/default를 다른 레파지토리에서 경로 그대로 복사해서 사용하면 됨 근데 이 방식으로 git-crypt unlock이 바로 되지는 않는다. 번거롭게 key 파일 위치를 지정해줘야 한다.]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git-crypt 적용</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] 객체 클로닝]]></title>
    <url>%2Fjava%2F%EA%B0%9D%EC%B2%B4-%ED%81%B4%EB%A1%9C%EB%8B%9D%2F</url>
    <content type="text"><![CDATA[http://javacan.tistory.com/entry/31 Cloneable 인터페이스를 implements하고(안하면 CloneNotSupportedException 발생), clone 메서드를 오버라이드 하고 Object의 clone(super.clone)을 실행한다. clone 메서드가 수행되면 원본과 같은 객체를 새로 생성하고, 모든 필드들을 원본의 필드들과 같은 값으로 초기화한다. (생성자는 실행되지 않음) 단순하게 대입에 의해 복사되는 형태이기 때문에, 배열이나 객체의 경우 참조값만 복사되게 된다. 즉, 원본 객체에 대해 deep clone 하지 않고 shallow clone 한다는 뜻이다. 따라서 객체가 가지고 있는 배열이나 객체에 Cloneable 인터페이스를 구현하고 clone을 오버라이딩 해줘야하고, (배열은 기본적으로 clone이 구현되어 있음) 클로닝의 대상이 되는 객체에서 해당 필드까지 전부 클로닝을 실행해줘야 한다. 클로닝 대상의 범위는 개발자가 필요로 하는 곳 까지 구현하면 된다.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java clone</tag>
        <tag>Object.clone()</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] 트랜잭션 격리 수준(isolation level)]]></title>
    <url>%2Fdb%2F%ED%8A%B8%EB%9E%9C%EC%9E%AD%EC%85%98-%EA%B2%A9%EB%A6%AC-%EC%88%98%EC%A4%80-isolation-level%2F</url>
    <content type="text"><![CDATA[트랜잭션 격리수준(isolation level)이란 동시에 여러 트랜잭션이 처리될 때, 트랜잭션끼리 얼마나 서로 고립되어 있는지를 나타내는 것이다. 즉, 간단하게 말해 특정 트랜잭션이 다른 트랜잭션에 변경한 데이터를 볼 수 있도록 허용할지 말지를 결정하는 것이다. 격리수준은 크게 아래의 4개로 나뉜다. READ UNCOMMITTED READ COMMITTED REPEATABLE READ SERIALIZABLE 아래로 내려갈수록 트랜잭션간 고립 정도가 높아지며, 성능이 떨어지는 것이 일반적이다. 일반적인 온라인 서비스에서는 READ COMMITTED나 REPEATABLE READ 중 하나를 사용한다. (oracle = READ COMMITTED, mysql = REPEATABLE READ) ISOLATION LEVEL 조회, 변경, 테스트 조회 123SHOW VARIABLES like 'tx_isolation';-- 또는SELECT @@tx_isolation; 변경 나와있는데로 해봤는데 안바뀌는데 방법좀… READ UNCOMMITTED READ UNCOMMITTED 격리수준에서는 어떤 트랜잭션의 변경내용이 COMMIT이나 ROLLBACK과 상관없이 다른 트랜잭션에서 보여진다. 이 격리수준에서는 아래와 같은 문제가 발생할 수 있다. A 트랜잭션에서 10번 사원의 나이를 27살에서 28살로 바꿈 아직 커밋하지 않음 B 트랜잭션에서 10번 사원의 나이를 조회함 28살이 조회됨 이를 더티 리드(Dirty Read)라고 한다 A 트랜잭션에서 문제가 발생해 ROLLBACK함 B 트랜잭션은 10번 사원이 여전히 28살이라고 생각하고 로직을 수행함 이런식으로 데이터 정합성에 문제가 많으므로, RDBMS 표준에서는 격리수준으로 인정하지도 않는다. READ COMMITTED 어떤 트랜잭션의 변경 내용이 COMMIT 되어야만 다른 트랜잭션에서 조회할 수 있다. 오라클 DBMS에서 기본으로 사용하고 있고, 온라인 서비스에서 가장 많이 선택되는 격리수준이다. 여기서는 B 트랜잭션에서 10번 사원의 나이를 조회해도 27살이 조회된다.(커밋되지 않았기 때문에) (이는 언두 영역에 저장된 데이터이다. MVCC 참조) A 트랜잭션에서 최종 커밋하면 B 트랜잭션에서 28살 이라는 값을 받아볼 수 있다. 언뜻보면 정합성 문제가 해결된 것 처럼 보이지만, 여기서도 NON-REPETABLE READ 부정합 문제가 발생할 수 있다. B 트랜잭션에서 10번 사원의 나이를 조회 27살이 조회됨 A 트랜잭션에서 10번 사원의 나이를 27살에서 28살로 바꾸고 커밋 B 트랜잭션에서 10번 사원의 나이를 다시 조회(변경되지 않은 이름이 조회됨) 28살이 조회됨 이는 하나의 트랜잭션내에서 똑같은 SELECT를 수행했을 경우 항상 같은 결과를 반환해야 한다는 REPEATABLE READ 정합성에 어긋나는 것이다. 일반적인 웹 어플리케이션에서는 크게 문제되지 않지만, 작업이 금전적인 처리와 연결되어 있다면 문제가 발생할 수 있다. 예를 들어 여러 트랜잭션에서 입금/출금 처리가 계속 진행되는 트랜잭션들이 있고 오늘의 입금 총 합을 보여주는 트랜잭션이 있다고하면, 총합을 계산하는 SELECT 쿼리는 실행될 때 마다 다른 결과값을 가져올 것이다. 이런 문제가 발생할 수 있기 떄문에 격리수준에 의해 실행되는 SQL 문장이 어떤 결과를 출력할 지 정확히 예측하고 있어야 한다. REPETABLE READ REPETABLE READ 격리수준은 간단하게 말해서 트랜잭션이 시작되기 전에 커밋된 내용에 대해서만 조회할 수 있는 격리수준이다. MySQL DBMS에서 기본으로 사용하고 있고, 이 격리수준에서는 NON-REPETABLE READ 부정합이 발생하지 않는다. 10번 트랜잭션이 500000번 사원을 조회 12번 트랜잭션이 500000번 사원의 이름을 변경하고 커밋 10번 트랜잭션이 500000번 사원을 다시 조회 언두 영역에 백업된 데이터 반환 즉, 간단하게 말해서 자신의 트랜잭션 번호보다 낮은 트랜잭션 번호에서 변경된(+커밋된) 것만 보게 되는 것이다. (모든 InnoDB의 트랜잭션은 고유한 트랜잭션 번호(순차적으로 증가하는)를 가지고 있으며, 언두 영역에 백업된 모든 레코드는 변경을 발생시킨 트랜잭션의 번호가 포함되어 있다.) REPETABLE READ 격리수준에서는 트랜잭션이 시작된 시점의 데이터를 일관되게 보여주는 것을 보장해야 하기 때문에 한 트랜잭션의 실행시간이 길어질수록 해당 시간만큼 계속 멀티 버전을 관리해야 하는 단점(?)이 있다. 하지만 실제로 영향을 미칠 정도로 오래 지속되는 경우는 없어서… READ COMMITTED와 REPETABLE READ의 성능차이는 거의 없다고 한다. REPETABLE READ에서 발생할 수 있는 데이터 부정합 UPDATE 부정합 12345678910START TRANSACTION; -- transaction id : 1SELECT * FROM Member WHERE name='junyoung'; START TRANSACTION; -- transaction id : 2 SELECT * FROM Member WHERE name = 'junyoung'; UPDATE Member SET name = 'joont' WHERE name = 'junyoung'; COMMIT;UPDATE Member SET name = 'zion.t' WHERE name = 'junyoung'; -- 0 row(s) affectedCOMMIT; 이 상황에서 최종 결과는 name = joont가 된다. REPETABLE READ이기 때문에, 2번 트랜잭션에서 name = joont로 변경하고 COMMIT을 하면 name = junyoung의 내용을 언두로그에 남겨놔야 한다. 그래야 1번 트랜잭션에서 일관되게 데이터를 보는 것을 보장해줄 수 있기 때문이다. 이 상황에서 아래 구문에서 UPDATE 문을 실행하게 되는데, UPDATE의 경우 변경을 수행할 로우에 대해 잠금이 필요하다. 하지만 현재 1번 트랜잭션이 바라보고 있는 name = junyoung 의 경우 레코드 데이터가 아닌 언두영역의 데이터이고, 언두영역에 있는 데이터에 대해서는 쓰기 잠금을 걸 수가 없다. 그러므로 위의 UPDATE 구문은 레코드에 대해 쓰기 잠금을 시도하려고 하지만 name = junyoung인 레코드는 존재하지 않으므로, 0 row(s) affected가 출력되고, 아무 변경도 일어나지 않게 된다. 그러므로 최종적으로 결과는 name = joont가 된다. 자이언티가 되지 못해 아쉽다. 간단하게 말해 DML 구문은 멀티버전을 관리하지 않는다 Phantom READ 한 트랜잭션 내에서 같은 쿼리를 두 번 실행했는데, 첫 번째 쿼리에서 없던 유령(Phantom) 레코드가 두 번째 쿼리에서 나타나는 현상을 말한다. REPETABLE READ 이하에서만 발생하고(SERIALIZABLE은 발생하지 않음), INSERT에 대해서만 발생한다. 아래와 같은 상황에서 재현될 수 있다. 1234567891011START TRANSACTION; -- transaction id : 1 SELECT * FROM Member; -- 0건 조회 START TRANSACTION; -- transaction id : 2 INSERT INTO MEMBER VALUES(1,'joont',28); COMMIT;SELECT * FROM Member; -- 여전히 0건 조회 UPDATE Member SET name = 'zion.t' WHERE id = 1; -- 1 row(s) affectedSELECT * FROM Member; -- 1건 조회 COMMIT; REPETABLE READ에 에 의하면 원래 출력되지 않아야 하는데 UPDATE 문의 영향을 받은 후 부터 출력된다. 이 시점에 스냅샷을 적용시키는 것 같다. 참고로 DELETE에 대해서는 적용되지 않는다. 1234567891011START TRANSACTION; -- transaction id : 1 SELECT * FROM Member; -- 1건 조회 START TRANSACTION; -- transaction id : 2 DELETE FROM Member WHERE id = 1; COMMIT;SELECT * FROM Member; -- 여전히 1건 조회 UPDATE Member SET name = 'zion.t' WHERE id = 1; -- 0 row(s) affectedSELECT * FROM Member; -- 여전히 1건 조회 COMMIT; SERIALIZABLE 가장 단순하고 가장 엄격한 격리수준이다. InnoDB에서 기본적으로 순수한 SELECT 작업은 아무런 잠금을 걸지않고 동작하는데, 격리수준이 SERIALIZABLE일 경우 읽기 작업에도 공유 잠금을 설정하게 되고, 이러면 동시에 다른 트랜잭션에서 이 레코드를 변경하지 못하게 된다. 이러한 특성 때문에 동시처리 능력이 다른 격리수준보다 떨어지고, 성능저하가 발생하게 된다. 참고 : 이성욱, 『Real MySQL』, 위키북스(2012)]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>Real MySQL</tag>
        <tag>isolatation level</tag>
        <tag>격리 수준</tag>
        <tag>mysql isolation</tag>
        <tag>REPETABLE READ</tag>
        <tag>Phantom READ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] MVCC]]></title>
    <url>%2Fdb%2FMVCC%2F</url>
    <content type="text"><![CDATA[Multi Version Concurrency Content 의 약자이며, Multi Version이라 함은 하나의 레코드에 대해 여러 버전이 관리된다는 의미이다. 일반적으로 레코드 레벨의 트랜잭션을 지원하는 DBMS가 제공하는 기능이며, 가장 큰 목적은 잠금을 사용하지 않는 일관된 읽기를 제공하는데 있다. (멀티버전 없이 일관된 읽기를 보장하려면 읽고 있는 레코드는 외부에서 수정이 불가능하도록 해야한다) MySQL은 언두 로그를 이용해 이 기능을 구현한다. 가령 아래와 같은 업데이트 문을 실행하고, 1UPDATE member SET area = '경기' WHERE id = 12 아직 COMMIT이나 ROLLBACK을 하지 않은 상태에서 사용자가(다른 트랜잭션에서) 아래와 같이 조회하면 어떻게 될까? 123SELECT * FROM MemberWHERE id = 12 정답은 MySQL 초기화 파라미터에 설정된 격리 수준에 따라 다르다. 격리 수준이 READ_UNCOMMITED라면 버퍼 풀이나 데이터 파일로부터 데이터를 읽어서 반환하지만, 격리 수준이 READ_COMMITED 이상이라면 버퍼 풀이나 데이터 파일에 있는 데이터를 읽는 대신에 변경 이전의 내용을 보관하고 있는 언두 로그 영역의 데이터를 반환한다. (디스크 데이터 파일에 데이터가 업데이트 되어있을 수도 있고, 아닐수도 있기 때문에 ???라고 표시되어있는데, 기본적으로 InnoDB는 ACID를 보장하므로 버퍼풀과 데이터파일은 같은 값이라고 봐도 무방하다) 보다시피 기본적으로 UPDATE 구문은 연산의 결과를 바로 레코드에 반영하고, 이전 데이터에 대해서는 멀티버전으로 관리한다. 이를 MVCC라고 한다. (INSERT나 DELETE도 멀티버전으로 관리하는지는 잘 모르겠다) 이렇게 관리되는 멀티버전은 데이터베이스 격리수준에 따라 보여지는게 달라지게 된다. READ UNCOMMITTED면 애초에 멀티버전을 관리하지 않으므로 다른 트랜잭션에서 레코드의 내용을 바로 바라보게 되고, READ COMMITTED면 커밋 전까지 다른 트랜잭션에 멀티 버전을 보여줄 것이고, REPETABLE READ면 커밋을 해도 먼저 시작한 다른 트랜잭션에게는 멀티 버전을 보여줄 것이다 (참고로 위 처럼 UPDATE를 실행한 로우는 LOCK이 걸리기 때문에 외부에서 수정이 불가능하다) 위 상황에서 커밋을 실행하면 더 이상의 변경작업 없이 지금 상태를 영구적인 데이터로 만들어버린다 언두 로그 영역의 내용을 더 이상 필요로 하는 트랜잭션이 없을 경우, 언두 로그에서 해당 내용을 삭제한다. 격리수준에 따라 삭제하는 시점이 다를 것이다 롤백을 실행하면 언두 로그 영역에 있는 데이터를 복구한다 언두 로그에서 해당 내용을 삭제한다. 참고 : 이성욱, 『Real MySQL』, 위키북스(2012)]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>Real MySQL</tag>
        <tag>Multi Version Concurrency Control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[copyright, copyleft]]></title>
    <url>%2Fetc%2Fcopyright-copyleft%2F</url>
    <content type="text"><![CDATA[http://www.mediaus.co.kr/news/articleView.html?idxno=18500 이러한 저작권을 비롯한 지적재산권의 국제적 강화와 표준화는 산업국들의 이익을 대변한다는 비판과 함께 저작권에 반대하는 운동이 함께 진행되어 왔다. 바로 카피레프트(Copyleft)이다. 카피라이트(Copyright), 즉 저작권이 저작자가 저작물의 사용, 복제, 변경 및 배포를 금지하는 것과는 반대로, 카피레프트는 지적 창작물의 복제, 사용, 변경 및 배포 등을 자유롭게 할 수 있도록 허가하는 것이다. 즉, 저작권에 기반을 둔 ‘이용제한’이 아니라, 저작권을 기반으로 한 ‘정보공유’인 것이다. 다시 말하면, 저작물의 소유권을 주장해 사용을 제한하는 것과는 달리, 새로운 저작물은 이미 앞선 세대가 창작한 저작물들을 바탕으로 창조된 것이기에 개인자산일 뿐 아니라 인류의 공공자산임으로 모두에게 공유되어야 한다는 논리이다. 또한 소수 특권층에 의한 지식과 정보의 독점과 상업화에 대한 거부이기도 하다.]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>copyright</tag>
        <tag>copyleft</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[google dark theme 적용하기]]></title>
    <url>%2Fetc%2Fgoogle-dark-theme-%EC%A0%81%EC%9A%A9%ED%95%98%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[Dark Theme for Google 확장 프로그램을 설치한다. https://chrome.google.com/webstore/detail/dark-theme-for-google/apiabgjfojnkcepfmbdechlhfocpeenc]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>google dark theme</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] github dark theme 적용하기]]></title>
    <url>%2Fgit%2Fgithub-dark-theme-%EC%A0%81%EC%9A%A9%ED%95%98%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[https://github.com/StylishThemes/GitHub-Dark 위 레파지토리를 이용해 접근 가능하다. 확장 프로그램에서 Stylus 를 설치한다. https://chrome.google.com/webstore/detail/stylus/clngdbkpkpeebahjckkjfobafhncgmne Installation의 첫번쨰 Install the usercss을 누른다. 왼쪽 상단에 install style을 누르면 바로 적용된다!! 이제 눈이 안아프다. 이제 눈이 안아프다.]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>github dark</tag>
        <tag>github dark theme</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] github을 이용하는 전체 흐름]]></title>
    <url>%2Fgit%2Fgithub%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%98%EB%8A%94-%EC%A0%84%EC%B2%B4-%ED%9D%90%EB%A6%84%2F</url>
    <content type="text"><![CDATA[1편 : https://blog.outsider.ne.kr/865 2편 : https://blog.outsider.ne.kr/866 repository fork local clone 개발하면서 local에 commit 브랜치는 맞춰주는 것이 좋고, 원본 저장소를 따로 라모트로 추가해서 개발 중 변경사항을 주기적으로 pull 받아 맞춰주는 것이 좋다 개발이 완료되면 본인 remote repository push 원본 repository로 pull request 생성 오픈소스 프로젝트마다 Pull Request를 받아주는 약속이 다르므로 보내기 전에 이를 먼저 확인해야 한다. 소스 수정이 잘 되었더라도 이 약속을 제대로 지키지 않으면 받아주지 않는다. 탭에서 Commits나 Files Changed를 클릭하면 Pull Request를 보내는 커밋과 변경사항이 제대로 되었는지 확인할 수 있다. Pull Request를 이용한 개발 흐름 : https://blog.outsider.ne.kr/1199 작업 시작과 동시에 PR을 올려서 내 작업 진행과정을 투명하게 공유한다 다른 작업자들이 내 작업 진행과정을 알 수 있어서 나중에 merge 작업이나 새로운 작업 시작시에 편리함. 코드 리뷰를 할 수 있다는 장점도. PR Comment에 할 일 목록을 만들고 체크리스트로 표시하도록 함. 작업중일떄는 WIP, 작업이 끝났을 경우 DONE]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>pull request</tag>
        <tag>코드리뷰</tag>
        <tag>오픈소스</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] stacktrace 읽는법]]></title>
    <url>%2Fjava%2Fstacktrace-%EC%9D%BD%EB%8A%94%EB%B2%95%2F</url>
    <content type="text"><![CDATA[https://okky.kr/article/338405 첫번째 줄이 가장 마지막에 실행된 메서드 밑에 줄은 위의 메서드를 호출한 메서드 CausedBy가 실제 발생한 에러 메세지임. 1234567Exception in thread "main" java.lang.IllegalStateException: A book has a null property at com.example.myproject.Author.getBookIds(Author.java:38) at com.example.myproject.Bootstrap.main(Bootstrap.java:14)Caused by: java.lang.NullPointerException at com.example.myproject.Book.getId(Book.java:22) at com.example.myproject.Author.getBookIds(Author.java:35) ... 1 more 이렇게 발생되었다면 보통 아래와 같이 구현된 것임 12345try &#123;....&#125; catch (NullPointerException e) &#123; throw new IllegalStateException("A book has a null property", e)&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>stacktrace</tag>
        <tag>causedby</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] JPQL]]></title>
    <url>%2Fjpa%2FJPQL%2F</url>
    <content type="text"><![CDATA[JPA에서 현재까지 사용했던 검색은 아래와 같다. 식별자로 조회 EntityManager.find() 객체 그래프 탐색 e.g. a.getB().getC() 하지만 현실적으로 이 기능만으로 어플리케이션을 개발하기에는 무리이다. 그렇다고 모든 엔티티를 메모리에 올려두고 어플리케이션 내에서 필터하는 것은 현실성이 없는 소리이다. 즉, 데이터베이스에서 필터해서 조회해올 무언가가 필요하고, 그게 객체지향 쿼리 언어(JPQL)이다. JPQL은 엔티티 객체를 조회하는 객체지향 쿼리 언어이다. 문법은 SQL과 비슷한데, 실제론 SQL을 추상화 한것이기 때문에 특정 데이터베이스에 의존하지 않는 특징이 있다. SQL과 비슷하게 SELECT, UPDATE, DELETE 문을 사용할 수 있다. (참고로 엔티티 저장은 그냥 entityManager.persist를 사용하면 되므로 INSERT 문은 없다.) JPQL에서 UPDATE, DELETE 문은 벌크 연산이라고 해서 뒤에서 따로 설명할 것이므로, SELECT 문만 작성하겠다. 기본 문법 기본 형태는 아래와 같다. 1SELECT m FROM Member AS m WHERE m.username = 'Hello' 대소문자 구문 엔티티와 속성은 대소문자를 구분한다. Member와 member는 다르고 username과 USERNAME은 다르다. SELECT, FROM 같은 JPQL 키워드는 대소문자를 구분하지 않는다. 엔티티 이름 FROM 이후에 오는 대상은 테이블 이름이 아니라 엔티티 이름이다. 기본값인 클래스명을 엔티티명으로 사용하는 것을 추천한다. 별칭은 필수 JPQL은 별칭을 필수로 사용해야 한다. AS 뒤에 m이 Member의 별칭이다. AS는 생략 가능하다. TypedQuery, Query 작성한 JPQL을 실행시키기 위해 만드는 쿼리 객체이다. JPQL이 반환할 타입을 명확하게 지정할 수 있으면 TypedQuery를 사용하고, 명확하게 지정할 수 없으면 Query를 사용하면 된다. 12345// 조회대상이 정확히 Member 엔티티이므로 TypedQuery 사용 가능TypedQuery&lt;Member&gt; query = em.createQuery("SELECT m FROM Member m", Member.class);// 조회대상이 String, Integer로 명확하지 않으므로 Query 사용Query query = em.createQuery("SELECT m.username, m.age FROM Member m"); TypedQuery로 실행된 쿼리는 두번쨰 인자로 주어진 클래스를 반환하고, Query의 경우 예제처럼 조회 컬럼이 1개 이상일 경우 Object[], 1개일 경우 Object를 반환한다. 참고로 String 타입의 필드만 조회하고 TypedQuery&lt;String[]&gt; 를 사용하는 방식은 안된다. 하나하나 다 체크하기에는 너무 많으니까… 결과 조회 쿼리 객체에서 아래의 메서드들을 사용해 JPQL을 실행한다. query.getResultList() 결과를 컬렉션으로 반환한다. 결과가 없으면 빈 컬렉션이 반환된다. 1건이면 1건만 들어간 컬렉션이 반환된다. query.getSingleResult() 결과가 정확히 1건 일때 사용한다. 결과가 없으면 javax.persistence.NoResultException, 결과가 1건 이상이면 javax.persistence.NonUniqueResultException이 발생한다. 근데 얘는 Optional을 반환해야 하지 않을까? 파라미터 바인딩 아래와 같은 이름 기준 파라미터 바인딩을 지원한다. 12345TypedQuery&lt;Member&gt; query = em.createQuery("SELECT m FROM Member m WHERE m.username = :username", Member.class) .setParameter("username", "joont1"); // JPQL은 대부분 메서드 체인 방식으로 되어있어서 이렇게 연속해서 작성하는 것이 가능하다List&lt;Member&gt; result = query.getResultLst(); username은 Member 클래스에 정의된 프로퍼티 이름이다. 앞에 :를 붙여서 바인딩한다. username 에 joont1 이 바인딩 될 것이다. 참고로 아래와 같이 위치 기준 파라미터 바인딩도 지원하기는 한다. 123TypedQuery&lt;Member&gt; query = em.createQuery("SELECT m FROM Member m WHERE m.username = ?1", Member.class) .setParameter(1, "joont1"); 이것보다는 전자가 더 명확하다. 참고로 LIKE 연산처럼 % 같은 특수문자가 필요할 경우 전달하는 파라미터에 붙여서 사용하면 된다. 123TypedQuery&lt;Member&gt; query = em.createQuery("SELECT m FROM Member m WHERE m.username LIKE :username", Member.class) .setParameter("username", "%joont%"); // 이런식으로 파라미터 바인딩 방식은 선택이 아닌 필수이다 JPQL에 직접 문자를 더하면 SQL Injection을 당할 수 있다 JPA에서 파라미터만 다를 뿐 같은 쿼리로 인식하므로, JPQL을 SQL로 파싱한 결과를 재사용할 수 있다 SQL 내에서도 같은 쿼리는 결과를 재사용한다 프로젝션 조회할 대상을 지정하는 것을 프로젝션이라고 한다. SELECT [프로젝션 대상] FROM 으로 대상을 지정한다. 대상은 엔티티 타입, 임베디드 타입, 스칼라 타입이 있다. 엔티티 프로젝션 123SELECT m FROM Member m // memberSELECT m.team FROM Memher m // team 둘 다 엔티티를 프로젝션 대상으로 사용했다. 참고로 이렇게 조회한 엔티티는 영속성 컨텍스트에서 관리된다. 임베디드 타입 프로젝션 엔티티를 통해서 조회한다. 123Address address = em.createQuery("SELECT m.address FROM Member m", Address.class) .getSingleResult(); 임베디드 타입은 엔티티 타입이 아닌 값 타입이므로 이렇게 조회한 임베디드 타입은 영속성 컨텍스트에서 관리되지 않는다. 스칼라 타입 프로젝션 1234567891011// 이름조회TypedQuery&lt;String&gt; query = em.createQuery("SELECT m.username FROM Member m", String.class);List&lt;String&gt; resultList = query.getResultList();// 이름조회(중복제거)TypedQuery&lt;String&gt; query = em.createQuery("SELECT DISTINCT m.username FROM Member m", String.class);List&lt;String&gt; resultList = query.getResultList();// 통계 쿼리TypedQuery&lt;Double&gt; query = em.createQuery("SELECT AVG(o.orderAmount) FROM Order o", Double.class);List&lt;Double&gt; resultList = query.getResultList(); 조회되는 컬럼이 1건이라 TypedQuery를 사용하였다. 보다시피 통계 쿼리도 스칼라 타입으로 조회할 수 있다. 여러 값 조회 아래와 같이 여러값으로 조회했을 때는 TypedQuery를 사용할 수 없고, Query만 사용할 수 있다. 12345678Query query = em.createQuery("SELECT m.username, m.age, m.team FROM Member m");List&lt;Object[]&gt; resultList = query.getResultList();for(Object[] row : resultList)&#123; String username = (String)row[0]; Integer age = (Integer)row[1]; Team team = (Team)row[2];&#125; 물론 아때도 조회한 엔티티는 영속성 컨텍스트에서 관리된다. NEW 명령어 NEW 명령어를 사용하면 Object[] 대신 바로 객체로 생성해서 받아볼 수 있다. 1234TypedQuery&lt;UserDTO&gt; query = em.createQuery("SELECT NEW com.joont.dto.UserDTO(m.username, m.age, m.team) FROM Member m", UserDTO.class);List&lt;UserDTO&gt; resultList = query.getResultList(); 기존이라면 하나하나 번거롭게 변환했어야 했을 작업을 NEW 명령어를 사용해서 간단하게 처리했다. NEW 명령어를 사용하려면 아래 2가지를 주의해야 한다. 패키지명을 포함한 클래스명을 입력해야 한다. 순서와 타입이 일치하는 생성자가 필요하다. 직접 쓰라고 있는 기능은 아닌 것 같다. 라이브러리들이 적절히 구현해라고 만들어놓은 기능인듯(QueryDSL 등) 페이징 API JPA는 데이터베이스들의 페이징들을 아래의 두 API로 추상화했다. (페이징은 데이터베이스마다 문법이 다 다르다) setFirstResult(int startPosition) : 조회 시작 위치(0부터 시작) setMaxResult(int maxResult) : 조회할 데이터 수 123456TypedQuery&lt;Member&gt; query = em.createQuery("SELECT m FROM Member m ORDER BY m.username DESC", Member.class);query.setFirstResult(10);query.setMaxResult(20);query.getResultList(); 11번쨰 데이터부터 시작해서 20개를 조회한다. 즉 11~30번 데이터를 조회하게 된다. 지원하는 모든 데이터베이스를 추상화했기 때문에 데이터베이스가 바껴도 방언만 바꿔주면 된다. 집합과 정렬 집합 함수 함수 설명 리턴타입 COUNT 결과 수를 구한다 Long MAX, MIN 최대, 최소값을 구한다 대상에 따라 다름 AVG 평균값을 구한다. 숫자타입만 사용할 수 있다. 숫자가 아니면 0을 리턴한다. Double SUM 합을 구한다. 숫자타입만 사용할 수 있다. 정수합 : Long소수합 : Double 집합 함수 사용 시 참고사항 통계를 계산할 때 NULL값은 무시된다(COUNT(*)은 제외). 값이 없을 때 SUM, AVG, MAX, MIN를 사용하면 NULL을 리턴한다. COUNT는 0을 리턴한다. DISTINCT를 집합 함수안에 사용하면 중복된 값을 제거하고 집합을 구한다. 그룹핑 GROUP BY, HAVING도 사용할 수 있다. 1234SELECT t.name, COUNT(m.age), SUM(m.age), AVG(m.age), MAX(m.age), MIN(m.age)FROM Member m LEFT JOIN m.team tGROUP BY t.name HAVING AVG(m.age) &gt;= 10 (team의 이름으로 그룹화한 뒤 나이의 평균이 10살 이상인 그룹에 대해서 집합을 구했다.) 문법은 아래와 같다. group by절 : GROUP BY {단일값 경로 | 별칭} having절 : HAVING 조건식 이런식의 통계 쿼리는 보통 전체 데이터를 기준으로 사용하므로 실시간으로 사용하기에는 부담이 많다. 정렬 ORDER BY도 사용할 수 있다. 1234SELECT t.name, COUNT(m.age) AS cnt FROM Member m LEFT JOIN m.team t GROUP BY t.name ORDER BY t.name ASC, cnt DESC 문법은 아래와 같다. order by절 : ORDER BY {상태필드 경로 | 결과변수 [ASC | DESC]} 상태필드는 m.name 같이 객체의 상태를 나타내는 필드를 말하고, 결과변수는 SELECT 절에 나타나는 값을 말한다. 위의 예제에서는 cnt가 결과변수이다. 조인 내부 조인 1SELECT m FROM Member m INNER JOIN m.team t 보다시피 일반적인 SQL 조인과 조금 다르다. 가장 큰 특징은 연관 필드를 사용해서 조인한다는 점이다. 즉, 조인을 사용하려면 엔티티에 연관관계 명시는 필수적으로 되어있어야 한다. (위와 같이 작성하면 Member의 team 필드에서 관계의 정보를 얻은 뒤 조인할 것이다) 아래는 잘못 작성된 JPQL 조인이다. 1SELECT m FROM Member m INNER JOIN Team t // 잘못된 조인 만약 조인한 두 개의 엔티티를 조회하려면 다음과 같이 작성하면 된다. 12SELECT m,tFROM Member m INNER JOIN m.team t 서로 다른 타입의 두 엔티티를 조회했으므로 TypedQuery는 사용할 수 없다. 123456List&lt;Object[]&gt; list = em.createQuery(jpql).getResultList(); // 위에서 작성한 쿼리for(Object[] o : list)&#123; Member m = (Member)o[0]; Team t = (Team)o[1];&#125; 외부 조인 키워드만 바꿔주면 된다. 1SELECT m FROM Member m LEFT JOIN m.team t 컬렉션 조인 일대다 관계나 다대다 관계처럼 컬렉션을 사용하는 곳에 조인하는 것을 말한다. 아래와 같이 컬렉션 값 연관 필드를 사용하면 된다. 1SELECT t, m FROM Team t LEFT JOIN t.members m 세타 조인 CROSS JOIN을 말한다. CROSS JOIN이란 일반적으로 INNER JOIN에 ON절을 주지 않을 것을 말한다. 그러므로 JPQL에서는 CROSS JOIN으로 외부 조인을 사용할 수 없다. (조인시 ON절이 자동 생성되므로) ON절 JPA 2.1부터 조인할 때 ON 절을 지원한다. 123SELECT m, t FROM Member m LEFT JOIN m.team tON t.name = 'A' 실행 결과는 아래와 같다. 123SELECT m.*, t.*FROM Member mLEFT JOIN Team t ON m.team_id = t.id AND t.name = 'A' ON 절을 사용하면 조인 대상을 필터링 하고 사용할 수 있다. 패치 조인 패치조인은 SQL에 있는 개념은 아니고 JPQL에서 성능 최적화를 위해 제공하는 기능이다. 간단하게 말해서 연관된 엔티티나 컬렉션을 한번에 같이 조회한 뒤에 대상 객체의 필드에 set 해서 내려주는 것이다. 문법은 아래와 같다. fetch join : [ LEFT [ OUTER ] | INNER ] JOIN FETCH 조인경로 엔티티 패치 조인 12SELECT m FROM Member m INNER JOIN FETCH m.team 실행해보면 연관 엔티티의 값을 채워주기 위해 M.*, T.*의 형태로 연관된 팀까지 함께 조회한다. 그리고 기존의 INNER JOIN 에서 Object[] 로 받아야 했던것 과는 달리, Member의 team 변수에 값이 다 채워진 상태로 리턴된다. 즉, 객체 그래프를 그대로 유지하면서 받을 수 있는 방법이다. 그러므로 성능 최적화를 위해 제공화는 기능이라고 하는 것이다. 123456List&lt;Member&gt; list = em.createQuery(jpql /* 위에서 작성한 쿼리 */, Member.class).getResultList();for(Member m : list)&#123; System.out.println(m.getTeam().getName()); // LAZY 로딩 발생 안함&#125; (Member의 Team은 fetch가 LAZY라고 가정한다) 패치 조인을 통해 이미 연관된 팀을 같이 조히했으므로 위와 같이 수행해도 LAZY 로딩이 발생하지 않는다. 컬렉션 패치 조인 일대다 관계에서도 패치 조인을 사용할 수 있다. 12SELECT tFROM Team t INNER JOIN FETCH t.members 이것 또한 SELECT 절에 t 만 명시했음에도 불구하고, T.*, M.*의 형태로 연관된 회원까지 함께 조회된다. 근데 여기서 주의할 점이 있는데, 쿼리의 결과가 증가해서 그런지 위 jpql의 결과를 리스트로 받아보면 Team의 개수가 Member의 개수와 동일함을 볼 수 있다. 123456789List&lt;Team&gt; list = em.createQuery(jpql, Team.class).getResultList(); // 위에서 작성한 쿼리for(Team t : list)&#123; System.out.println(t); for(Member m : t.getMembers())&#123; System.out.println("-&gt; " + m); &#125;&#125; Team@0x100 -&gt; Member@0x200 -&gt; Member@0x300 Team@0x100 -&gt; Member@0x200 -&gt; Member@0x300 이렇듯 일대다 조인은 결과가 증가할 수 있음에 주의해야 한다. DISTINCT JPQL의 DISTINCT는 SQL에 DISTINCT를 추가하는 것은 물론이고, 어플리케이션에서 한번 더 중복을 제거한다. 이 특징을 이용해서 위의 컬렉션 패치 조인에서 리스트가 중복되서 나오는 문제를 해결할 수 있다. 12SELECT DISTINCT tFROM Team t INNER JOIN FETCH t.members 이렇게 작성하면 먼저 SQL에 DISTINCT가 적용된다. 하지면 지금은 로우의 데이터가 다르므로 DISTINCT는 효과가 없다. 다음으로 어플리케이션에서 DISTINCT 명령을 보고 중복된 데이터를 걸러낸다. SELECT DISTINCT t는 Team 엔티티의 중복을 제거하라는 의미이므로, 여기에서 중복이 제거되고, 예상했던 결과를 받아볼 수 있게된다. 패치조인과 일반조인의 차이 위에서도 언급했지만, 일반조인의 경우 결과를 반환할 때 연관관계까지 고려하지 않는다. 단지 SELECT 절에 지정한 엔티티만을 조회하고, 연관된 엔티티에 대해서는 프록시나 컬렉션 래퍼를 반환한다. 12345List&lt;Team&gt; list = em.createQuery(jpql, Team.class).getResultList(); // 일반 조인 쿼리for(Team t : list)&#123; System.out.println(t.getMembers.get(0)); // ?&#125; 그러므로 위와 같이 조회하면 fetchType이 LAZY일 경우 ? 부분에서 LAZY 로딩이 발생할 것이고, fetchType이 EAGER일 경우 회원 컬렉션을 즉시 로딩하기 위해 쿼리를 한번 더 실핼하게 된다. 패치조인의 특징과 한계 패치조인은 글로벌 전략보다 우선한다. (글로벌 전략 : 엔티티에 직접 적용하는 로딩 전략 e.g. fetch=FetchType.LAZY) 그러므로 최적화를 위해 글로벌 로딩 전략을 즉시 로딩으로 설정하기 보다는, 글로벌 전략은 지연 로딩으로 설정하고 최적화가 필요한 곳에서 패치조인을 사용하는 것이 전체적으로 봤을때 훨씬 효과적이다. 물론 이런 좋은 패치조인에도 한계가 있다. 패치조인 대상에는 별칭을 줄 수 없다 별칭을 줄 수 없다는 말인 즉 SELECT, WHERE, 서브쿼리에 패치조인 대상을 사용할 수 없음을 말한다. 하이버네이트를 포함한 몇몇 구현체들은 패치조인에 별칭을 지정하는 것을 허용해주나 잘못 사용하면 무결성이 깨질 수 있으므로 조심해서 사용해야 한다. 둘 이상의 컬렉션을 패치할 수 없다 컬렉션 * 컬렉션의 카테시안 곱이 만들어지므로 주의해야한다. 하이버네이트를 사용할 경우 MultipleBagFetchException이 발생한다. (근데 컬렉션이 Set이면 왜 될까?) 컬렉션을 패치 조인하면 페이징 API를 사용할 수 없다 HHH000104: firstResult/maxResults specified with collection fetch; applying in memory! 와 같은 경고로그를 남기면서 메모리에서 페이징한다. 데이터가 적으면 상관없겠지만 많으면 성능 이슈가 발생할 수 있어서 위험하다. driven 엔티티의 중복이 제거된 다음 페이징을 하려고 메모리에서 하는건데… 왜 그런걸까? DISTINCT 없는 패치조인일 때는 그냥 다 뿌려줬으면서… 이것도 그냥 다 뿌려진 데이터에 페이징을 넣으면 되는 것 아닌지? 이렇듯 패치조인으로 모든것을 해결할수는 없다. 필요할 때만 사용하여 성능 최적화를 꾀하는 것이 좋다. 경로 표현식 경로 표현식이란 .(점)을 찍어 그래프를 탐색하는 것을 말한다. 12345SELECT m.usernameFROM Member m INNER JOIN m.team t INNER JOIN m.orders oWHERE t.name = 'TeamA'; 여기서 m.username, m.team, m.orders, t.name 모두 경로 표현식이다. 아래는 경로 표현식의 종류와 특징들이다. 상태필드 : 단순히 값을 저장하기 위한 필드. 일반적인 자바 기본 타입의 컬럼들을 말한다. m.username, t.name이 해당한다. 더는 탐색할 수 없다 연관필드 : 연관관계를 위한 필드, 임베디드 타입 단일 값 연관 필드 : 대상이 엔티티인것을 말한다. (@ManyToOne, @OneToOne) m.team이 해당된다. 묵시적으로 내부 조인이 일어난다. 계속 탐색할 수 있다 임베디드 타입도 단일 값 연관 필드이지만 연관관계가 없으므로 조인이 일어나지 않는다. 컬렉션 값 연관 필드 : 대상이 컬렉션것을 말한다. (@OneTomany, @ManyToMany) m.order가 해당된다. 묵시적으로 내부 조인이 일어난다. 기본적으로 더는 탐색할 수 없으나, FROM 절에서 별칭을 얻으면 별칭으로 탐색할 수 있다. 단일 값 연관 경로 탐색 예제 1SELECT o.member from Order o 위 JPQL은 아래와 같이 변환된다. 123SELECT m.*FROM Order_ o INNER JOIN Member m ON o.member_id = m.id 위처럼 JPQL에 JOIN을 적어주지 않았는데 JOIN이 발생하는 것을 묵시적 조인이라고 하고, JOIN을 직접 적어주는 것을 명시적 조인이라고 한다. 묵시적 조인은 내부 조인만 가능 하다. 외부 조인을 하고 싶으면 명시적 조인을 사용해야 한다. 컬렉션 값 연관 경로 탐색 컬렉션 값에서는 경로 탐색이 불가능하다(가장 많이 하는 실수) 1SELECT t.members.username FROM Team t // 실패 만약 경로 탐색을 하고 싶으면 명시적 조인을 사용해서 외부 별칭을 획득해야 한다. 123SELECT m.usernameFROM Team t INNER JOIN t.members m 참고로 컬렉션을 컬렉션의 크기를 구할 수 있는 size라는 특별한 기능을 제공한다. 1SELECT t.member.size FROM Team t 는 COUNT 함수를 사용하는 함수로 적절히 변환된다. 기본적으로 쿼리에서 조인이 성능상 차지하는 부분은 아주 크다. 단순하면 별로 문제될 것 없으나, 복잡하고 성능이 중요하면 분석이 용이하도록 명시적 조인을 사용하는 것이 좋다. 서브쿼리 JPQL에서는 서브쿼리를 WHERE, HAVING 절에서만 사용할 수 있다. SELECT, FROM 절에서는 사용할 수 없다. 아래는 간단한 서브쿼리 예시이다. 1234// 회원들의 평균 나이를 넘는 회원 조회SELECT mFROM Member mWHERE m.age &gt; (SELECT AVG(m2.age) FROM Member m2) 서브쿼리 함수 EXISTS 문법 : [NOT] EXISTS {subquery} 설명 : 서브쿼리가 결과에 존재하면 참이다(NOT은 반대) 12345678// teamA에 소속인 회원SELECT m FROM Memner m WHERE EXISTS ( SELECT t FROM m.team t WHERE t.name = 'teamA') ALL | ANY | SOME 문법 : { ALL | ANY | SOME } {subquery} 설명 : 비교 연산자와 같이 사용한다 ALL : 조건을 모두 만족하면 참 ANY, SOME : 둘은 같은 의미임. 조건을 하나라도 만족하면 참 1234567891011121314// 전체 상품 각각의 재고보다 주문량이 많은 주문들 SELECT oFROM Order oWHERE o.orderAmoun &gt; ALL( SELECT p.stockAmoun from Product p // o.p가 아니고?)// 어떤 팀이든 팀에 소속된 회원 SELECT mFROM Member mWHERE m.team = ANY( SELECT t FROM Team t // 이것도 좀 이상한데...) IN 문법 : [NOT] IN {subquery} 설명 : 서브쿼리의 결과 중 하나라도 같은 것이 있으면 참이다. IN은 서브쿼리가 아닌 곳에서도 사용할 수 있다. 조건식 타입 표현 종류 설명 예제 문자 작은 따옴표 사이에 표현.작음 따옴표를 표현하고 싶으면 작은 따옴표 2개(’’) 사용 ‘HELLO’‘She’'s ’ 숫자 L(Long 타입 지정)D(Double 타입 지정)F(Float 타입 지정) 10L10D10F 날짜 DATE {d ‘yyyy-mm-dd’}TIME {t ‘hh:mm:ss’}TIMESTAMP {ts 'yyyy-mm-dd hh:mm:ss.f} m.createDate = {d ‘2012-03-24’} Boolean TRUE, FALSE Enum 패키지명을 포함한 전체 이름 com.joont.MemberType.Admin 엔티티 타입 엔티티의 타입을 표현함. 주로 상속과 관련해 사용. TYPE(m) = Member 연산자 우선 순위 경로 탐색 연산 : . 수학 연산 : +(단항 연산), -(단항 연산), *, /, +, - 비교 연산 : =, &gt;, &gt;=, &lt;, &lt;=, &lt;&gt;, [NOT] BETWEEN, [NOT] LIKE, [NOT] IN, IS [NOT] NULL, IS [NOT] EMPTY, [NOT] MEMBER [OF], [NOT] EXISTS 논리연산 : NOT, AND, OR IS [NOT] EMPTY, [NOT] MEMBER [OF] 만 뺴면 사용법은 일반적인 SQL과 동일하다. 이 두개는 컬렉션 식으로써 JPA에서 제공하는, 컬렉션에만 사용가능환 특별 기능이다. 컬렉션 식 컬렉션에만 사용될 수 있음에 주의해야 한다. 컬렉션이 아닌 곳에 사용하면 오류가 발생한다. 빈 컬렉션 비교 식 문법 : {컬렉션 값 연관 경로} IS [NOT] EMPTY 설명 : 컬렉션에 값이 비었으면 참 123SELECT mFROM Member mWHERE m.orders IS EMPTY 는 아래와 같이 실행된다. 1234567SELECT m.*FROM Member mWHERE EXISTS ( SELECT o.id FROM Order_ o WHERE o.member_id = m.id) 컬렉션 멤버 식 문법 : {엔티티나 값} [NOT] MEMBER [OF] {컬렉션 값 연관경로} 설명 : 엔티티나 값이 컬렉션에 포함되어 있으면 참 1234// 전달된 멤버가 포함되어 있는 팀 조회 SELECT tFROM Team tWHERE :memberParam MEMBER OF t.members 스칼라 식 숫자, 문자, 날짜, case, 엔티티 타입 같은 가장 기본적인 타입들을 스칼라 타입이라고 한다. 문자함수 함수 설명 예제 CONCAT(문자1, 문자2) 문자를 합한다 CONCAT(‘A’, ‘B’) = AB SUBSTRING(문자, 위치[, 길이]) 위치부터 시작해 길이만큼 문자를 구한다. 길이 값이 없으면 나머지 전체 길이를 뜻한다 SUBSTRING(‘ABCDEF’, 2, 3) = BCD TRIM([[LEADING TRAILING BOTH] [트림 문자] FROM] 문자) LOWER(문자) 소문자로 변경 LOWER(‘ABC’) = abc UPPER(문자) 대문자로 변경 UPPER(‘abc’) = ABC LENGTH(문자) 문자 길이 LENGTH(‘ABC’) = 3 LOCATE(찾을 문자, 원본 문자[, 검색 시작 위치]) 검색위치부터 문자를 검색한다. 1부터 시작하고 못찾으면 0을 반환한다. LOCATE(‘DE’, ‘ABCDEFG’) = 4 수학함수 함수 설명 예제 ABS(식수학식) 절대값을 구한다 ABS(-10) = 10 SQRT(수학식) 제곱근을 구한다 SQRT(4) = 2.0 MOD(수학식, 나눌 수) 나머지를 구한다 MOD(4, 3) = 1 SIZE(컬렉션 값 연관 경로식) 컬렉션의 크기를 구한다 SIZE(t.members) INDEX(별칭) LIST 타입 컬렉션의 위치값을 구함. 단 컬렉션이 @OrderColumn을 사용하는 LIST 타입일 때만 사용할 수 있다 t.members m where INDEX(m) &gt; 3 날짜함수 함수 설명 CURRENT_DATE 현재 날짜 CURRENT_TIME 현재 시간 CURRENT_TIMESTAMP 현재 날짜 + 시간 하이버네이트는 날짜 타입에서 년,월,일,시,분,초 값을 구하는 기능을 지원한다 (YEAR,MONTH,DAY,HOUR,MINUTE,SECOND) 1SELECT YEAR(m.createdDate), MONTH(m.createdDate), DAY(m.createdDate) FROM Member; CASE 식 기본 CASE 문법 : 1234CASE &#123;WHEN &lt;조건식&gt; THEN &lt;스칼라식&gt;&#125;+ ELSE &lt;스칼라식&gt; END 심플 CASE 문법 : 1234CASE &lt;조건대상&gt; &#123;WHEN &lt;스칼라식1&gt; THEN &lt;스칼라식2&gt;&#125;+ ELSE &lt;스칼라식&gt;END COALESCE 문법 : COALESCE(&lt;스칼라식&gt;, {,&lt;스칼라식&gt;}+) 설명 : 스칼라식을 차례대로 조회해서 null이 아니면 반환한다. IFNULL과 약간 비슷하다. 12SELECT COALESCE(m.usernae, 'nobody') FROM Member m NULLIF 문법 : NULLIF(&lt;스칼라식&gt;, &lt;스칼라식&gt;) 설명 : 두 값이 같으면 null 반환, 다르면 첫번째 값을 반환한다. 다형성 쿼리 상속관계(@Inheritance)로 구성된 엔티티를 JPA에서 조회하면 그 자식 엔티티도 같이 조회한다. 이건 기존과 동일하다. TYPE 상속 구조에서 조회 대상을 특정 타입으로 한정할 때 사용한다. 123SELECT iFROM Item iWHERE TYPE(i) IN(Book, Movie) 는 아래와 같이 실행된다 123SELECT i.*FROM Item iWHERE i.DTYPE IN('B', 'M') TREAT 상속 구조에서 부모 타입을 특정 타입으로 다룰 때 사용한다.(자바의 타입 캐스팅과 비슷하다) JPA 표준은 FROM, WHERE절에서만 사용 가능하고, 하이버네이트의 경우 SELECT에서도 가능하다. 123SELECT iFROM Item iWHERE TREAT(i as Book).author = 'kim' Item을 자식 타입인 Book으로 다뤘다. 그래서 Book의 필드인 author에 접근할 수 있다. 사용자 정의 함수 호출(since JPA2.1) JPA 2.1부터 사용자 정의 함수를 지원한다. 문법 : FUNCTION(function_name {, function_arg}*) 12SELECT FUNCTION('group_concat', i.name)FROM Item i 하이버네이트를 사용할 경우 아래와 같이 방언 클래스를 상속해서 사용할 데이터베이스 함수를 미리 등록해야 한다. 12345678public class MyH2Dialect extends H2Dialect&#123; public MyH2Dialect()&#123; registerFunction( "group_concat", new StandardFunction("group_concat", StandardBasicTypes.STRING) ); &#125;&#125; registerFunction의 두번째 인자로는 하이버네이트의 SQLFunction 구현체를 주면 된다. 지금은 기본 함수를 사용하겠다는 의미로 StandardFunction을 사용하였고, 첫번째 인자로 함수 이름, 두번째 인자로 리턴 타입을 주고 있는 모습이다. 상속한 Dialect는 아래와 같이 등록하면 되고, 1&lt;property name="hibernate.dialect" value="com.joont.dialect.MyH2Dialect" /&gt; 하이버네이트를 사용하면 기본 문법보다 축약해서 사용할 수 있다. 12SELECT group_concat(i.name)FROM Item i 엔티티 직접 사용 객체 인스턴스는 참조 값으로 식별하고 테이블 로우는 기본 키 값으로 식별하기 때문에 JPQL에서 엔티티 객체를 직접 사용하면 SQL에서는 해당 엔티티의 기본 키 값을 사용한다. 몇 가지 예시를 보자. 12SELECT COUNT(m)FROM Member m 은 아래와 같이 변환된다. 12SELECT COUNT(m.id)FROM Member m 기본키 비교에 엔티티 사용 1234List&lt;Member&gt; result = em.createQuery("SELECT m FROM Member m WHERE m = :member") .setParameter("member", member) // 엔티티 객체 직접 사용 .getResultList(); member가 영속성 컨텍스트에 있을 필요는 없다. 그냥 식별자만 가지고 있으면 된다. 실행되는 sql은 아래와 같다. 123SELECT m.*FROM Member m WHERE m.id = ? -- member 파라미터 id 값 외래키 비교도 마찬가지다 1234List&lt;Member&gt; result = em.createQuery("SELECT m FROM Member m WHERE m.team = :team") .setParameter("team", team) // 엔티티 객체 직접 사용 .getResultList(); 실행되는 sql은 아래와 같다. 123SELECT m.*FROM Member m WHERE m.team_id = ? -- team 파라미터 id 값 MEMBER 테이블은 이미 TEAM의 식별자 값을 가지고 있기 때문에 묵시적 조인은 일어나지 않는다. Named 쿼리(정적 쿼리) em.createQuery(&quot;select ... &quot;) 처럼 JPQL을 직접 문자로 넘기는 것을 동적 쿼리라고 하고, 미리 정의한 쿼리에 이름을 부여해서 해당 이름으로 사용하는 것을 Named 쿼리(정적 쿼리)라고 한다. Named 쿼리는 어플리케이션 로딩 시점에 JPQL 문법을 체크하고 미리 파싱해두므로 오류를 빨리 확인할 수 있고, 사용하는 시점에는 파싱된 결과를 재사용하므로 성능상 이점도 있다. Named 쿼리는 @NamedQuery 어노테이션을 사용해서 자바 코드에 작성하거나 XML 문서에 작성할 수 있다. 어노테이션에 정의 1234567891011121314@Entity@NamedQueries(&#123; @NamedQuery( name = "Member.findByUsername", query = "SELECT m FROM Member WHERE m.username = :username" ), @NamedQuery( name = "Member.count", query = "SELECT COUNT(m) FROM Member m" )&#125;)class Member&#123; // ...&#125; 위처럼 엔티티에 @NamedQuery, @NamedQueries 어노테이션을 사용해서 직접 정의해주면 된다. (Named 쿼리의 이름에 있는 Member가 뭔가 기능적으로 하는게 있는 것은 아니다. 그냥 관리의 편의성을 위함이다. 그리고 Named 쿼리는 영속성 유닛 단위로 관리되므로 충돌을 방지하기 위해 이름으로 구분한 것이기도 하다) 그리고 아래와 같아 사용해주면 된다. 1234List&lt;Member&gt; result = em.createNamedQuery("Member.findByName", Member.class) .setParameter("username", "joont") .getResultList(); XML에 정의 사실상 자바로 멀티라인 문자를 다루는 것은 상당히 귀찮은 일이므로, Named 쿼리를 작성할 때는 XML을 사용하는 것이 더 편리하다. 123456789101112131415161718192021222324&lt;!--xml version="1.0" encoding="UTF-8"?--&gt;&lt;entity-mappings xmlns="http://java.sun.com/xml/ns/persistence/orm" version="2.0"&gt; &lt;named-query name="Member.findByUserName"&gt; &lt;query&gt; select m from Member m where m.username = :username &lt;/query&gt; &lt;/named-query&gt; &lt;named-query name="Member.findByAgeOver"&gt; &lt;query&gt;&lt;![CDATA[ select m from Member m where m.age &gt; :age ]]&gt;&lt;/query&gt; &lt;/named-query&gt; &lt;named-native-query name="Inter.findByAlal" result-class="sample.jpa.Inter"&gt; &lt;query&gt;select a.inter_seq, a.inter_name_ko, a.inter_name_en from tb_inter a where a.inter_name_ko = ?&lt;/query&gt; &lt;/named-native-query&gt;&lt;/entity-mappings&gt; XML에서 &amp;, &lt;, &gt;,는 예약문자어 이므로 &amp;amp;, &amp;lt;, &amp;gt;를 사용해야 한다. &lt;![CDATA[ ]]&gt;를 사용하면 그 사이에 있는 문자를 그대로 출력하므로 예약 문자도 사용할 수 있다. 기타 Enum은 = 비교연산만 지원한다 는 JPQL 명세이고, 하이버네이트에서는 아래가 가능하다 123Delivery delivery = em.createQuery("select d from Delivery d where d.deliveryStatus like '%CO%'", Delivery.class) .getSingleResult(); 임베디드 타입은 비교를 지원하지 않는다 또한 JPQL 명세이고, 하이버네이트에서는 아래가 가능하다 1234Delivery foundDelivery = em.createQuery("select d from Delivery d where d.address = :address", Delivery.class) .setParameter("address", new Address("seoul", "새마을로", "1111-1111")) .getSingleResult(); 하이버네이트에서만 지원하는건가? JPA는 ''를 길이 0인 Empty String으로 정했지만 데이터베이스에 따라 ''를 null로 사용하는 곳이 있으니 확인하고 사용해야 한다. NULL 정의 조건을 만족하는 데이터가 하나도 없으면 NULL 이다 NULL은 알수 없는 값이다. NULL과의 모든 수학적 연산은 NULL이다. JPA 표준명세에서 정하는 NULL과의 논리연산은 NULL과 False를 AND 연산하면 False. NULL과 True를 OR 연산하면 True이다. 네이티브 SQL JPA는 표준 SQL이 지원하는 대부분의 SQL 문법과 함수들을 지원하지만, 특정 데이터베이스만 지원하는 함수나 문법, SQL 쿼리 힌트 같은 것들은 지원하지 않는다. 이런 기능을 사용하기 위해선 네이티브 SQL을 사용해야 한다. 네이티브 SQL이란 JPA에서 일반 SQL을 직접 사용하는 것을 말한다. 실제 데이터베이스 쿼리를 사용한다는 점 외에는 JPQL을 사용할때와 거의 비슷하다. (원래는 위치기반 파라미터만 지원하지만 하이버네이트는 이름기반 파라미터까지 지원한다) 엔티티 조회 Query createNativeQuery(String sqlString, Class resultClass) 를 말한다. 반환타입을 줘도 TypedQuery가 아닌 Query를 반환하는 이유는, JPA 1.0에서 규약이 그렇게 정의되어 버렸기 때문에 그렇다고하니 신경쓰지 않아도 된다. 이 메서드로 조회해온 엔티티는 영속성 컨텍스트에서 관리된다. 그러므로 모든 필드를 다 조회하는 SQL을 실행해야 한다 특정 필드만 조회해오면 오류가 발생한다. 123456String sql = "SELECT * FROM Member WHERE id = 1";Member memberFromNative = (Member)em.createNativeQuery(sql, Member.class).getSingleResult();Member memberFromJPQL = em.find(member.class, 1);assertSame(memberFromnNative, memberFromJPQL); // success 값 조회 Query createNativeQuery(String sqlString) 를 말한다. 123456789String sql = "SELECT id, name, age FROM Member";List&lt;Object[]&gt; list = em.createNativeQuery(sql).getResultList();for(Object[] row : list)&#123; Integer id = row[0]; String name = row[1]; Integer age = row[2];&#125; 결과 매핑 사용 네이티브 쿼리에서 여러값들이 나올 때 결과를 여러 엔티티나 엔티티+스칼라 형태로 적절히 합치는건데… 굳이 이것까지… Named 네이티브 쿼리 어노테이션의 경우 @NamedNativeQuery 사용하면 되고, XML의 경우 &lt;named-native-query&gt; 사용하면 된다. 될수 있으면 JPQL을 사용하고, 기능이 부족하면 HQL 등을 사용해보고, 그래도 안되면 네이티브 SQL을 사용하자 스토어드 프로시저 JPQL에서 사용 가능하지만(Named 스토어드 프로시저도 가능) MySQL에서 성능 이점이 그리 많지 않아 잘 사용되지 않으니 패스 벌크 연산(UPDATE, DELETE) JPQL로 여러 건을 한번에 수정하거나 삭제할 떄 사용한다. 아래는 UPDATE 벌크 연산이다. 1234567String sql = "UPDATE Product p " + "SET p.prce = p.price * 1.1 " + "WHERE p.stockAmount &lt; :stockAmount";int resultCount = em.createQuery(sql) .setParameter("stockAmount", 10) .executeUpdate(); executeUpdate 메서드를 사용한다. 벌크 연산으로 영향을 받은 엔티티 건수를 반환한다. 아래는 DELETE 벌크 연산이다. 123456String sql = "DELETE FROM Product p " + "WHERE p.price &lt; :price";int resultCount = em.createQuery(sql) .setParameter("price", 100) .executeUpdate(); 벌크 연산시 주의사항 벌크 연산은 영속성 컨텍스트를 무시하고 데이터베이스에 직접 쿼리한다는 특징이 있으므로 주의해야 한다. 아래는 발생가능한 문제 상황이다. 12345678Product product = em.find(Product.class, 1);assertThat(product.getPrice(), is(1000));String sql = "UPDATE Product p " + "SET p.prce = p.price * 1.1";em.createQuery(sql).executeUpdate();assertThat(product.getPrice(), is(1100)); // FAIL!! 벌크 연산은 영속성 컨텍스트와 2차 캐시를 무시하고 데이터베이스에 직접 쿼리한다. 따라서 위와 같이 영속성 컨텍스트와 데이터베이스 간에 데이터 차이가 발생할 수 있는 것이다. 이를 해결하기 위한 방법은 아래와 같다. em.refrest(entity) 사용 벌크 연산 직후에 em.refresh(entity)를 사용하여 데이터베이스에서 다시 상품을 조회하면 된다. 벌크 연산 먼저 실행 벌크 연산을 가장 먼저 실행하면 이미 변경된 내용을 데이터베이스에서 가져온다. 가장 실용적인 해결책이다. 벌크 연산 수행 후 영속성 컨텍스트 초기화 영속성 컨텍스트가 초기화되면 데이터베이스에서 다시 조회해오기 때문에 이것도 방법이다. 영속성 컨텍스트와 JPQL 영속성 컨텍스트에 이미 있는 엔티티를 JPQL로 다시 조회해올 경우 어떻게 처리될까? 12345Member member1 = em.find(Member.class, 1);List&lt;Member&gt; list = em.createQuery("SELECT m FROM Member m", Member.class) .getResultLst(); 이미 영속성 컨텍스트에 들어있는 1번 member가 JPQL에 의해 다시 한번 조회되는 상황이다. 결과부터 말하자면 JPQL 쿼리는 쿼리대로 다 날라가고, 조회한 엔티티를 영속성 컨텍스트에 다 저장한다. 여기서 중요한 점은 1번 member의 경우 영속성 컨텍스트에 이미 들어있으므로, JPQL로 조회해온 1번 member는 그냥 버려진다는 것이다. 보다시피 조회해온 member들 중 1번 member는 영속성 컨텍스트에 이미 있으므로 그 결과가 버려진다. 영속성 컨텍스트에 없는 2번 member의 경우 영속성 컨텍스트에 저장된다. 새로 조회해온 결과를 기존 영속성 컨텍스트에 덮어쓰지 않는 이유는 영속 상태인 엔티티의 동일성을 보장해야하기 때문이다. find로 들고오든, JPQL로 들고오든 동일한 엔티티를 반환해야 한다. 1234567Member member1 = em.find(Member.class, 1);Member member2 = em.createQuery("SELECT m FROM Member WHERE m.id = :id", Member.class) .setParameter("id", 1) .getSingleResult();assertSame(member1, member2); // SUCCESS 보다시피 영속성 컨텍스트에 1번 member 엔티티가 있더라도 무조건 SQL을 실행해서 조회해온다. (JPQL을 분석해서 영속성 컨텍스트를 조회하는 것은 너무 힘들기 때문이다.) 그리고 조회해온 엔티티를 영속성 컨텍스트에 넣을 때, 이미 있는 엔티티일 경우 결과를 버린다. JPQL과 플러시 모드 플러시 모드는 FlushMode.AUTO(Default), FlushMode.COMMIT이 있다. 이때까지 FlushMode.AUTO 는 트랜잭션이 끝날때나 커밋될 때만 플러시를 호출하는 것으로 알고 있었으나, 사실은 시점이 하나 더 있다. JPQL 쿼리를 실행하기 직전이다. 123456789Member member1 = em.find(Member.class, 1);member1.setName("modified name");Member member2 = em.createQuery("SELECT m FROM Member WHERE m.id = :id", Member.class) .setParameter("id", 1) .getSingleResult();assertThat(member1.getName(), member2.getName()); 변경감지는 플러시 될때 발생하므로, JPQL에서 아직 변경되지 않은 name 값을 가진 1번 member를 가져올 것이라고 생각할 수 있지만, FlushMode.AUTO는 영속 상태인 엔티티의 동일성을 보장하기 위해 JPQL 실행 전에 플러시를 수행한다. 그러므로 위의 테스트는 성공한다. 어떻게 동작하는지 정확히는 모르겠으나, 영속성 컨텍스트에 있는 엔티티에 대해 JPQL을 실행할 떄만 플러시를 수행한다. 즉, 위의 상황에서 JPQL로 Team을 조회해올 경우 플러시가 발생하지 않는다. 여기서 플러시 모드를 FlushMode.COMMIT으로 설정하면 쿼리전에 플러시를 수행하지 않으므로 위의 테스트가 실패하게 된다. 이때는 직접 em.flush를 호출해주거나, Query 객체에 직접 플러시 모드를 설정해주면 된다. 1234567891011121314em.setFlushMode(FlushMode.COMMIT); // 커밋시에만 플러시Member member1 = em.find(Member.class, 1);member1.setName("modified name");em.flush(); // 1. em.flush 직접 호출Member member2 = em.createQuery("SELECT m FROM Member WHERE m.id = :id", Member.class) .setParameter("id", 1) .setFlushMode(FlushMode.AUTO) // 2. setFlushMode 설정 .getSingleResult();assertThat(member1.getName(), member2.getName()); FlushMode.COMMIT은 너무 잦은 플러시가 일어나는 경우, 플러시 횟수를 줄여서 성능을 최적화하고자 할 때 사용할 수 있다. 한 트랜잭션 안에서 특정 엔티티에 대한 insert, update, delete를 수행하고 그 뒤에서 JPQL로 전혀 다른 엔티티의 값을 읽어올 경우 불필요한 flush가 날라가게 된다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>객체지향 쿼리 언어</tag>
        <tag>JPQL 문법</tag>
        <tag>네이티브 쿼리</tag>
        <tag>벌크 연산</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 값 타입]]></title>
    <url>%2Fjpa%2F%EA%B0%92-%ED%83%80%EC%9E%85%2F</url>
    <content type="text"><![CDATA[JPA의 데이터 타입은 크게 엔티티 타입과 값 타입이 있다. 엔티티 타입은 @Entity로 정의하는 객체이고, 값 타입은 int, Integer, String 처럼 단순 값으로 사용하는 자바 기본 타입이나 객체를 말한다. (영한님은 여기서 엔티티 타입은 살아있는 생물이고, 값 타입은 단순한 수치 정보라고 표현했다.) JPA는 값 타입을 표현하는 방법을 몇가지 더 제공한다. 아래는 JPA에서 제공하는 값 타입의 목록이다. 기본값 타입(primitive, wrapper, String) 임베디드 타입 값 타입 컬렉션 값 타입은 기본적인 특징은 아래와 같다. 식별자가 없다 생명주기가 엔티티에 의존한다 공유하면 안된다 값 타입 자바의 primitive 타입, Wrapper 클래스, String 클래스를 말한다. 12345678@Entityclass Member&#123; @Id private Long id; private String name; private int age;&#125; 임베디드 타입(복합 값 타입) 여러개의 값 타입을 묶어서 하나의 값 타입으로 정의하는 방법이다. 우선 값 타입을 적용하기 전의 코드는 아래와 같다. 12345678910111213141516@Entityclass Member&#123; @Id private Long id; // 근무기간 @Temporal(TemporalType.DATE) private Date startDate; @Temporal(TemporalType.DATE) private Date endDate; // 집주소 private String city; private String street; private String zipCode;&#125; 위 처럼 엔티티가 모든 속성을 flat 하게 가지는 것은 객체지향적이지 않다. 근무기간, 집주소로 묶을 수 있다면 더 좋을 것이다. 12345678910111213141516171819202122232425262728293031@Entityclass Member&#123; @Id private Long id; @Embedded private Period workPeriod; @Embedded private Address homeAddress;&#125;@Embeddableclass Period&#123; @Temporal(TemporalType.DATE) private Date startDate; @Temporal(TemporalType.DATE) private Date endDate; public boolean isWork(Date date)&#123; // 값 타입을 위한 메서드 또한 작성 가능 &#125;&#125;@Embeddableclass Address&#123; @Column(name = "city") // 매핑할 컬럼 지정 가능 private String city; private String street; private String zipcode;&#125; 작성한 값 타입은 다른 곳에서 재사용 될수도 있고, 값 타입만을 위한 메서드도 작성 가능하다. 엔티티가 더욱 의미있고 응집력있게 변했다 이러한 임베디드 타입을 정의하려면 아래의 2가지 어노테이션이 필요하다. @Embeddable : 값 타입을 정의하는 곳에 표시 @Embedded : 값 타입을 사용하는 곳에 표시 임베디드 타입과 테이블 매핑 이렇게 작성한 임베디드 타입은 테이블에 아래와 같이 매핑된다. 임베디드 타입은 엔티티의 값일 뿐이다. 그러므로 보다시피 임베디드 타입을 사용하기 전과 후에 매핑되는 테이블은 같다. ORM을 사용하지 않았더라면 객체와 테이블은 대부분 1:1로 매핑되었을 것을, ORM을 사용함으로써 객체와 테이블을 더 세밀하게 매핑할 수 있다. (잘 설계한 ORM 어플리케이션은 매핑힌 클래스의 수가 테이블의 수보다 더 많다) 임베디드 타입의 포함과 연관관계 임베디드 타입은 다른 임베디드 타입을 포함할 수 있고, 다른 엔티티를 참조할 수도 있다. 1234567891011121314151617181920212223242526272829303132333435363738394041@Entityclass Member&#123; // ... @Embedded private Address address; @Embedded private PhoneNumber phoneNumber;&#125;@Embeddableclass Address&#123; private String city; private String street; @Embedded // 포함 가능 private Zipcode zipcode;&#125;@Embeddableclass Zipcode&#123; String zip; Strign code;&#125;class PhoneNumber&#123; String areaCode; String localNumber; @ManyToOne // 연관관계 가능 PhoneServiceProvider phoneServiceProvider;&#125;@Entityclass PhoneServiceProvider&#123; @Id private String name;&#125; 속성 재정의: @AttributeOverride 아래와 같이 정의하고 싶을 수 있다. 12345678class Member&#123; // ... @Embedded private Address homeAddress; @Embedded private Address companyAddress;&#125; ORM 에서만 객체로 묶을 뿐, 테이블 레벨에선 flat하게 펴지므로 위와 같이 정의하는 것은 불가능하다. 컬럼명이 중복되기 때문이다. 이럴땐 @AttributeOverride를 통해 컬럼명을 재정의해줘야 한다. 12345678910111213class Member&#123; // ... @Embedded private Address homeAddress; @Embedded @AttributeOverrides(&#123; @AttributeOverride(name = "city", column = @Column(name = "company_city")), @AttributeOverride(name = "street", column = @Column(name = "company_street")), @AttributeOverride(name = "zipcode", column = @Column(name = "company_zipcode")) &#125;) private Address companyAddress;&#125; name에는 Address 내의 필드명을 써주고, column에는 @Column 어노테이션을 써서 재정의 해주면 된다. 어노테이션을 너무 많이 사용되서 지저분하긴 하지만, 다행히(?) 이렇게 한 엔티티에 중복해서 임베디드를 사용할 일이 많이 없다. @AttributeOveride는 엔티티에 설정해야 한다. 임베디드 타입이 임베디드 타입을 가지고 있어도 엔티티에 설정해야 한다. 임베디드 타입과 null 임베디드 타입이 null 이면 매핑한 컬럼 값을 모두 null 이 된다(!!) 12// city, street, zipcode가 모두 null이 됨 member.setAddress(null); 임베디드 타입의 딜레마 값을 다룰때는 기본적으로 참조가 아닌 복제의 형태를 따른다. 하지만 여기서 문제는, JPA에서는 임베디드 타입이 값 타입인데, 형태는 일반적인 클래스라 참조 방식으로 동작한다는 것이다. 불변성 참조 방식으로 동작하므로 아래와 같은 상황을 막을 수 없다. 123456789101112131415Address address1 = new Address("city", "street", "zipcode1");Member member1 = Member.builder() .name("joont1") .homeAddress(address1) .build();Address address2 = address1;address2.setZipcode("zipcode2");Member member2 = Member.builder() .name("joont2") .homeAddress(address2) .build();em.persist(member1);em.persist(member2); 기대하는 것은 member1에 zipcode1, member2에 zipcode2가 저장되어야 하는 것이지만(값 타입의 특성상), 당연히 그렇게 처리되지 않는다. 참조 방식으로 동작하기 때문이다. 때문에 JPA에서 값 타입을 사용할때는 setter 등을 모두 제거한 불변객체로 다루어야 하고, (자바에서 불변 객체로 만드는 가장 간단한 방법은 setter 제거이다) 값을 재사용 할 때는 deep copy를 수행해서 절대 재사용 되는 일이 없도록 해야한다. 1234567891011@AllArgsConstructor // 전체 프로퍼티를 받는 생성자@Embeddableclass Address implements Cloneable&#123; private String city; private String street; private String zipcode; public Object clone() throws CloneNotSupportedException&#123; return super.clone(); &#125;&#125; 123456789101112131415Address address1 = new Address("city", "street", "zipCode1");Member member1 = Member.builder() .name("joont1") .homeAddress((Address)address1) .build();Address address2 = address1.clone(); // 복사address2.setZipcode("zipcode2");Member member2 = Member.builder() .name("joont2") .homeAddress((Address)address2) .build();em.persist(member1);em.persist(member2); 현재는 Address 내부가 flat해서 간단히 Object.clone()의 호출만으로도 클로닝이 되지만, 다른 임베디드 타입을 사용하거나 배열을 사용하고 있었을 경우 해당 필드까지 전부 deep clone을 해줘야한다. 하지만 또 반대로 임베디드 타입이 엔티티를 가지고 있을 경우 deep clone 하면 안된다. 이렇듯이 골치아픈게 deep clone이기 때문에, 가급적이면 @Embeddable 내부는 flat 하게 유지해주는 것이 좋다. 비교 값 타입이라면 동일성 비교(==)나 동등성 비교(equals)가 동작해야 한다. 하지만 현재 Address는 그것이 보장되지 않으므로, equals 메서드를 재정의 해줘야한다. 12345678@AllArgsConstructor@EqualsAndHashCode // 전체 필드에 대해 equals와 hashCode 재정의@Embeddableclass Address&#123; private String city; private String street; private String zipcode;&#125; 임베디드 타입의 equals 메서드를 재정의 할 때는 보통 모든 필드의 값을 비교하도록 구현한다. 그리고 equals 메서드를 재정의하면 hashCode까지 같이 재정의 해주는 것이 좋다. 그렇지 않으면 해시를 사용하는 컬렉션(HashSet, HashMap)에서 문제가 발생할 수 있기 때문이다. 값 타입 컬렉션 여러개의 값 타입을 저장할 떄 사용한다. 그러려면 컬렉션에 저장해야 하는데, RDB에서는 필드에 컬렉션을 저장할 수 없다. 그러므로 값 만을 저장하는 테이블을 따로 만들어서 사용해야 한다. 위 ERD를 엔티티에서 매핑하면 아래와 같다. 12345678910111213141516171819202122@Entityclass Member&#123; // ... @Embedded private Address homeAddress; @ElementCollection @CollectionTable( name = "FAVORITE_FOOD", joinColumns = @JoinColumn(name = "member_id") ) @Column(name = "food_name") private List&lt;String&gt; favoriteFoodList = new ArrayList&lt;&gt;(); @ElementCollection @CollcetionTable( name = "ADDRESS_HISOTRY", joinColumns = @JoinColumn(name = "member_id") ) private List&lt;Address&gt; addressHistory = new ArrayList&lt;&gt;(); // Address는 위와 동일&#125; @ElementCollection으로 값 타입 컬렉션 인것을 알려주고, @CollectionTable로 해당 값들을 저장한 테이블을 알려주면 된다(외래키랑 같이). favorite_food처럼 값으로 사용되는 컬럼이 하나일 경우 @Column을 사용해서 컬럼명을 지정할 수 있다. 값 타입 컬렉션은 무조건적으로 CascadeType.ALL, orphanRemoval = true가 붙은것 처럼 동작한다. 값 타입 컬렉션 사용 저장 123456789101112Member member = new Member();member.setHomeAddress(new Address("city", "street", "zipCode4"));member.getFavoriteFoodList().add("pork");member.getFavoriteFoodList().add("beef");member.getAddressHistory().add(new Address("city1", "street1", "zipcode1"));member.getAddressHistory().add(new Address("city2", "street2", "zipcode2"));member.getAddressHistory().add(new Address("city3", "street3", "zipcode3"));em.persist(member); member : insert 1번 homeAddress : 임베디드 값 타입이므로 member에 포함됨 favoriteFoodList : insert 2번 addressHistory : insert 3번 조회 값 타입 컬렉션도 조히할 때 패치 전략을 사용할 수 있다. Default는 LAZY이다. 1@ElementCollection(fetch = FetchType.LAZY) 조회 방식은 일반적인 @OneToMany 조회 할때와 동일하다. 직접 사용할 때 조회된다. 수정 1234567Member member = em.find(Member.class, 1);List&lt;String&gt; favoriteFoodList = member.getFavoriteFoodList();favoriteFoodList.set(0, "changed pork");favoriteFoodList.set(1, "changed beef");List&lt;Address&gt; addressHisotry = member.getAddressHistory();addressHisotry.get(0).setStreet("changed street"); 값 타입은 식별자가 없는 단순한 값들의 모음으로 테이블에 저장된다. 그래서 여기에 저장된 값이 변경되면 데이터베이스에 저장된 원본 데이터의 값을 찾기 어렵게 된다. 이러한 문제로 인해 JPA는 값 타입 컬렉션에 변경사항이 발생하면, 값 타입 컬렉션에 매핑된 테이블의 모든 데이터를 삭제하고, 현재 값 타입 컬렉션에 있는 모든 값을 다시 데이터베이스에 저장한다. 즉, 위와 같은 코드에서는 아래와 같이 쿼리가 발생한다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** address_history **/delete from ADDRESS_HISTORY where member_id=1insert into ADDRESS_HISTORY (member_id, city, street, zipcode) values (1, "city1", "street1", "zipcode1")insert into ADDRESS_HISTORY (member_id, city, street, zipcode) values (1, "city2", "changed street", "zipcode2") -- insert modifired datainsert into ADDRESS_HISTORY (member_id, city, street, zipcode) values (1, "city3", "street3", "zipcode3")/** favorite_food **/delete from FAVORITE_FOOD where member_id=1insert into FAVORITE_FOOD (member_id, food_name) values (1, "changed pork"); -- insert modifired datainsert into FAVORITE_FOOD (member_id, food_name) values (1, "changed beef"); -- insert modifired data 이러한 비효율적인 특징이 있으므로 만약 값 타입 컬렉션에 매핑된 테이블에 데이터가 많다면 값 타입 컬렉션 대신 일대다 관계를 고려해보는 것이 좋다. 게다가 값 타입 컬렉션은 모든 컬럼을 묶어서 기본키를 구성하므로, 컬럼에 null을 입력할 수 없고, 중복된 값을 입력할 수 없는 제약조건도 있다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>@Embedded</tag>
        <tag>@Embeddable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UML]]></title>
    <url>%2Fetc%2FUML%2F</url>
    <content type="text"><![CDATA[UML 종류 https://myeonguni.tistory.com/752 클래스 다이어그램, 시퀀스 다이어그램, 활동 다이어그램 외에는 잘 안쓴다 패키지 다이어그램 가끔씩 쓰고… UML 정의와 표기법 http://www.nextree.co.kr/p6753/ generalization : 상속(extends) realization : 구현(implemetns) assocication : 참조 aggregation : 컬렉션으로 참조(whole과 part의 관계) composition : 컬렉션으로 참조(whole이 part의 전체 생명주기를 책임짐. 강력한 aggregation)]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>UML 표기법</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 신규 엔티티에 기존 엔티티의 id를 넣어줄때 발생하는 일]]></title>
    <url>%2Fjpa%2F%EC%8B%A0%EA%B7%9C-%EC%97%94%ED%8B%B0%ED%8B%B0%EC%97%90-%EA%B8%B0%EC%A1%B4-%EC%97%94%ED%8B%B0%ED%8B%B0%EC%9D%98-id%EB%A5%BC-%EB%84%A3%EC%96%B4%EC%A4%84%EB%95%8C-%EB%B0%9C%EC%83%9D%ED%95%98%EB%8A%94-%EC%9D%BC%2F</url>
    <content type="text"><![CDATA[JPA를 개발하다 보면, 아래와 같은 행위를 할떄가 가끔씩 있다. (Spring Data JPA를 사용한다고 가정) 123456Member newMember = newMemberDTO.toEntity();newMember.setId(1); // 이렇게 id를 직접 넣어주는 행위// ...memberRepositroy.save(newMember); 이는 간단하다. Repository를 만들때 implements하는 JpaRepository의 구현체인 SimpleJpaRepository의 save 메서드를 보면 아래와 같다. 123456789101112131415161718192021222324252627// repository.save@Transactionalpublic &lt;S extends T&gt; S save(S entity) &#123; if (entityInformation.isNew(entity)) &#123; em.persist(entity); return entity; &#125; else &#123; return em.merge(entity); &#125;&#125;// entityInformation.isNewpublic boolean isNew(T entity) &#123; ID id = getId(entity); Class&lt;ID&gt; idType = getIdType(); if (!idType.isPrimitive()) &#123; return id == null; &#125; if (id instanceof Number) &#123; return ((Number) id).longValue() == 0L; &#125; throw new IllegalArgumentException(String.format("Unsupported primitive id type %s!", idType));&#125; 보다시피 엔티티의 식별자가 전달되었다면 merge, 그렇지 않다면 persist를 수행한다. 위와 같이 식별자를 강제로 넣어서 전달하면 해당 엔티티에 대해서 em.merge가 발생하게 되는 것이다. merge니까, 해당 식별자로 member 테이블을 조회하고, 엔티티가 존재하면 영속성 컨텍스트에 넣게 된다. 그리고 flush 되면 변경감지에 의해 update 문이 발생하게 될 것이다.(newEntity로 완전 대체되었기 때문에) 만약 해당 식별자에 해당하는 엔티티가 없다면, 새로 insert 될 것이다. PUT REST API를 작성하기에 적절한 방법이다.(틀릴수도…) 그렇다면 아래와 같이 하면 어떻게 될까? 12345678910Member newMember = newMemberDTO.toEntity();newMember.setId(1); // 기존 member의 id값을 그대로 유지// oldItem의 값들이 몇개 필요해서 들고와서 세팅Member oldMember = memberRepository.findById(1);newMember.setXXX(oldMember.getXXX()); oldItem.setXXX(~~~); // 이렇게 하면 어떻게 되지? memberRepository.save(newMember); newItem으로 대체하는데 oldItem의 값들이 몇개 필요해서 oldEntity를 조회한 다음 해당 값을 사용하는 케이스이다. 이럴 경우, find에 의해 처음 영속성 컨텍스트로 들고오면 아래와 같을 것이다. key entity 1 oldEntity 이 상태에서 아래의 save에 의해 merge가 수행될 것이고, 결과적으로 아래처럼 변할것이다. key entity 1 newEntity 즉 oldEntity는 아예 영속성 컨텍스트에 의해 관리되지 않는 준영속 상태가 되어버리므로, 위처럼 oldItem에 무언가 변경을 수행해도 아무일도 일어나지 않는다. (당연한가?) 자식이 있을 경우 해당 엔티티에 속하는 자식 엔티티들이 있을 경우 조금 복잡해진다.(안 복잡할수도 있다) 1234class Member&#123; @OneToMany(mappedBy = "member", cascade = CascadeType.PERSIST) private List&lt;Order&gt; orderList = new ArrayList&lt;&gt;();&#125; 위와 같다고 할 때, 아래와 같이 작성하면 문제가 발생한다. 12345Member newMember = newMemberDTO.toEntity(); // orderList를 가지고 있음newMember.setId(1); // 기존 member의 id값을 그대로 유지memberRepository.save(newMember); 이때는, newMember의 orderList에 대해 연쇄적으로 persist를 수행하고 끝나버릴 것이므로, 만약 기존의 1번 member에 해당하는 order가 있었다고 할 경우, 해당 order는 그대로 있고 신규로 전달된 order들이 저장되게 될 것이다. 그러므로 만약 기존 1번 member에 해당하는 order가 2개 있고, 전달된 order가 2개인 상태에서 위의 메서드를 실행하게 되면 order가 총 4개가 되어버린다. 사실상 이건 명확한 교체가 아니므로, 의도한 상황이라면 상관없는데, 그게 아니라면 orphanRemoval = true를 사용하여 존재하지 않는 엔티티에 대해 삭제하게끔 해야한다. 1234class Member&#123; @OneToMany(mappedBy = "member", cascade = CascadeType.PERSIST, orphanRemoval = true) private List&lt;Order&gt; orderList = new ArrayList&lt;&gt;();&#125; 1234Member newMember = newMemberDTO.toEntity(); // orderList를 가지고 있음newMember.setId(1); // 기존 member의 id값을 그대로 유지memberRepository.save(newMember); 이렇게 하면 기존 1번 member에 있던 order는 모두 삭제되고, newMember에 있는 order가 전부 insert 된다. 만약 기존 order에 새로 전달받은 order를 덧붙이고 싶으면 newMember의 orderList에 살려놓을 order들을 추가해줘야 한다. 123456789Member newMember = newMemberDTO.toEntity(); // orderList를 가지고 있음newMember.setId(1); // 기존 member의 id값을 그대로 유지Member oldMember = memberRepository.findById(1);List&lt;Order&gt; aliveOrder = 살려놓을_주문_구하기(oldMember.getOrder());newMember.getOrderList().addAll(aliveOrder);memberRepository.save(newMember); 참고로 여기서 merge가 진행되고 나면 기존에 oldMember의 자식들에 변형이 가해진다(무슨 기준인지는 모르겠다) 이거까지 신경쓰는건 좋지 않은 것 같다… id를 바꾸고 merge 할 생각이라면 merge 할 대상만 신경쓰도록 하고, 기존 entity의 list에 뭔가를 수행하고 싶을 경우 merge 전에 해주는게 좋겠다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>orphanRemoval</tag>
        <tag>entity child diff</tag>
        <tag>merge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] CascadeType.PERSIST를 함부로 사용하면 안되는 이유]]></title>
    <url>%2Fjpa%2FCascadeType-PERSIST%EB%A5%BC-%ED%95%A8%EB%B6%80%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%A9%B4-%EC%95%88%EB%90%98%EB%8A%94-%EC%9D%B4%EC%9C%A0%2F</url>
    <content type="text"><![CDATA[엔티티의 자식에 CascadeType.PERSIST를 지정할 경우 JPA에서 추가적으로 수행하는 동작이 있고, 이 때문에 예상치 못한 사이드 이펙트가 발생할 수 있으므로 이를 남겨두고자 한다. 일단 기본적으로 cascade(영속성 전이)는 간단하다. EntityManager를 통해 영속성 객체에 수행하는 행동이 자식까지 전파되는 것이다. 12345em.persist(parent);==em.persist(parent);em.persist(child1);em.persist(child2); 변경 감지에서의 CascadeType.PERSIST 근데 여기 JPA 2.2 specification 문서의 3.2 장 Entity Instance's Life Cycle에 변경감지 부분인 3.2.4 Synchronization to the Database에 보면 아래와 같은 내용이 추가적으로 있음을 볼 수 있다. The semantics of the flush operation, applied to an entity X are as follows: • If X is a managed entity, it is synchronized to the database. • For all entities Y referenced by a relationship from X, if the relationship to Y has been annotated with the cascade element value cascade=PERSIST or cascade=ALL, the persist operation is applied to Y. flush가 발생할 때 CascadeType.PERSIST나 CascadeType.ALL이 있을 경우 자식에 연쇄적으로 persist operation이 발생한다는 의미이다. 이 특징을 기반으로 아래의 행위들을 설명할 수 있다. Member와 Order의 관계는 아래와 같다고 가정한다. 123456789class Member&#123; @OneToMany(mappedBy = "member", cascade = CascadeType.PERSIST) private List&lt;Order&gt; orderList = new ArrayList&lt;&gt;();&#125;class Order&#123; @ManyToOne @JoinColumn(name = "member_id") private Member member;&#125; em.persist 12345678910111213Member member = new Member();Order order1 = new Order();Order order2= new Order();member.addOrder(order1);member.addOrder(order2);em.persist(member);Order order3 = new Order();member.addOrder(order3);// order1, order2, order3 insert 됨 member를 persist할 때 order1, order2 까지 연쇄적 persist가 발생하고, 트랜잭션이 끝나고 flush 될 때 자식들에 대해 다시 persist operation을 수행하게 된다. spec에 보면 persist operation은 아래와 같다. • If X is a new entity, it becomes managed. The entity X will be entered into the database at or before transaction commit or as a result of the flush operation. • If X is a preexisting managed entity, it is ignored by the persist operation. // … 즉 member의 orderList 3개에 대해 모두 persist operation이 발생하고, 앞의 2개는 이미 존재하던 것이므로 무시되고, 마지막 order3는 추가적으로 insert 되는 것이다. em.merge 12345678910111213Member member = new Member();Order order1 = new Order();Order order2 = new Order();member.addOrder(order1);member.addOrder(order2);member = em.merge(member);Order order3 = new Order();member.addOrder(order3);// order1, order2, order3 insert 됨 CascadeType.PERSIST 이므로 em.merge 할 때 자식까지 연쇄적으로 merge가 발생하지는 않는다. 하지만 flush 될 때 CascadeType.PERSIST에 의해 member 3개에 대해 모두 persist operation이 발생한다. em.find 12345678Member member = em.find(Member.class, 1);Order order1 = new Order();Order order2 = new Order();member.addOrder(order1);member.addOrder(order2);// order1, order2 insert 됨 이 또한 flush 될 떄 CascadeType.PERSIST에 의해 자식 order1, order2에 대해 persist operation이 수행된다. 즉 모든 행위는 flush의 CascadeType에 대한 특징 때문이다. 이러한 특징으로 봤을때, 우리가 의문을 가졌던 아래 코드 또한 설명이 된다. 1234567891011121314151617class Member&#123; @OneToMany(mappedBy = "member", cascade = CascadeType.MERGE) private List&lt;Order&gt; orderList = new ArrayList&lt;&gt;();&#125;Member member = new Member();Order order1 = new Order();Order order2= new Order();member.addOrder(order1);member.addOrder(order2);member = em.merge(member);Order order3 = new Order();member.addOrder(order3); // order1, order2 insert 됨 반면에 CascadeType.MERGE의 경우 flush와 관련이 없기 떄문에, em.merge 메서드에 전달한 엔티티까지만 연쇄적으로 merge가 되고, 아래는 그냥 무시되었던 것이다. persist operation의 대상 위에서 언급했다시피 CascadeType.ALL, CascadeType.PERSIST 어노테이션이 추가된 자식에 대해 모두 persist operation을 발생시킨다. (소스를 정확히 본것은 아니므로 틀릴 수 있음) 그러므로 아래의 두 코드에서 발생하는 insert가 동일하게 된다. 12345678910111213141516public void cascadeTest()&#123; Member member = em.find(Member.class, 1); // 5개의 orderList가 있다고 가정 member.addOrder(order1); member.addOrder(order2);&#125;// ==public void cascadeTest()&#123; Member member = em.find(Member.class, 1); // 5개의 orderList가 있다고 가정 member.getOrderList().clear(); // 기존의 애들을 다 지워버려도 member.addOrder(order1); member.addOrder(order2);&#125; 첫번쨰의 경우 총 7개의 order에 대해 persist operation을 수행하여 5개는 무시되고, 2개가 insert 된것이고, 두번쨰의 경우 clear로 날려버렸기 때문에 총 2개의 order에 대해 persist operation이 수행되어 2개가 insert 된 것이다.(기존에 있던 것을 삭제하고 싶으면 orphanRemoval = true를 줘야한다) 그러므로 위의 두 행위는 결과적으로 데이터베이스에 동일한 행위를 수행하게 되는 것이다. 예상치 못한 동작1 1234567891011121314151617181920212223class Member&#123; // ... @OneToMany(mappedBy = "member", cascade = CascadeType.PERSIST) private List&lt;Order&gt; orderList = new ArrayList&lt;&gt;();&#125;class Order&#123; // ... @ManyToOne @JoinColumn(name = "member_id") private Member member;&#125;public void deleteTest()&#123; Member member = em.find(Member.class, 1); List&lt;Order&gt; orderList = member.getOrderLsit(); for(Order order : orderList)&#123; em.remove(order); &#125;&#125; (지금은 예제가 간단하지만, 위와 같은 상황은 얼마든지 나올 수 있음) order가 삭제될 것이라고 예상할 수 있지만, flush시에 orderList에 남아있는 모든 order에 대해 persist 연산을 수행하므로 결과적으로 delete 메서드가 날라가지 않는 현상이 발생한다. 그러므로 CascadeType.PERSIST를 사용하고자 할 경우 삭제하는 order에 맞춰 orderList에서 요소를 삭제해주거나, orphanRemoval = true를 사용해 orderList에서 삭제되면 자동으로 delete 가 날라가게끔 해야한다. 예상치 못한 동작2 12345678910111213141516171819202122232425262728@Entityclass Member&#123; // ... @OneToMany(mappedBy = "member", cascade = CascadeType.ALL, orphanRemoval = true) private List&lt;Order&gt; orders;&#125;@Entityclass Item&#123; // ... @OneToMany(mappedBy = "item", cascade = CascadeType.ALL, orphanRemoval = true) private List&lt;Order&gt; orders;&#125;@Entityclass Order&#123; // ... @ManyToOne(fetch = FetchType.LAZY) @JoinColumn(name = "member_id") private Member member; @ManyToOne(fetch = FetchType.LAZY) @JoinColumn(nane = "item_id") private Item item;&#125; Member, Item이 있고 중간에 연결테이블로 Order를 가지고 있는 일반적인 구조이다. 여기서 만약 아래와 같은 행위를 수행하면 어떻게 될까? 123456789Member member = em.find(Member.class, 1);List&lt;Order&gt; orders = member.getOrders();for(Order order : orders)&#123; Integer size = order.getItem().getOrders().size(); // ...&#125;member.getOrders().clear(); // expecting delete operation Member가 가진 order들의 item이 가진 order들의 size를 가져오고 있다. (좀 어거지스럽긴 하지만 뭐 발생할려고 하면 어떻게든 발생할 수는 있는 상황이다.) 어찌됐든 여기서 중요한것은, size를 얻기 위한 행위 때문에 Item이 lazy 로딩 되었고, item의 Order들이 lazy 로딩 되었다는 점이다. 하고싶은 행위를 다 하고… 결과적으로 member 내의 order들이 다 쓸모없다고 판단해서 버리기로 한 모양이다. orphanRemoval에 의해 clear() 만 해줘도 다 삭제되어야 하는데, 삭제가 잘 될까? 아쉽게도 삭제되지 않는다. Member의 입장에서는 orders의 개수가 0개가 되었으므로 삭제를 시도하려고 할 것이다. 하지만 위에서 size를 얻기위해 Item, Item의 orders를 Lazy 로딩 시킨것이 문제이다. (굳이 lazy 로딩이 아닌 EAGER로 초기 로딩 등등 어떻게든 반대편도 영속성 컨텍스트에 올라갔다는 점이 중요 포인트이다) Item이 영속성 컨텍스트에 올라가게 되었는데, orders에는 CascadeType.ALL(PERSIST)가 걸려있다. 그러므로 member의 orders를 비워서 delete operation을 수행하고 싶어도, item의 orders 요소들이 아직 남아있기 때문에 PERSIST operation이 발생하게 되고, 결과적으로 delete가 씹히게 되는 것이다. 이를 해결하기 위한 가장 적절한(?) 방법으로는, 양쪽 다 CascadeType.ALL을 걸지않고 정말 필요한 한쪽만 거는 것이다. 위의 경우도 다시보면 item쪽에서 order를 cascade로 여러개 등록시킬 상황은 굳이 존재하지 않는다(물론 있을수도 있다) 과감히 제거해주면, delete 명령이 정상적으로 동작할 것이다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>CascadeType.PERSIST</tag>
        <tag>CascadeType.ALL</tag>
        <tag>add list insert</tag>
        <tag>delete doesn&#39;t work</tag>
        <tag>orphanRemoval doesn&#39;t work</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 프록시와 연관관계 관리]]></title>
    <url>%2Fjpa%2F%ED%94%84%EB%A1%9D%EC%8B%9C%EC%99%80-%EC%97%B0%EA%B4%80%EA%B4%80%EA%B3%84-%EA%B4%80%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[JPA에는 지연 로딩이라는 기능이 있다. 아래와 같은 엔티티가 있다고 할 때, 12345678910111213141516171819@Entityclass Member&#123; @Id private Long id; // ... @ManyToOne @JoinColumn(name = "team_id") private Team team;&#125;@Entityclass Team&#123; @Id private Long id; // ...&#125; 아래와 같이 Member를 조회하면 Team 테이블이 join 되어 같이 조회되었었다. 12Member member = em.find(Member.class, 1);// do something 1234SELECT T.*FROM MEMBER M LEFT JOIN TEAM T ON M.TEAM_ID = T.IDWHERE M.ID = 1; 만약 Member 테이블만 필요하다고 하면, 매번 위처럼 조회되는 것은 매우 비효율적이다. JPA는 이런 상황을 위해 지연 로딩이라는 기능(명세?)을 제공한다. 말 그대로 지연해서 로딩하는 것으로써, Member의 Team이 실제로 사용되는 순간에 해당 엔티티를 데이터베이스에서 조회해올 수 있다. JPA의 표준 명세는 지연로딩의 구현 방법을 JPA 구현체에 위임했다. 하이버네이트는 지연로딩을 지원하기 위해 프록시를 사용하는 방법과 바이트코드를 수정하는 방법을 사용한다. 프록시 em.getReference() EntityManager의 getRefernece() 메서드를 호출하면 엔티티를 바로 조회해오지 않고, 실제 사용하는 시점에 조회해올 수 있다.(find는 바로 조회해온다) 이러한 지연 로딩을 지원하기 위해 프록시 객체를 사용하는데, 반환되는 프록시 객체의 모습은 아마 아래와 같을 것이다. 1234567891011121314151617181920class MemberProxy extends Member&#123; Member target = null; public String getName()&#123; if(target == null)&#123; // DB 조회 // 실제 엔티티 생성 및 참조 보관 this.target = // ... &#125; return this.target.getName(); &#125;&#125;// 사용@Testpublic void getReferneceTest()&#123; Member member = em.getReference(Member.class, 1); member.getName();&#125; 보다시피 프록시 객체는 상속을 사용하여 구현한다. if(target == null) 내부에서 영속성 컨텍스트에 의해 데이터베이스를 조회해 실제 엔티티를 생성하는 것을 프록시 객체의 초기화라고 한다. 흐름은 아래와 같다.(영속성 컨텍스트는 비어있다고 가정한다) getReference()를 호출하면 프록시 객체를 생성한 뒤 영속성 컨텍스트(1차 캐시)에 저장한다. 실제 데이터를 얻기 위해 getName()을 호출한다. 프록시 객체는 영속성 컨텍스트에 실제 엔티티 생성을 요청한다(초기화) 영속성 컨텍스트는 데이터베이스를 조회해서 실제 엔티티 객체를 생성하고, 해당 객체의 참조를 target 변수에 보관한다. 프록시 객체는 target 변수에 저장된 실제 엔티티 객체의 getName()을 호출해서 결과를 반환한다. 영속성 컨텍스트는 1차 캐시를 포함한 큰 개념이므로, 헷갈릴 수 있음에 유의 프록시 객체는 다음과 같은 특징을 가진다. 프록시 객체의 초기화는 딱 한번만 실행된다. target 객체에 저장되면 그것을 계속 사용하기 때문이다. 원본 엔티티를 상속받은 객체이므로 타입 체크에 주의해야 한다. em.getReference() 실행 시 영속성 컨텍스트에 찾는 엔티티가 이미 있으면(식별자로 조회), 프록시 객체 대신 실제 엔티티를 반환한다. 12345Member member1 = em.getReference(Member.class, 1);Member member2 = em.find(Member.class, 1);System.out.println("member1 : " + member1.getClass().getName());System.out.println("member2 : " + member2.getClass().getName()); member1 : Member$HibernateProxy8guhz2idmemver2:Member8guhz2id memver2 : Member8guhz2idmemver2:MemberHibernateProxy$8guhz2id 처음 호출될 때 식별자를 이용해 1차 캐시에 저장하고, 초기화 되면 해당 프록시 객체내의 target 변수에 값이 저장되게 되는것이다. 이후에 em.find로 엔티티를 조회해와도 이미 프록시 객체가 저장되어 있기 떄문에 해당 객체가 반환된다. 영속성 컨텍스트의 동작 방식을 더 잘 설명하기 위해 일부러 반대로 예시를 들었다. 참고로 아래는 getReference() 에서 추적한 내용인데, 보다시피 처음 초기화 될 때 proxy 객체들을 따로 저장해둔다. 123456789101112131415161718192021222324// 초기화public void initialize(MetadataImplementor mappingMetadata, JpaMetaModelPopulationSetting jpaMetaModelPopulationSetting) &#123; // ... for ( final PersistentClass model : mappingMetadata.getEntityBindings() ) &#123; // ... // entity 객체 저장 entityPersisterMap.put( model.getEntityName(), cp ); // .... // proxy 객체 저장 final String old = entityProxyInterfaceMap.put( cp.getConcreteProxyClass(), cp.getEntityName() ); // ... &#125;&#125;// 사용@Overridepublic EntityPersister locateEntityPersister(Class byClass) &#123; EntityPersister entityPersister = entityPersisterMap.get( byClass.getName() ); if ( entityPersister == null ) &#123; String mappedEntityName = entityProxyInterfaceMap.get( byClass ); // ... &#125;&#125; 프록시와 식별자 프록시 객체는 target 변수만 가지고 있는것이 아니라, 전달받은 식별자 값도 같이 저장한다. 그러므로 아래와 같이 식별자 값만 조회할 경우 직접적인 데이터베이스 조회가 일어나지 않는다. 12Member member = em.getRefernece(Member.class, 1);member.getId(); // SQL 실행하지 않음 이러한 특징을 이용하면 연관관게를 설정할 때 유용하게 사용할 수 있다. 123456789Member member = new Member();member.setName("joont");member.setAge(27);// team settingTeam team = em.getReference(Team.class, 1);member.setTeam(team);em.persist(member); 데이터베이스에서 연관관계를 설정할때 외래키로 해당 데이터베이스의 식별자밖에 사용하지 않는다. 즉, member를 persist 할 때 team의 id만 필요할것이고, 실제로도 그렇게 처리될것이다. 이럴 경우 team을 전체 조회해오는 find 보다는 getReference()를 사용해서 데이터베이스 접근 횟수를 줄일 수 있다. 참고로 현업에서는 외래키 제약조건을 안거는 경우가 많으니… DB레벨에서 오류가 발생하지 않으므로 위험할 수 있다. 프록시 확인 JPA에서 제공하는 PersistenceUnitUtil.isLoaded(Object entity) 메서드를 사용하면 프록시 객체의 초기화 여부를 확인할 수 있다. 아직 초기화 되지 않은 엔티티의 경우 false를 반환한다. 쓸일이 있을랑가… 즉시로딩, 지연로딩 JPA에서는 연관된 엔티티를 조회해올 때도 프록시 객체를 사용하여 지연로딩을 할 수 있다. 지연로딩 여부는 연관관계를 맺는 어노테이션(@ManyToOne, @OneToMany…)의 속성(fetch)으로 제공하여 상황에 따라 개발자가 선택해서 사용할 수 있게 해준다. 제공되는 속성은 즉시로딩, 지연로딩 두 가지이다. 즉시로딩 fetch 속성을 FetchType.EAGER로 주면 된다. 123456789@Entityclass Member&#123; @Id private Long id; @ManyToOne(fetch = FetchType.EAGER) // 즉시로딩으로 설정 @JoinColumn(name = "team_id") private Team team;&#125; 12Member member = em.find(Member.class, 1); // team 까지 같이 조회됨 Team team = em.getTeam(); // 실제 엔티티 이렇게 설정해두면 Member 엔티티가 조회될 때 Team 엔티티가 항상 같이 조회된다. 대부분의 JPA 구현체는 즉시로딩을 최적화하기 위해 가능하면 조인 쿼리를 사용한다. 지연로딩 fetch 속성을 FetchType.LAZY로 주면 된다. 123456789@Entityclass Member&#123; @Id private Long id; @ManyToOne(fetch = FetchType.LAZY) // 지연로딩으로 설정 @JoinColumn(name = "team_id") private Team team;&#125; 123Member member = em.find(Member.class, 1); Team team = member.getTeam(); // 프록시 객체team.getName(); // 이때 조회됨! em.find(Member.class, 1)을 호출하면 Member만 조회하고 team 멤버변수에는 프록시 객체를 넣어둔다. 그리고 아래 실제 사용되는 부분에서 데이터가 조회된다.(동작 방식은 em.getReference()와 동일하다) 사용 시점에 조회해오므로 쿼리는 당연히 따로따로 날라간다. 컬렉션 래퍼 하이버네이트는 엔티티를 영속상태로 만들 때 엔티티에 컬렉션이 있으면 해당 컬렉션을 추적하고 관리할 목적으로 원본 컬렉션을 하이버네이트가 제공하는 내장 컬렉션으로 변경한다. 이를 컬렉션 래퍼라고 하고, org.hibernate.collection.internal.PersistentBag 클래스이다. 에 클래스가 컬렉션 레벨에서 프록시 객체의 역할까지 같이 해주므로, 이 클래스를 통해 지연로딩을 달성할 수 있다. 참고로 컬렉션의 실제 데이터를 조회할 때 데이터베이스를 조회해서 초기화한다. 12member.getTeam(); // SQL 실행안함 member.getTeam().get(0); // SQL 실행 그래서 뭘 설정해야 하는데? 양방향 연관관계 설정과 똑같다. 사용되는 곳에 따라 어떤 전략을 선택하면 좋을지 체크해보고, 선택하면 된다. 참고로 각 연관관계들은 default fetch 값이 있다. @OneToOne : EAGER @ManyToOne : EAGER @OneToMany : LAZY @ManyToMany : LAZY 보다시피 default값은 추가적으로 하나만 로딩해도 될때는 즉시로딩 되도록, 추가적으로 많은 데이터가 로딩될 수 있을 경우에는 지연로딩 되도록 설정되어 있다. (컬렉션을 로딩하는 것은 비용도 많이들고, 한번에 너무 많은 데이터를 로딩할 수 있기 때문이다.) 추천하는 방법은 전부 FetchType.LAZY를 사용하는 것이다. 그리고 어플리케이션 개발이 어느정도 완료단계에 왔을 때, 실제 사용 상황을 보고 꼭 필요한 곳에만 즉시 로딩을 사용하도록 최적화하면 된다. 참고로 SQL Mapper를 사용하면 이런 유연한 최적화가 어렵다.(ㅎㅎ) 컬렉션에 FetchType.EAGER를 사용할 때 주의할 점 EAGER를 하나 이상 설정하는 것은 권장하지 않는다. 컬렉션은 기본적으로 일대다 관계에서 사용되므로, 조인되는 테이블이 많아질수록 출력되는 row가 급격하게 증가하기 때문이다. 예를 들어 A 테이블과 N, M 테이블을 일대다 조인하면 N * M 개수의 행이 반환되고, 결과적으로 성능이 저하될 수 있다. 또한 JPA는 이렇게 조회된 결과 N과 M을 메모리에서 필터링 해서 반환하므로, 2개 이상의 컬렉션을 즉시 로딩으로 설정하는 것은 권장되지 않는다. 컬렉션 즉시 로딩은 항상 외부 조인을 사용한다. 내부 조인을 사용하면 자식이 없는 엔티티가 조회되지 않는 결과가 발생한다. 이를 제약조건으로 막을 수 있는 방법이 없으므로, 무조건 외부 조인을 사용한다. 영속성 전이(CASCADE) 특정 엔티티를 영속 상태로 만들 때 연관된 엔티티도 함께 영속 상태로 만들고 싶을 때 영속성 전이를 사용한다. CASCADE라는 옵션으로 제공하고, 실제 데이터베이스의 CASCADE와 동일하다. 영속성 전이는 매우 간단하다. EntityManager를 통해 영속성 객체에 수행하는 행동이 자식까지 전파된다고 보면 된다. 객체에 선언한 관계(@OneToOne, @OneToMany…)에 cascade 라는 속성값으로 지정해 줄 수 있다. 1234567891011121314151617181920212223242526@Entityclass Parent&#123; @Id private Long id; @OneToMany(mappedBy = "parent", cascade = CascadeType.PERSIST) private List&lt;Child&gt; children = new ArrayList&lt;&gt;();&#125;class Child&#123; @Id private Long id;&#125;public void save()&#123; Parent parent = new Parent(); // ... Child child1 = new Child(); Child child2 = new Child(); parent.addChild(child1); parent.addChild(child2); em.persist(parent); // 한방으로 해결&#125; 원래라면 child, child2 따로따로 다 저장해줬어야 했을것이지만, 영속성 전이를 사용하여 편리하게 저장함을 볼 수 있다. 쿼리는 아래와 같이 날라간다. 1234insert into parent values(...);insert into child values(...);insert into child values(...); 간단하게 설명해 CascadeType.PERSIST를 설정함으로써 아래와 같아졌다고 보면 된다. 12345em.persist(parent);==em.persist(parent);em.persist(child1);em.persist(child2); 영속성 전이의 범위는 위의 cascade 속성의 값으로 준 애들에 대해서만 동작한다. cascade의 종류는 아래와 같다. 12345678public enum CascadeType &#123; ALL, PERSIST, MERGE, REMOVE, REFRESH, DETACH&#125; CascadeType.MERGE를 주고 em.merge를 실행하면 자식까지 모두 em.merge가 실행되는 것이고, CascadeType.REMOVE를 주고 em.remove를 실행하면 자식까지 모두 em.remove가 실행되는 것이다. 간단하다. 키워드는 간단히 부모가 XXX 될 때, 자식들도 전부 XXX 시켜라 정도로 이해하면 된다. 당연한 얘기지만 cascade 속성을 줬을 경우 속성이 설정된 엔티티의 자식까지 모두 cascade 속성을 줘야한다. 그렇지 않으면 CascadeType.PERSIST를 줬을 경우 자식 엔티티가 전부 저장되지 않을 것이고, CascadeType.REMOVE를 줬을 경우 FK 제약조건에 걸려 에러가 발생할 것이다. 어디에, 어떻게 사용하는게 좋은가? https://vladmihalcea.com/a-beginners-guide-to-jpa-and-hibernate-cascade-types/ 이 블로그에서 영속성 전이의 Best Practice에 대해 설명하고 있다(짱짱맨) 첫 부분에서 JPA와 Hibernate의 CascadeType 속성에 대해 비교해주고 있는데, 보다시피 hibernate가 더 많은 CascadeType을 지원함을 볼 수 있다. 여기서 문제가 될 수 있는건, CascadeType.ALL을 지정했을 경우다. 구현체가 hibernate이기 때문에, JPA의 CascadeType.ALL 을 지정하면 hibernate의 LOCK CascadeType 등등이 다 적용될 수 있다. 그러므로 주의해서 사용해야 한다. 개인적으로 ALL보다는 사용하는 애들만 적절히 써주는게 좋을것 같다고 생각함. 그리고 그 아래에서 각 관계에서의 Best Pracetice에 대해 설명해주고 있는데, 간단히 요약하면 아래와 같다. @OneToOne은 bidirection 하게 설정해주는 것이 좋고, orphanRemoval 까지 주는 것이 좋다. 편의 메서드 또한 같이 넣어주는 것이 좋다. 기본적으로 @OneToMany 에서 사용하는 것이 일반적이고, 가장 많이 사용된다. @ManyToOne에서 cascade를 거는 것은 비정상적인 행위이니, 하지 않도록 한다. @ManyToMany에서 CascadeType.REMOVE를 사용하게 되면 내가 예상한 것 보다 훨씬 많은 삭제가 일어날 수 있다. 고아 객체 JPA는 부모 엔티티와 연관관계가 끊어진 자식 엔티티를 자동으로 삭제하는 기능을 제공하는데, 이를 고아(orphan)객체 제거라고 한다. 주가 되는 엔티티에서 연관된 객체의 참조가 제거되면, 그것을 고아 객체로 보고 삭제하는 기능이다. 1234567// @OneToOneMember member = em.find(Member.class, 1);member.setLocker(null); // locker 삭제됨// @OneToManyParent parent = em.find(Parent.class, 1);member.getChildren().remove(0); // child 삭제됨 자식이 부모의 생명주기에 묶여있는(특정 엔티티가 개인 소유하는) 엔티티에만 이 기능을 적용하는 것이 좋다. 삭제한 엔티티를 다른 곳에서도 참조하면 문제가 발생할 수 있기 떄문이다. 그래서 orphanRemoval은 @OneToMany, @OneToOne 관계에만 사용할 수 있다. 참고로 orphanRemoval에는 추가적인 기능이 하나 더 있는데, 부모를 자식까지 같이 제거되는 CascadeType.REMOVE의 기능이다. 개념적으로 부모를 제거하면 자식이 고아가 되기 떄문이다. 영속성 전이 + 고아객체 CacadeType.ALL + orphanRemoval = true 를 동시에 사용하면 부모를 통해서 자식 엔티티의 생명주기를 관리할 수 있게 된다. 1234567// child insertParent parent = em.find(Parent.class, 1);parent.addChild(child); // child removeParent parent = em.find(Parent.class, 1);parent.getChildren().remove(0); CacadeType.PERSIST로 자식을 컨트롤 할 수 있게 한 이유가 이해가 안간다. 일단, EntityManager로 수행하는 메서드와 CascadeType은 별개라고 생각해야 한다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>orphanRemoval</tag>
        <tag>FetchType.EAGER</tag>
        <tag>FetchType.LAZY</tag>
        <tag>지연 로딩</tag>
        <tag>Cascade</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] spring boot test]]></title>
    <url>%2Fspring%2Fspring-boot-test%2F</url>
    <content type="text"><![CDATA[https://meetup.toast.com/posts/124 spring boot test 모듈은 아래의 2개가 존재함 spring-boot-test spring-boot-test-autoconfigure 대부분 spring-boot-starter-test로 충분함(위의 2개를 다 포함하고 있나?) @SpringBootTest @ContextConfiguration의 발전된 기능? 테스트에 사용할 ApplicationContext를 쉽게 조작할 수 있음 반드시 @RunWith(SpringRunner.class)와 함께 사용해야함 Bean 설정 classes 속성을 통해 할 수 있음 설정한 클래스들만 빈으로 등록함 @Configuration을 설정할 경우 내부의 빈들도 전부 등록됨 @TestConfiguration 기존에 정의한 @Congifuration을 재정의하고 싶을 경우 @TestConfiguration에 정의된 bean으로 override 된다 테스트 클래스내에 선언할수도 있지만, 이러면 @ComponentScan시에만 감지되므로 classes를 명시했을 경우 사용할 수 없다는 단점이 있다 @Import를 써서 직접 사용하는 것이 더 좋다 이렇게 하면 여러클래스에서도 사용할 수 있다 @MockBean @MockBean 어노테이션을 사용하면 mock 객체를 빈드로 등록할 수 있다 @Autowired 등으로 주입받는 객체가 @MockBean으로 선언된 객체라면, 해당 mock 객체가 주입된다 @MockBean으로 선언한 객체가 이미 빈에 등록되어져 있다면 override 된다 properties 스프링은 테스트시에 기본적으로 class path의 application.properties(yml)을 참조한다 properties 속성으로 별도의 테스트를 위한 설정파일을 지정할 수 있다 test classpath의 application.yml이 있으면 그걸 먼저 참조…하나? TestRestTemplate @SpringBootTest와 RestTestTemplate를 같이 사용한다면 편리하게 웹 통합테스트가 가능하다 @SpringBootTest에서 Web Environment를 설정했다면 TestRestTemplate는 그에맞춰 자동으로 빈으로 생성된다 MockMvc는 servlet container를 생성하지 않고, TestRestTemplate는 servlet container를 생성함 그러므로 실제 동작되는 것처럼 테스트를 수행할 수 있다 클라이언트가 수행하는 것 처럼 테스트할 수 있다 트랜잭션 @Test 어노테이션과 @Transactional 어노테이션을 함꼐 사용했을 경우 테스트가 끝나면 rollback 됨 기존의 spring-test와 동일 만약 롤백하고 싶지 않다면 아래와 같이 함 12345@Test@Rollback(false)public void insetTest()&#123; // ...&#125; 하지만 webEnvironment의 RANDOM_PORT나 DEFINED_PORT로 테스트를 설정하면 테스트가 별도의 스레드에서 수행되기 때문에 rollback이 수행되지 않음 @JsonTest json serialize, deserialize를 편하게 테스트해볼 수 있다 @WebMvcTest 서버사이드 API Test? Async web Test async 테스트? @DataJpaTest memory db를 사용하고 테스트가 끝날떄마다 롤백됨 @Transactional 어노테이션을 포함하고 있음 테스트를 위한 TestEntityManager 클래스가 빈으로 등록된다 Spring Data JPA를 쓰지 않을 경우 유용할까? @JdbcTest @DataJpaTest와 유사하게 순수 JDBC를 테스트하고 싶을 경우 사용 테스트를 위한 JdbcTemplate가 생성된다 @DataMongoTest mongodb @RestClientTest 좀 더 리얼환경에 가깝게 api 테스트를 할 수 있음 RestTemplate에 반응하는 가상의 mock 서버라고 생각하면 된다?]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>@SpringBootTest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] externalId의 용도]]></title>
    <url>%2Fdb%2FexternalId%EC%9D%98-%EC%9A%A9%EB%8F%84%2F</url>
    <content type="text"><![CDATA[사용방법 PK를 그냥 key로 사용할 경우 외부에서 마스터 데이터의 양을 어느정도 알 수 있기 때문에 externalId 컬럼을 따로 만들어주고, 이것을 외부에 노출시키는 것이 좋다. externalId는 PK와 동일하게 unique key가 되어야 한다(unique constraints 필요) 내부적인 룰을 통해 이 값을 지정하거나 특정한 룰이 없다면 UUID로 생성한다 주의 externalId를 PK로 잡으면 클러스터드 인덱스의 특징상 미친듯한 재정렬이 일어나므로, 추가 컬럼으로 사용하는 것이 좋다.]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>externalId</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[liquibase]]></title>
    <url>%2Fetc%2Fliquibase%2F</url>
    <content type="text"><![CDATA[개념 database schema 변경을 tracking 하여 관리할 수 있게 해주는 open source이다. liquibase 문법에 맞춰 xml(yml) 파일을 작성한 뒤 liquibase 를 실행하면 해당 파일의 내용이 데이터베이스에 반영된다. 반영 방법은 command line, maven, gradle 등 다양한 방법으로 사용가능하다. (gradle은 liquibase-gradle-plugin을 사용하면 된다) 이 라이브러리를 사용한 이후 부터는 데이터베이스 스키마를 직접 수정하는 일은 지양(금지)해야 한다. 그리고 어느 라이브러리에서 추가해준 task인지는 모르곘으나…(jhipster애서 generate 된걸 그대로 사용하다보니…) gradle에 있는 liquibase 관련 task들 중 liquibaseDiffChangeLog를 실행하면 프로젝트에 작성한 entity와 데이터베이스 내 스키마를 참고하여 다른 부분을 liquibase 문법의 xml 파일로 생성해준다. (diff가 완벽하게 생성되지는 않으니, xml 파일을 다시 보면서 잘못된 부분이 없나 최종적으로 확인해야한다. 직접 작성해도 됨) 문법 아래는 liquibase의 간단한 예제이다. 123456789101112131415161718192021222324252627282930&lt;databaseChangeLog xmlns="http://www.liquibase.org/xml/ns/dbchangelog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ext="http://www.liquibase.org/xml/ns/dbchangelog-ext" xsi:schemaLocation="http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-3.1.xsd http://www.liquibase.org/xml/ns/dbchangelog-ext http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-ext.xsd"&gt; &lt;preConditions&gt; &lt;runningAs username="liquibase"/&gt; &lt;/preConditions&gt; &lt;changeSet id="1" author="nvoxland"&gt; &lt;createTable tableName="person"&gt; &lt;column name="id" type="int" autoIncrement="true"&gt; &lt;constraints primaryKey="true" nullable="false"/&gt; &lt;/column&gt; &lt;column name="firstname" type="varchar(50)"/&gt; &lt;column name="lastname" type="varchar(50)"&gt; &lt;constraints nullable="false"/&gt; &lt;/column&gt; &lt;column name="state" type="char(2)"/&gt; &lt;/createTable&gt; &lt;/changeSet&gt; &lt;changeSet id="2" author="nvoxland"&gt; &lt;addColumn tableName="person"&gt; &lt;column name="username" type="varchar(8)"/&gt; &lt;/addColumn&gt; &lt;/changeSet&gt; &lt;/databaseChangeLog&gt; changeSet에 어떤 행위들을 할 지 나열되어 있다.(createTable, addColumn 등) 이 파일을 liquibase로 실행하면 실제 데이터베이스에 반영될 것이다. 그리고 변경사항이 반영되면 DATABASECHANGELOG라는 테이블에(liquibase에서 사용하는 테이블) 위의 changeLog id값으로 로우가 쌓이게 된다. 이 말은 해당 변경사항은 적용되었다는 뜻이다. liquibase는 이 테이블 로우를 참고하여 changeSet이 이미 반영되었으면 skip, 반영되지 않았다면 반영한다. sql 쿼리를 그대로 적용하는법 123456789&lt;changeSet author="junyoung.park (generated)" id="20190307124000-7"&gt; &lt;sql&gt; CREATE TABLE BATCH_STEP_EXECUTION_SEQ ( ID BIGINT NOT NULL, UNIQUE_KEY CHAR(1) NOT NULL, constraint UNIQUE_KEY_UN unique (UNIQUE_KEY) ) ENGINE=InnoDB; &lt;/sql&gt;&lt;/changeSet&gt; 여러 파일 적용 한번에 여러 changelog 파일을 실행할 수 있다. 12345678910&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;databaseChangeLog xmlns="http://www.liquibase.org/xml/ns/dbchangelog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-3.5.xsd"&gt; &lt;include file="config/liquibase/changelog/00000000_initial_schema.xml" relativeToChangelogFile="false"/&gt; &lt;include file="config/liquibase/changelog/2018103001_added_spring_acl_schema.xml" relativeToChangelogFile="false"/&gt; &lt;include file="config/liquibase/changelog/2018122001_refactor-column-length.xml" relativeToChangelogFile="false"/&gt;&lt;/databaseChangeLog&gt; 이 파일을 liquibase로 실행하면 위에서부터 순서대로 반영한다. 주의사항 liquibase는 변경사항을 반영하기 전에 DATABASECHANGELOGLOCK 테이블에 LOCKED=1 인 상태로 레코드를 넣고 락을 건다 일반적인 상황에서는 문제되지 않지만, 한번에 서버를 2개 이상 띄워야 할 경우 데드락이 발생할 수 있으니 작성전에 이 레코드가 들어있는지 체크해보면 좋다 혹은 잘 뜨지 않는다면 이 테이블에 데이터가 있는지 체크해보자…]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>liquibase 사용법</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapStruct]]></title>
    <url>%2Fetc%2FMapStruct%2F</url>
    <content type="text"><![CDATA[기본 사용법 installation http://mapstruct.org/documentation/stable/reference/html/#setup 기본 사용법 http://mapstruct.org/documentation/stable/reference/html/#basic-mappings 클래스간 변환을 간편하게 해주는 라이브러리이다(Car &lt;&gt; CarDTO) 아래는 Mapper 선언법이다. 123456789101112@Mapper(componentModel = "spring", uses = &#123; OwnerMapper.class // 3&#125;)public interface CarMapper &#123; CarMapper INSTANCE = Mappers.getMapper( CarMapper.class ); @Mapping(source = "numberOfSeats", target = "seatCount") // 1 CarDto toDto(Car car); @Mapping(target = "id", ignore = true) // 2 Car toEntity(CarDTO dto);&#125; 보다시피 entity를 DTO로 변환해주는 작업을 한다(지금은 엔티티와 DTO를 예시로 들었지만 어떤 클래스든 상관없다). 아래의 generate 된 코드를 보면 알 수 있곘지만, 변환될 클래스는 setter가 필요하고, 변환대상 클래스는 getter가 필요하다. source와 target의 필드 이름이 다를 경우 직접 지정할 수 있다 ignore를 통해 특정 필드는 변환되지 않도록 설정할 수 있다. target만 신경쓰면 된다. 기본적으로 deep mapping 하는 코드까지 generation 해주긴하나, 기본적인 방식으로만 generation 된다(이름에 맞춰서 get/set) 그러므로 custom한 mapper가 필요하다면 위와 같이 선언해줘야 한다. install한 library로 빌드하면 @Mapper 인터페이스들을 찾아서 XXXImpl의 형태로 구현체를 모두 만든다.(빌드 방식 알아봐야함) 현재 componentModel을 spring으로 줬기때문에 생성되는 Impl은 스프링의 싱글톤 빈으로 관리된다(@Component 붙음) 생성된 구현체는 아래와 같다. 간단하다. 1234567891011121314@Overridepublic CarDTO toDto(Car entity) &#123; if ( entity == null ) &#123; return null; &#125; CarDTO carDTO = new CarDTO(); carDTO.setName( entity.getName() ); carDTO.setSeatCount( entity.numberOfSeats() ); carDTO.setOwner( ownerMapper.toDto(entity.getOwner() ) ); return carDTO;&#125; 아래는 간단한 사용법이다. 123456789101112// some servicepublic void save(CarDTO carDTO)&#123; Car car = CarMapper.INSTANCE.toEntity(carDTO); em.persist(car);&#125;public CarDTO select()&#123; Car car = em.find(Car.class, 1); return CarMapper.INSTANCE.toDTO(car);&#125;]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>@Mapper</tag>
        <tag>@Mapping</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[REST API Best Practice]]></title>
    <url>%2Frest%2FREST-API-Best-Practice%2F</url>
    <content type="text"><![CDATA[REST API 좋은 예제(읽어봐야함) https://developer.github.com/v3/ REST API는 Github API가 좋은 에제이다. 구글이나 페이스북의 경우 사람이 많아서 오히려 관리가 잘 안되고 있댜. 그에 비해 Github은 300명 정도로 운영하기 때문에, 좀 더 낫다고 함. Rest API 좋은 가이드라인(읽어봐야함) https://allegro-restapi-guideline.readthedocs.io/en/latest/ https://docs.microsoft.com/ko-kr/azure/architecture/best-practices/api-design 리스트 response에 대한 안티패턴 123456[ &#123; ... &#125; ...] 추가적인 정보는 대부분 헤더에 내려주긴 하지만, 가끔씩 헤더에 넣기 애매한 애들이 있다(page meta information 등). 그런 애들은 응답 바디에 넣어주는 것이 좋은데, 위와 같이 api 설계를 하면 추가적인 정보를 내려줄 수 없게된다. 그러므로 아래와 같이 해주는 것이 좋다. 12345678&#123; "data": [ &#123; ... &#125;, ... ]&#125;]]></content>
      <categories>
        <category>rest</category>
      </categories>
      <tags>
        <tag>REST</tag>
        <tag>REST API example</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] lombok practice 정리]]></title>
    <url>%2Fjava%2Flombok-practice-%EC%A0%95%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[상속 클래스에 builder 적용 https://reinhard.codes/2015/09/16/lomboks-builder-annotation-and-inheritance/ 또는 @SuperBuilder를 사용할 수 있음 intellij 구조적 문제로 에러로 표시된다는 특징이 있다… 컴파일은 잘 된다. Builder에서 필수값, 선택값 구분하기 빌더 패턴은 필수값과 선택값을 구분할 수 있다는 장점도 가지고있는데, 클래스 위에 그냥 @Builder를 선언하면 그 장점을 누리지 못하게 된다. 아래와 같이 작성해서 필수값, 선택값을 구분하게끔 할 수 있다. 1234567891011121314@Builder@AllArgsConstructorpublic class Member &#123; private String name; private Integer age; private String nickname; private String address; public static MemberBuilder builder(String name, Integer age)&#123; return new MemberBuilder() .name(name) .age(age); &#125;&#125; 사용은 아래와 같이 할 수 있다. 123Member.builder("joont",28) .address("somewhere in seoul") .build();]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>lombok</tag>
        <tag>@Builder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 고급 매핑]]></title>
    <url>%2Fjpa%2F%EA%B3%A0%EA%B8%89-%EB%A7%A4%ED%95%91%2F</url>
    <content type="text"><![CDATA[상속 관계 매핑 RDB는 객체지향 언어처럼 상속이라는 개념이 없다. 대신 슈퍼타입 서브타입 관계라는 모델링 기법이 있는데, 이게 상속 개념과 가장 유사하다. 상속 관계 매핑 기법은 물리 모델로 구현된 어떠한 슈퍼타입 서브타입 관계든, 객체 지향 상속 기법으로 추상화해서 접근할 수 있게 해준다는 것이 핵심이다. DB를 슈퍼타입 서브타입 관계의 조인 전략으로 모델링했든, 단일 테이블 전략으로 모델링했든 객체 입장에서는 동일한 상속 구조로 접근할 수 있다. 각각의 테이블로 변환(조인 전략) 엔티티 각각(자식, 부모 전부)을 테이블로 만들고, 자식 테이블이 부모의 기본키를 받아서 기본키 + 외래키로 사용하는 방법이다. 클래스 상속 구조랑 가깝게 생기긴 했다. 장점 테이블이 정규화된다 외래키 참조 무결성 제약 조건 사용 가능 저장 공간을 효율적으로 사용함(불필요하게 null을 넣는 부분이 없으므로) 단점 조회할 때 항상 조인해서 들고와야함 등록할 때 INSERT를 항상 2번 실행해야함 초반에 모든 것이 예측된 케이스가 아니라면 이 전략을 사용할 일이 별로 없을 것 같다. 객체는 리팩토링을 통해서 공통 부분을 추출해내는 등 자유롭게 변경될 수 있지만, 데이터베이스는 마이그레이션이 필요하기 때문이다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 엔티티 정의**/@Entity@Inheritance(strategy = InheritanceType.JOINED) // 1@DiscriminatorColumn(name = "DTYPE") // 2public abstract class Item&#123; @Id @GeneratedValue private Integer id; private String name; private int price;&#125;@Entity@DiscriminatorValue("A") // 3public class Album extends Item&#123; private String author;&#125;@Entity@DiscriminatorValue("M") // 3@PrimaryKeyJoinColumn(name = "MOVIE_ID")public class Movie extends Item&#123; private String director; private String actor;&#125;/** * 등록, 조회**/public void save()&#123; Album album = new Album(); // set... em.persist(album);&#125;public void select()&#123; Album album = em.find(Album.class, 1);&#125; 사용하는 부분은 별로 다를 것 없다. 상속 매핑을 사용할 것이고, 조인 전략을 사용할 것이라는 의미이다. 자식 테이블을 구분할 컬럼이다. 실제 테이블의 컬럼으로 생성된다. 기본값이 DTYPE 이다. 조인 전략에서는 이 컬럼이 생략 가능하다.(hibernate는 그렇고, 다른 구현체는 아닐수도 있다) 자식으로 직접 접근할때는(e.g. Album 엔티티로 접근) 생략해도 문제가 되지 않는데, 부모로 직접 접근할때는(e.g. 통계를 위해 Item 엔티티로 접근) TYPE이 없으면 해당 데이터가 어느 데이터를 나타내는지 알 수 없다. 그러므로 hibernate는 아래와 같은 전략을 선택한다. 1234567891011121314151617select -- ~~~ case -- 상속된 테이블의 개수만큼 when 반복 when itemlist0_1_.id is not null then 1 when itemlist0_2_.id is not null then 2 when itemlist0_.id is not null then 0 end as clazz_1_ from item itemlist0_ left outer join movie itemlist0_1_ on itemlist0_.id=itemlist0_1_.id left outer join album itemlist0_2_ on itemlist0_.id=itemlist0_2_.id -- 상속된 테이블의 개수만큼 left join 반복 결국 이런식으로 처리되기 때문에, TYPE을 지정해주는 것이 좋다. 구분 컬럼에 저장될 값이다. 생략하면 엔티티 이름을 사용한다. https://stackoverflow.com/questions/3639225/single-table-inheritance-strategy-using-enums-as-discriminator-value @DiscriminatorValue에 enum을 사용하는 방법이라는데, 왜 이렇게 쓰는지 모르겠다. 기본적으로 자식 테이블은 부모 테이블의 ID 컬럼명을 그대로 사용하는데, 이를 바꿔주고 싶을 때 사용한다. 실행결과 123456789101112131415161718-- 삽입INSERT INTO ITEM(id, name, price, DTYPE) VALUES(1, '앨범', 10000, 'A');INSERT INTO ALBUM(id, author) VALUES(1, '소녀시대');INSERT INTO ITEM(id, name, price, DTYPE) VALUES(1, '인셉션', 10000, 'M');INSERT INTO MOVIE(id, director, actor) VALUES(1, '크리스토퍼 놀란', '디카프리오');-- 조회select *from album album0_ inner join item album0_1_ on album0_.id=album0_1_.id where album0_.id = 1 -- and i.DTYPE = 'A' (hibernate에서는 조건절에 따로 DTYPE이 추가되지 않았다) 통합 테이블로 변환(단일 테이블 전략) 전략 이름 그대로 하나의 테이블에 다 때려넣는 전략이다. 저장된 서브 타입마다 사용하지 않는 컬럼들에는 null이 들어가게 된다. 조인 전략과 달리 구분 컬럼은 생략이 불가능하다. 생략하면 기본값이 사용된다. 장점 조인이 필요없다 단점 자식 엔티티가 매핑한 컬럼은 모두 null을 허용해야 한다.(데이터 관점에서 아주 좋지 않음) 단일 테이블에 모든 것을 저장하므로 테이블이 커질 수 있다. 그러므로 오히려 성능이 느려질 수 있다. 123456789101112131415161718192021@Entity@Inheritance(strategy = InheritanceType.SINGLE_TABLE) // 1@DiscriminatorColumn(name = "DTYPE")public abstract class Item&#123; @Id @GeneratedValue private Long id; private String name; private int price;&#125;@Entity@DiscriminatorValue("A")public class Album extends Item&#123;&#125;@Entity@DiscriminatorValue("M")public class Movie extends Item&#123;&#125; 단일 테이블 전략을 사용할 것이라는 의미이다. 실행 결과 1234567891011-- 삽입INSERT INTO ITEM(id, name, price, artist, DTYPE) VALUES(1, '앨범', 10000, '소녀시대', 'A');-- 조회select *from item album0_ where album0_.id=1 and album0_.DTYPE='A' 서브타입 테이블로 변환(구현 클래스마다 테이블 전략) 실제 데이터들을 모두 별도의 테이블에 저장하는 방법이다. 테이블이 모두 별개이다 보니, 공통 부분에 대한 내용이 보장되지도 않고, 관리하기도 힘들다. 쿼리도 전부 UNION으로 날라가서 성능 로스가 극심하다. 데이터베이스 설계자와 객체지향 설계자 둘 다 추천하지 않는 방법이다. 조인이나 단일 테이블 전략을 고려하는 것이 좋다. 1234567891011121314151617181920@Entity@Inheritance(strategy = InheritanceType.TABLE_PER_CLASS) // 1public abstract class Item&#123; @Id @GeneratedValue private Long id; private String name; private int price;&#125;@Entity@DiscriminatorValue("A")public class Album extends Item&#123;&#125;@Entity@DiscriminatorValue("M")public class Movie extends Item&#123;&#125; 구현 클래스마다 테이블 전략을 사용하겠다는 의미이다. 매핑 정보만 상속(@MappedSuperClass) 부모 클래스는 테이블과 매핑하지 않고 부모 클래스를 상속 받는 자식 클래스에게 매핑 정보만 제공하고 싶을 경우 사용한다. 단순히 매핑 정보만 상속할 목적으로 사용한다. 1234567891011121314151617181920212223@MappedSuperClass // 1public abstract class BaseEntity&#123; @Id @GeneratedValue private Long id; @Temporal(TemporalType.TIMESTAMP) private Date createdDate; @Temporal(TemporalType.TIMESTAMP) private Date lastModifiedDate;&#125;@Entityclass Member extends BaseEntity&#123; // ...&#125;@Entity@AttributeOverride(name = "id", column = @Column(name = "TEAM_ID")) // 2class Team extends BaseEntity&#123; // ...&#125; BaseEntity는 테이블과 매핑되지 않고 단순히 자식 엔티티에게 매핑 정보만 제공하는 용도로 사용된다. (참고로 ORM에서 말하는 진정한 상속 매핑은 처음 설명했던 상속 관계 매핑을 말한다.) 매핑 정보만 제공할 클래스라는 의미이다. 매핑정보를 재정의 하고 싶을 경우 사용한다. 여러개를 지정하고 싶을 경우 @AttributeOverrides를 사용한다. 위에 명시하진 않았지만 관계를 재정의 하고 싶을 경우 @AssociationOverride를 사용한다. 근데 이것보다 그냥 @Embeddable을 쓰는게 나을 것 같다. @MappedSuperClass는 추상 클래스만이 가능한데, 다중 상속이 안되는 자바에서 단순히 매핑 정보를 추가 정의하기 위해 상속을 써버리는 것은 좋지 않은 것 같다… 일단 위에는 createdDate, lastModifiedDate로 작성했지만, 이렇게 사용하는게 좋은 예시는 아닌 것 같다. 복합키 매핑 JPA에서 식별자를 둘 이상 사용하려면 별도의 식별자 클래스를 만들어야 한다. 그냥 자바 기본 타입 2개 쓰고 @Id 선언하면 안된다. JPA에서 별도의 식별자 클래스를 만드는 방법은 2가지가 있다. 두 방식의 장단점이 있으니, 원하는 방식을 선택해서 일관성 있게 하나만 사용하는 것이 좋다. @IdClass @IdClass를 이용한 복합키 선언은 아래와 같다. 1234567891011121314151617181920@Entity@IdClass(ParendId.class)public class Parent&#123; @Id @Column(name = "PARENT_ID1") private String id1; @Id @Column(name = "PARENT_ID2") private String id2;&#125;@NoArgsConstructor@AllArgsConstructorpublic class ParentId implements Serializable&#123; private String id1; // Parent.id1 에 대한 정보 제공 private String id2; // Parent.id2 에 대한 정보 제공 // equals, hashCode&#125; @IdClass가 정보 제공용도(식별자 정보는 여기를 참고해라) 정도로 쓰이고 있다. @IdClass로 사용된 식별자 클래스는 아래 조건을 만족해야 한다. 식별자 클래스의 속성명과 엔티티에서 사용하는 식별자의 속성명이 같아야 함 Entity에 매핑 정보를 적고, IdClass에서 해당 변수명에 맞춰 정보를 제공해주고 있다. 아래 식별/비식별 관계에서 복합키 사용 매핑하는 부분에서 더 상세히 볼 수 있다. Serializable 인터페이스 구현해야 함 equals, hashCode 구현해야함 기본 생성자 필요 식별자 클래스는 public 이어야 함 실제 사용은 아래와 같다. 1234567891011121314// savepublic void save()&#123; Parent parent = new Parent(); parent.setId1("id1"); parent.setId2("id2"); em.persist(parent);&#125;// selectpublic void select()&#125;&#123; ParentId parentId = new ParentId("id1", "id2"); Parent foundParent = em.find(Parent.class, parentId);&#125; @IdClass의 장점은 엔티티에 flat한 attribute를 제공해서 그나마 RDB와 가깝다는 것인데, 저장만 그렇지 조회는 또 그렇지도 않다. (저장의 경우 em.persist를 호출하면 JPA가 내부에서 Parent.id1, Parent.id2 값을 이용해서 ParentId를 생성하고 영속성 컨텍스트의 키로 사용한다.) @EmbededId @IdClass보다 좀 더 객체지향적인 방법이다. 1234567891011121314151617@Entitypublic class Parent&#123; @EmbeddedId private ParentId id;&#125;@NoArgsConstructor@AllArgsConstructor@Embeddablepublic class ParentId implements Serializable&#123; @Column(name = "PARENT_ID1") private String id1; @Column(name = "PARENT_ID2") private String id2; // equals, hashCode&#125; (사용하는 쪽에서 @EmbeddedId로 사용하므로 @Id를 사용할 필요없고, 복합키이므로 자동생성을 사용할 수 없다) @IdClass 처럼 정보 제공 용도로 사용하지 않고 직접 엔티티에서 사용해버렸다. 매핑 정보도 ParentId 클래스에 들어감으로써 키를 명확히 하나의 클래스로 분리한 느낌이난다. 좀 더 객체지향적인 방법이다.(id1에 접근하고자 할 경우 entity.getId().getId1()처럼, 언뜻보기에 좀 이상한 접근법이 사용되긴 하지만) @EmbeddedId를 사용한 식별자 클래스는 아래 조건을 만족해야 한다. @Embeddable 어노테이션을 붙여주어야 함 Serializable 인터페이스 구현해야 함 equals, hashCode 구현해야함 기본 생성자 필요 식별자 클래스는 public 이어야 함 실제 사용은 아래와 같다. 1234567891011121314// savepublic void save()&#123; Parent parent = Parent.builder() .id(new ParentId("id1", "id2")) .build(); em.persist(parent);&#125;// selectpublic void select()&#125;&#123; ParentId parentId = new ParentId("id1", "id2"); Parent foundParent = em.find(Parent.class, parentId);&#125; 의문 : @EmbeddedId 사용시 JPQL에서 id.id1 의 형태로 접근해야하는데, delegate 메서드를 활용할 순 없나? 결과 : 엔티티의 대상이 필드이기 때문에 delegate 메서드로는 불가능하다 (대상이 아니기때문) 복합키의 equals, hashCode 위의 복합키 조건을 보면 equals와 hashCode를 필수로 구현해줘야 한다고 하는데, 이는 JPA는 영속성 컨텍스트에 엔티티를 보관할 때 엔티티의 식별자를 키로 사용하고, 식별자를 구분하기 위해 equals와 hashCode를 사용해서 동등성 비교를 하기 때문이다. 이게 단일 식별자일 경우에는 자바의 기본 타입을 사용하므로 별 문제없이 동등성이 보장되지만, 복합 식별자일 경우에는 클래스를 사용하므로 equals와 hashCode를 구현해주지 않으면 동등성을 보장할 수 없다. 1234ParentId id1 = new ParentId("id1", "id2");ParentId id2 = new Parentid("id1", "id2");assertTrue(id1.equas(id2)); // fail 같은 id 값을 가졌지만, 동등하지 않은 것이 된다. java는 equals, hashCode를 오버라이드 하지 않으면 기본적으로 Object의 것을 사용하기 때문이다. 기본적으로 Object의 equals는 동일성 비교(==)를 하기 때문에 위의 두 키는 동등하지 않은 것이 된다. JPA는 엔티티의 식별자를 가지고 영속성 컨텍스트를 관리하기 때문에 식별자의 동등성이 지켜지지 않으면 예상과 다른 엔티티가 조회되거나 엔티티를 찾을 수 없는 등 심각한 문제가 발생할 수 있다. 그러므로 equals와 hashCode는 필수로 구현해줘야 한다. 식별/비식별 관계에서 복합키 사용 식별 관계와 비식별 관계 식별관계 부모 테이블의 기본키를 내려받아서 자식 테이블의 기본키 + 외래키로 사용하는 관계이다. 1234567891011CREATE TABLE parent( parent_id integer, PRIMARY KEY(parent_id))CREATE TABLE child( parent_id integer, child_id integer, PRIMARY KEY(parent_id, child_id), FOREIGN KEY(parent_id) REFERENCES parent(parent_id)) 비식별 관계 부모 테이블의 기본키를 내려받아서 자식 테이블의 외래키로만 사용하는 관계이다. 요즘은 비식별 관계를 주로 사용하고, 필요할 때만 식별 관계를 사용하는 추세이다. 1234567891011CREATE TABLE parent( parent_id integer, PRIMARY KEY(parent_id))CREATE TABLE child( parent_id integer, child_id integer, PRIMARY KEY(child_id), FOREIGN KEY(parent_id) REFERENCES parent(parent_id)) 필수적 비식별 관계 : FK NOT NULL(INNER JOIN 사용됨) 선택적 비식별 관계 : FK NULLALBE(OUTER JOIN 사용됨) 식별 관계 매핑 부모, 자식, 손자까지 계속 기본키를 전달하는 식별관계이다. 식별관계는 부모의 키를 포함해 복합키를 구성해야 하므로 @IdClass나 @EmbeddedId를 사용해야 한다. 1234567891011121314151617181920CREATE TABLE parent( parent_id integer, PRIMARY KEY(parent_id))CREATE TABLE child( parent_id integer, child_id integer, PRIMARY KEY(parent_id, child_id), FOREIGN KEY(parent_id) REFERENCES parent(parent_id))CREATE TABLE grandchild( parent_id integer, child_id integer, grandchild_id integer, PRIMARY KEY(parent_id, child_id, grandchild_id), FOREIGN KEY(parent_id) REFERENCES parent(parent_id), FOREIGN KEY(child_id) REFERENCES child(child_id)) @IdClass 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Entitypublic class Parent&#123; @Id private String parentId;&#125;@Entity@IdClass(ChildId.class)public class Child&#123; // 매핑 정보 나열 @Id @ManyToOne @JoinColumn(name = "parent_id") private Parent parent; @Id private String childId;&#125;@EqualsAndHashCodepublic class ChildId implements Serializable&#123; private String parent; // Child.parent 에 대한 정보 제공 private String childId; // Child.childId 에 대한 정보 제공&#125;@Entity@IdClass(GrandChildId.class)public class GrandChild &#123; @Id @ManyToOne @JoinColumns(&#123; @JoinColumn(name = "parent_id"), @JoinColumn(name = "child_id") &#125;) private Child child; @Id private String grandChildId;&#125;@EqualsAndHashCodepublic class GrandChildId implements Serializable &#123; private ChildId child; // GrandChild.child 에 대한 정보 제공 private String grandChildId; // GrandChild.grandChildId 에 대한 정보 제공&#125; @IdClass가 pk에 매핑되는 애들에게 정보를 바로 제공하고 있다. @EmbeddedId 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Entitypublic class Parent &#123; @Id private String parentId;&#125;@Entitypublic class Child &#123; @EmbeddedId private ChildId childId; @MapsId("parentId") @ManyToOne @JoinColumn(name = "parent_id") private Parent parent;&#125;@EqualsAndHashCode@Embeddablepublic class ChildId implements Serializable &#123; private String parentId; // @MapsId("paretnId") 로 매핑 private String childId;&#125;@Entitypublic class GrandChild &#123; @EmbeddedId private GrandChildId grandChildId; @MapsId("childId") @ManyToOne @JoinColumns(&#123; @JoinColumn(name = "parent_id"), @JoinColumn(name = "child_id") &#125;) private Child child;&#125;@EqualsAndHashCode@Embeddablepublic class GrandChildId implements Serializable &#123; private ChildId childId; // @MapsId("childId") 로 매핑 private String grandChildId;&#125; id들을 따로 묶고 @MapsId를 통해 연관관계와 id를 연결했다. (@IdClass에서 id들을 class로 모으는 과정이 추가된 형태라고 봐도 될듯하다.) @mapsid는 @id로 지정한 컬럼에 @OnetoOne이나 @ManyToOne 관계를 매핑시키는 역할을 한다. http://docs.jboss.org/hibernate/jpa/2.2/api/javax/persistence/MapsId.html 매핑의 대상이 되는 속성은 @OnetoOne이나 @ManyToOne의 기본키와 타입이 같아야한다. ※ 번외로 아래와 같이 세팅 할수도 있는데, 이는 잘못된 방식이다. 12345678910111213141516171819202122232425262728293031323334353637383940@Entitypublic class Parent &#123; @Id private String parentId;&#125;@Entitypublic class Child &#123; @EmbeddedId private ChildId childId;&#125;@EqualsAndHashCode@Embeddablepublic class ChildId implements Serializable &#123; @ManyToOne @JoinColumn(name = "parent_id") private Parent parent; private String childId;&#125;@Entitypublic class GrandChild &#123; @EmbeddedId private GrandChildId grandChildId;&#125;@EqualsAndHashCode@Embeddablepublic class GrandChildId implements Serializable &#123; @ManyToOne @JoinColumns(&#123; @JoinColumn(name = "parent_id"), @JoinColumn(name = "child_id") &#125;) private Child child; private String grandChildId;&#125; 얼핏보면 more 객체지향스럽긴 하지만, 연관관계를 항상 id를 통해 접근하는 이상한 방식이 탄생하게 되고, @Embeddable 에서 연관관계까지 equals, hashCode의 대상이 되는 이상한 구조가 탄생한다. id는 id대로 놔둬야 한다. 일대일 식별 관계(feat.@MapsId) 일대일 식별 관계는 자식 테이블의 기본키 값으로 부모 테이블의 기본키 값을 사용하는 조금 특별한 관계이다. 이 경우 연관관계의 주인이 될 외래키 칼럼이 없으므로 @MapsId를 사용하여 매핑해줘야 한다. 123456789101112131415161718192021222324@Entitypublic class Board&#123; @Id private Long boardId; private String title; @OneToOne(mappedBy = "board") private BoardDetail boardDetail;&#125;@Entitypublic class BoardDetail&#123; @Id private Long boardId; @Lob private String content; @MapsId("boardId") @OneToOne @JoinColumn(name = "board_id") private Board board;&#125; board.getBoardDetail().getContent()로 접근해야해서 사용이 조금 자연스럽지 않게 느껴지지만, @Delegate 같은것으로 충분히 해결할 수 있다. 비식별 관계 매핑 비식별 관계는 복합키를 사용하지 않기 때문에 아주 심플하다. 12345678910111213141516171819202122232425@Entitypublic class Parent &#123; @Id private String parentId;&#125;@Entitypublic class Child &#123; @Id private String childId; @ManyToOne @JoinColumn(name = "parent_id") private Parent parent;&#125;@Entitypublic class GrandChild &#123; @Id private String grandChildId; @ManyToOne @JoinColumn(name = "child_id") private Child child;&#125; 그래서 식별이냐 비식별이냐? 데이터베이스 설계관점에서 보면, 아래와 같은 이유로 비식별 관계를 선호한다. 식별 관계는 부모 테이블의 기본키를 자식 테이블로 전파하면서 자식 테이블의 기본키 컬럼이 점점 늘어나는 구조이다. depth가 깊어질수록 기본키 인덱스가 불필요하게 커지고, 조인할 때 SQL이 복잡해진다. 식별 관계는 2개 이상의 컬럼을 묶어서 복합 기본키를 만들어야 하는 경우가 많다. 복합 기본키는 컬럼이 하나인 단일 기본키보다 작성하는데 많은 노력이 필요하다. 식별 관계의 경우 기본키로 비즈니스 로직이 있는 자연키 컬럼을 조합하는 경우가 많고, 비식별 관계의 경우 기본키로 비즈니스와 전혀 관계없는 대리키를 주로 사용한다. 변하지 않는 요구사항이란 세상에 존재하지 않는다. 자연키 컬럼 조합은 나중에 변경될 가능성이 있다. 이런 상태에서 식별 관계로 구성할 경우 나중에 변경하기 매우 힘들어진다. e.g. 주민등록번호 언급했듯이 비식별 관계의 경우 대리키를 주로 사용하는데, JPA는 @GeneratedValue 처럼 대리키를 생성하기 위한 편리한 방법을 제공한다. 그래서 정리하면! 될수있으면 비식별 관계를 사용하고 기본키는 Long 타입의 대리키를 사용히고 필수적 비식별 관계를 사용하자(optional = false) 조인 테이블 데이터베이스의 테이블의 연관관계를 설정하는 방법은 총 2가지이다. 조인 컬럼 일반적인 외래키 컬럼을 사용하여 연관관계를 관리하는 것 조인 테이블 별도의 테이블을 사용하여 연관관계를 관리하는 것 조인 테이블의 경우 테이블을 하나 추가해야 된다는 단점이 있다.(추가 조인 필요) 그러므로 기본적으로 조인 컬럼을 사용하고, 필요할 때만 조인 테이블을 사용하도록 해야한다. 조인 테이블 == 연결 테이블 == 링크 테이블 하나의 테이블이 여러 테이블과 관계를 맺을 수 있는 구조라던가, 원래 관계가 없었는데 관계가 생겼다거나(FK를 일괄 추가하기에는 너무 부담스럽), 관계 변경(update) 때문에 메인 테이블에 락이 걸리는 걸 방지하기 위해(연결 테이블만 컨트롤 하므로써 성능향상) 사용하는 등 여러 상황에서 사용될 수 있을 것이다. 일대일 조인테이블 1234567891011121314151617181920@Entitypublic class A &#123; @Id private String id; @OneToOne(optional = false) @JoinTable(name = "a_b", joinColumns = @JoinColumn(name = "a_id"), inverseJoinColumns = @JoinColumn(name = "b_id")) private B b;&#125;@Entitypublic class B &#123; @Id private String id; @OneToOne(mappedBy = "b") // optional private A a;&#125; 생성되는 DDL은 아래와 같다. 1234567891011121314151617181920CREATE TABLE A( id varchar(255) not null, primary key (id))create table B ( id varchar(255) not null, primary key (id))create table a_b ( b_id varchar(255) not null, a_id varchar(255) not null, primary key (a_id), FOREIGN KEY(a_id) REFERENCES A(a_id), FOREIGN KEY(b_id) REFERENCES B(b_id))alter table a_b add constraint UK_pam4mvekk45ceoippm3ffvi2t unique (b_id) a_id가 primary key, b_id에 unique constraints가 걸리면서 1:1 관계가 형성된다. 다대일 조인테이블 B를 다로 한다. 1234567891011121314151617181920@Entitypublic class B &#123; @Id private String id; @ManyToOne @JoinTable(name = "a_b", joinColumns = @JoinColumn(name = "a_id"), inverseJoinColumns = @JoinColumn(name = "b_id")) private A a;&#125;@Entitypublic class A &#123; @Id private String id; @OneToMany(mappedBy = "a") // optional private List&lt;B&gt; bList;&#125; 1234567891011121314151617CREATE TABLE A( id varchar(255) not null, primary key (id))create table B ( id varchar(255) not null, primary key (id))CREATE TABLE a_b( a_id varchar(255) not null, b_id varchar(255) not null, PRIMARY KEY(b_id), FOREIGN KEY(a_id) REFERENCES A(a_id), FOREIGN KEY(b_id) REFERENCES B(b_id)) 다 쪽이 primary key로 생성됨으로써 다대일 관계 형성이 가능하다. 일대다 조인테이블 일대다 조인컬럼처럼 일쪽에서 연관관계를 컨트롤 하고 싶을 경우 형성하는 방법이다. 1234567891011121314151617@Entitypublic class A &#123; @Id private String id; @OneToMany @JoinTable(name = "a_b", joinColumns = @JoinColumn(name = "a_id"), inverseJoinColumns = @JoinColumn(name = "b_id")) private List&lt;B&gt; bList;&#125;@Entitypublic class B &#123; @Id private String id;&#125; 일대다 조인컬럼때와 같이 단방향만을 지원한다. 아래는 생성되는 DDL이다. 12345678910111213141516171819CREATE TABLE A( id varchar(255) not null, primary key (id))create table B ( id varchar(255) not null, primary key (id))CREATE TABLE a_b( a_id varchar(255) not null, b_id varchar(255) not null, FOREIGN KEY(a_id) REFERENCES A(a_id), FOREIGN KEY(b_id) REFERENCES B(b_id))alter table a_b add constraint UK_pam4mvekk45ceoippm3ffvi2t unique (b_id) pk 대신 unique로 생성되는게 조금 다르다. 다대다 조인테이블 앞서 나왔으므로 작성하지 않겠다. parent_id, child_id 에 각각 FK가 생성되고, PK로 묶이지는 않는다. 엔티티 하나에 여러 테이블 매핑 아까 위의 일대일 식별 관계에서 나왔었던 형태이다. board와 board_detail을 나눠서 저장하고, 같은 PK를 쓰는 형태 자주 사용하는 형태는 아니지만 가끔 나오기도 한다. 123456789101112@Entity@SecondaryTable(name = "board_detail", // 1 pkJoinColumns = @PrimaryKeyJoinColumn(name = "board_detail_id")) // 2public class Board&#123; @Id private Long boardId; private String title; @Column(table = "board_detail") // 3 private String content;&#125; @SecondaryTable을 사용해 board_detail 테이블을 추가로 매핑했다. 추가로 매핑할 테이블의 이름이다. 추가로 매핑된 테이블의 기본키 컬럼명이다. 추가로 매핑된 테이블에 저장될 속성이다. 1234567891011CREATE TABLE board ( board_id BIGINT NOT NULL, title VARCHAR(255), PRIMARY KEY (board_id)) CREATE TABLE board_detail ( board_detail_id BIGINT NOT NULL, content VARCHAR(255), PRIMARY KEY (board_detail_id)) 위의 일대일 식별관계와 반대로, board.getContent()로 접근할 수 있어 사용은 자연스럽다. 하지만 이 방식의 경우, 매핑이 부자연스럽다.(lazy loading도 안된다) 결과적으로 사용의 자연스러움 보다는 매핑의 자연스러움을 추구하는 것이 맞는것 같다. 그니까… 이거 어디서 쓸일은 없을꺼 같음…ㅋ]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>@MappedSuperClass</tag>
        <tag>@IdClass</tag>
        <tag>@EmbeddedId</tag>
        <tag>@JoinTable</tag>
        <tag>@SecondaryTable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] java mysql 연동시 오류]]></title>
    <url>%2Fjava%2Fjava-mysql-%EC%97%B0%EB%8F%99%EC%8B%9C-%EC%98%A4%EB%A5%98%2F</url>
    <content type="text"><![CDATA[The server time zone value ‘KST’ is unrecognized or represents more than one time zone https://yenaworldblog.wordpress.com/2018/01/24/java-mysql-연동시-발생하는-에러-모음/ public key retrieval is not allowed mysql 8.x 버전 이후로 발생 jdbc url에 allowPublicKeyRetrieval=true&amp;useSSL=false 필요 1jdbc:mysql://localhost:3306/dev-product?useUnicode=true&amp;characterEncoding=utf8&amp;allowPublicKeyRetrieval=true&amp;useSSL=false]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>The server time zone value &#39;KST&#39; is unrecognized or represents more than one time zone</tag>
        <tag>public key retrieval is not allowed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] UTF-8, UTF-16과 java 문자형]]></title>
    <url>%2Fjava%2FUTF-8-UTF-16%EA%B3%BC-java-%EB%AC%B8%EC%9E%90%ED%98%95%2F</url>
    <content type="text"><![CDATA[UTF-8, UTF-16 이란? UTF 뒤에 붙는 숫자의 의미는 유니코드 문자 하나를 표현할 때 사용하는 최소 bit를 의미한다 이게 무슨말이냐 하면은, UTF-8의 경우 최소 1byte로 유니코드 문자를 하나 담을 수 있고, UTF-16의 경우 최소 2byte로 유니코드 문자를 하나 담을 수 있다는 의미이다 이 두 인코딩 방식을 이용해 유니코드에서 기본 다국어 평면에 해당하는 BMP 영역의 문자를 담는다고 생각해보자 BMP의 경우 일반적인 다국어 문자 대부분을 포함하며, 범위는 0000 ~ FFFF 까지이다 https://namu.wiki/w/유니코드 UTF-16의 경우 자신이 가지고 있는 최소 공간에 BMP 영역의 모든 문자를 다 담을 수 있지만, UTF-8의 경우 FF 의 범위가 넘어가는 문자의 경우 자신의 영역을 추가 확장한 뒤 해당 문자를 담아야 한다 이러한 특징으로 봤을때는 무조건 UTF-8이 좋아보인다 00 ~ FF 범위는 1byte를 사용해서 저장할것이고, 100 ~ FFFF 범위는 2byte를 사용해서 저장하면 되기 떄문이다 반면에 UTF-16의 경우 무조건 2byte니 낭비인 것 처럼 보인다 하지만 UTF-8 같은 가변 비트의 경우, 고정된 공간이 아니기 때문에 어디가 문자의 시작인지, 어디가 문자의 끝인지를 표시하는 영역이 추가로 필요하게 된다 이와 같은 이유로 UTF-8은 1byte의 경우 7bit, 2byte는 11bit, 3byte는 16bit 만을 문자저장에 사용할 수 있다 즉, 똑같은 BMP 문자를 저장하더라도 UTF-8은 최대 3byte가 필요하게 되는 것이다 그러므로 처음 인코딩 방식을 선택할때는 이러한 특징을 이용해 인코딩 방식을 선택해야 한다 예를 들어 영문자가 많이 사용되는 시스템에서는 UTF-8을 사용하는 것이 좋고, 다국어가 많이 사용되는 시스템에서는 UTF-16을 사용하는 것이 좋을 것이다 (확실하진 않지만 UTF-16의 경우 문자열 검색에서 더 빠르다고 한다) 웹의 경우 대부분 아스키코드를 사용하므로, UTF-8을 채택했다 그렇다면 java는? java는 인코딩 방식으로 UTF-16을 채택했다 (이유는 모르겠지만 아마도 ascii 문자의 비율이 웹만큼 높지 않아서가 아닐런지… 그리고 문자열 처리 속도도 이유가 될 수 있을 것 같다) 그러므로 java에서 문자를 저장하기 위해 사용되는 char형의 경우, 2byte의 용량을 차지한다 BMP 영역을 넘어가는 문자(emoji 같은)는 char 형에 저장할 수 없다 String 에 char 배열로 저장하는 것 같다 이 과정에서 4byte 를 쓰게 될 것 같다(char 가 2byte 이므로) 이모지가 4 byte 가 필요하다는 것은 utf8 기준인 것 같다 어쩌피 utf16 은 최소용량이 2 byte 이므로 똑같이 4byte 가 쓰일 것 같긴 하지만… 뭐 이 이상 얘기할 것은 없는것 같고, char형과 관련된 몇가지 특징들은 아래와 같다 문자의 경우 결국 다 유니코드 숫자값이기 때문에 숫자로 저장 가능하다 12345678910char a1 = 'a';char a2 = '97';char a3 = '\u0061';/**출력aaa**/ 문자와 숫자를 저장하면 숫자가 문자로 형 변환되어 연산된다 7 + “7” == “77” String의 경우 char의 배열이다]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>char</tag>
        <tag>UTF-8</tag>
        <tag>UTF-16</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 다양한 연관관계 매핑]]></title>
    <url>%2Fjpa%2F%EB%8B%A4%EC%96%91%ED%95%9C-%EC%97%B0%EA%B4%80%EA%B4%80%EA%B3%84-%EB%A7%A4%ED%95%91%2F</url>
    <content type="text"><![CDATA[RDB에는 앞서 언급했던 것 보다 더 많은 관계가 존재한다. 1:N(@OneToMany, @ManyToOne) 1:1(@OneToOne) N:M(@ManyToMany) 이를 다중성이라고 한다. 각각의 다중성에서 형성될 수 있는 연관관계들과 그 특징을 나열해보겠다. 모든 다중성은 왼쪽이 연관관계의 주인이라고 가정하겠다(다대일 -&gt; 다가 연관관계의 주인) 다대일 N:1의 관계이고, N이 연관관계의 주인인(외래키를 관리하는) 형태이다. RDB에서 외래키는 항상 N쪽에 존재한다는 특성에 가장 잘 들어맞는다. 그러므로 대부분 이 형태를 사용한다. 다대일 단방향(N:1) 12345678910111213141516171819@Entityclass Member&#123; @Id private String id; private String username; @ManyToOne @JoinColumn(name = "TEAM_ID") private Team team;&#125;@Entityclass Team&#123; @Id private String id; private String name;&#125; Member.team 필드로 TEAM 테이블의 TEAM_ID 외래키를 관리한다. Member는 Team을 참조할 수 있지만 Team은 Member를 참조할 수 없다. 이 관계는 선택이 아닌 필수이다. 다대일 양방향(N:1, 1:N) 1234567891011121314151617181920212223242526@Entityclass Member&#123; @Id private String id; private String name; @ManyToOne @JoinColumn(name = "TEAM_ID") private Team team; // 연관관계 편의 메서드&#125;@Entityclass Team&#123; @Id private String id; private String name; @OneToMany(mappedBy = "team") private List&lt;Member&gt; members = new ArrayList&lt;&gt;(); // 연관관계 편의 메서드&#125; 참조가 양쪽 모두 있으므로 연관관계의 주인을 정해야 한다. RDB에서 1:N, N:1 관계에서 외래키는 항상 N쪽에 있으므로 Member가 연관관계의 주인이 된다. 보다시피 mappedBy 속성으로 연관관계의 주인이 아님을 명시해주고 있다. 아주 일반적인 구조이다. 양방향 연관관계는 항상 서로 참조해야 하므로, 각각 연관관계 편의 메서드를 작성해주는 것이 좋다. RDB에서는 외래키 하나만 넣어줘도 양방향 관계가 성립하지만, 객체에서는 그렇지 않기 때문이다. 이 관계는 편의를 위한 선택이다. 일대다 1:N의 관계이고, 1이 연관관계의 주인인 형태이다. 외래키는 당연히 N쪽 테이블에 있지만, 관리를 1쪽에서 하므로 관리에 불편함이 있다. 일반적으로 잘 쓰이지는 않는 방법이다. 일대다 단방향(1:N) 일대다 단방향 관계는 JPA 2.0부터 지원한다. 12345678910111213141516171819@Entityclass Team&#123; @Id private String id; private String name; @OneToMany @JoinColumn(name = "TEAM_ID") private List&lt;Member&gt; members = new ArrayList&lt;&gt;();&#125;@Entityclass Member&#123; @Id private String id; private String username;&#125; 외래키는 N쪽 테이블에 있으나, N쪽 엔티티에 외래키를 매핑할 수 있는 참조 필드가 없고, 1쪽에만 참조필드가 있다. 단방향 관계에서는 참조를 가진쪽이 외래키를 관리한다. 즉, 이 상태에서는 1쪽에서 외래키를 관리하는, 조금 특이한(다소 불편한) 형태가 나오게 된다. 외래키를 관리해야하므로 보다시피 @JoinColumn을 꼭 명시해줘야 한다. 이를 명시해주지 않으면 JPA는 @JoinTable 전략을 기본으로 사용해버린다. 이렇게 하면 어떤점이 불편할까? 기본적인 RDB의 1:N 구조에 역행하는 방식이므로 관리가 불편하다. N 엔티티가 직접 외래키를 컨트롤 할 수 없으므로, 외래키를 변경하려면 무조건 Team을 통해야 한다. 성능상 문제도 발생한다. N 저장 시 연관관계를 설정하고 저장하는 것이 불가능하다. insert 후 update로 연관관계를 설정해줘야 한다. 쿼리가 2번 필요하다. 아래는 1:N 단방향 형태에서 연관관계를 설정하는 예시이다. 12345678910public void save()&#123; Member member = new Member("member1"); // 외래키는 member가 가지지만 외래키를 설정할 수 없는 상황 em.persist(member); Team team = new Team("team1"); team.getMembers().add(member); // 1이 연관관계의 주인이라 외래키 컨트롤 가능 em.persist(team);&#125; 실행되는 쿼리는 아래와 같다. 1234insert into Member (MEMBER_ID, username) values (null, ?)insert into Team (TEAM_ID, name) values (null, ?)update Member set TEAM_ID=? where MEMBER_ID=? 보다시피 불필요한 UPDATE 쿼리가 발생한다. Member는 Team의 존재를 모르기 때문에 바로 외래키를 설정할 수 없기 때문이다. 보면 알겠지만 사용될 일이 많이 없는 형태이다. 항상 부모를 통해 접근하고, 자식이 직접 부모를 참조할 일이 없는 구조의 경우 가끔씩 사용하기도 한다. (이럴 경우라도 위처럼 1쪽에서 의존관계를 관리할 경우는 거의 없다. 대부분 cascade 전략으로 처리한다) 하지만 위와 같지 않고 일반적인 상황이라면 이 구조보다는 다대일 양방향 매핑을 권장한다. 일대다 양방향 일대다 양방향 매핑은 존재하지 않는다. 양방향 연관관계를 형성하게 되면 N쪽 테이블에 @ManyToOne을 명시하게 되는데, 테이블 상에서 외래키를 가진 애가 객체상에서도 참조를 컨트롤 할수 있게 된 상황에서 굳이 1쪽에서 연관관계를 컨트롤 하도록 할 이유가 없다. (기능이란건 결국 필요에 의해 만들어지는데, 이 기능은 굳이 지원할 이유가 전혀 없다) 그래서 @ManyToOne는 mappedBy 속성 자체가 없다. 근데 뭐… 완전히 불가능한 것은 아니고, 설정할 수는 있다. 기본적으로 일대다 단방향으로 설정하고, N쪽의 단방향 매핑을 읽기전용으로 설정하면 된다. 1234567891011@Entitypublic class Member &#123; @Id private Long id; private String name; @ManyToOne @JoinColumn(name = "TEAM_ID", insertable = false, updatable = false) private Team team;&#125; 이렇게 까지 사용할 일이 있을라나 모르겠다. 일대일(1:1) 양쪽이 서로 하나의 관계만을 가지는 형태이다. 사람과 사물함의 관계와 같다고 보면 된다. 1:1 구조의 특징은 주 테이블, 대상 테이블 중 어느 곳이던 외래키를 가질 수 있다는 것이다. 즉 일대일 관계에서는 누가 외래키를 가질지 선택해야 한다. 주 테이블에 외래키 객체지향 개발자들이 선호하는 방법이다. 외래키를 객체 참조 비슷하게 사용 할 수 있고, 주 테이블만 확인해도 대상 테이블과의 연관관계를 확인 가능하다. 대상 테이블에 외래키 데이터베이스 개발자들이 선호하는 방법이다. 관계를 일대일에서 일대다로 변경할 떄 테이블 구조를 그대로 유지할 수 있는 장점이 있다. 이제 해당 방식에 대해 객체를 매핑할건데, 결론부터 얘기하자면 JPA는 대상 테이블에 외래키 방식을 지원하지 않는다. 그러므로 연관관계의 주인을 바꿔서 사용하는 방법밖에 없다. 왜 지원하지 않을까 생각해봤는데… 모호한 부분이 많은 듯 하다. 일단 문법적으로, 이런 모양으로 매핑할 수 있는 방법이 없다. 일대다 관계처럼 객체-컬렉션 형태로 구분지어지지도 않기 때문이다. 결국 문법적으로 구분할 수 있는 방법은 mappedBy로 명시해주는 방법밖에 없는데, 이럴려면 무조건 양방향 매핑을 사용해야 하고, 이렇게 해서 대상테이블에 외래키를 구현한다고 해도, 반대편 엔티티에서 보면 어쩌피 또 주 테이블에 외래키 전략이 된다. 나는 … 이러한 이유로 주 테이블에 외래키 전략을 더 선호한다. 너무 RDB의 형태에 갇혀있을 필요는 없는 것 같다. 일대일 단방향(주테이블에 외래키) 123456789101112131415161718@Entitypublic class Member &#123; @Id private Long id; private String name; @OneToOne @JoinColumn(name = "LOCKER_ID") private Locker locker;&#125;@Entitypublic class Locker &#123; @Id private Long id; private String naame;&#125; 양방향(주테이블에 외래키) 1234567891011121314151617181920@Entitypublic class Member &#123; @Id private Long id; private String username; @OneToOne @JoinColumn(name = "LOCKER_ID") private Locker locker;&#125;@Entitypublic class Locker &#123; @Id private Long id; private String naame; @OneToOne(mappedBy = "locker") private Member member;&#125; 나는 객체지향 관점에서 봤을떄, 주테이블에 외래키를 가지는것이 좀 더 객체지향스럽다고 생각하고, 이 방식을 좀 더 선호한다(!!) 사실상 대상 테이블에 외래키를 가지는 것은 RDB 스러운 방법이라고 생각한다. 주 테이블에 외래키를 가지는 식으로 설계하면 주 테이블에서 관계의 개수만큼 외래키를 관리해줘야 하기 때문이다. 근데… 어플리케이션을 만드는 개발자가, 특히 객체지향을 사용하는 개발자가 이런 관점에 굳이 얽매여있을 필요가 있을까? 객체지향의 장점을 끌어올리고자 ORM을 사용하는 입장에서, 그러한 설계에 얽매이고, 굳이 연관관계를 뒤집어 가며 개발해야 할 이유가 있나 생각이 든다… 게다가, 나중에 나오곘지만 ORM에서는 프록시의 한계 때문에 연관관계의 주인이 아닌쪽에서의 lazy 로딩을 허용하지 않는다. 이 말인 즉, 대상 테이블에 외래키를 사용하는 형태로 양방향 관계를 형성하면, 주 테이블 쪽에서는 조회될 때 마다 자신과 연관된 모든 관계를 다 가져와야 한다는 의미가 된다. (물론 해결 방법은 있다. byte instrument…) 다대다(N:M) RDB는 다대다 관계를 표현할 수 없다. 그러므로 연결 테이블이라는 것을 사용해야 한다. 회원과 상품은 바로 N:M 관계를 맺을 수 없으니 중간에 주문 같은 테이블을 넣어줘야 N:M 관계를 형성할 수 있다. 반면에 객체는 다대다 관계를 표현할 수 있다. 컬렉션을 사용해서 서로 참조하고 있기만 하면된다. 다대다 관계는 연결 테이블 여부라는 패러다임 차이가 있기 때문에, 이를 풀어줘야 한다. 그러기에 기존의 방식인 다중성 표현 + 외래키 지정으로는 위의 패러다임 차이를 풀 수 없다. 그래서 JPA는 @JoinTable이라는 전략을 사용해 이를 지원한다. 단방향 1234567891011121314151617181920212223// 다대다 단방향 회원@Entitypublic class Member &#123; @Id private String id; private String username; @ManyToMany @JoinTable( name = "MEMBER_PRODUCT", joinColumns = @JoinColumn(name = "MEMBER_ID"), inverseJoinColumns = @JoinColumn(name = "PRODUCT_ID")) private List&lt;Product&gt; products = new ArrayList&lt;&gt;();&#125;@Entitypublic class Product &#123; @Id private String id; private String name;&#125; 다대다 관계이므로 @ManyToMany를 사용하였고, 위에서 언급한 @JoinTable이 등장하였다. @ManyToMany는 별다른거 없고(진짜 그런건 아니지만), @JoinTable의 속성 대해 알아보자. name : 연결 테이블을 지정한다 joinColumns : 현재 방향에서 매핑할 조인 컬럼 정보 inverseJoinColumns : 반대 방향에서 매핑할 조인 컬럼 정보 이 정보들을 기반으로 연결 테이블을 생성한다. 이로 인해 우리는 연결 테이블을 전혀 신경쓰지 않아도 된다! 아래는 저장하는 코드다. 12345678910111213public void save() &#123; Product product = new Product(); product.setId("productA"); product.setName("상품A"); em.persist(productA); Member member = new Member(); member.setId("member1"); member.setUsername("회원1"); member.getProducts().add(ProductA); // 연관관계 설정 em.persist(member1);&#125; 123INSERT INTO PRODUCT ...INSERT INTO MEMBER ...INSERT INTO MEMBER_PRODUCT ... 연결 테이블에 데이터가 저장된다. 아래는 탐색하는 코드이다. 12345678public void find() &#123; Member member = em.find(Member.class, "member1"); List&lt;Product&gt; products = member.getProducts(); // 객체 그래프 탐색 for(Product product : products) &#123; System.out.println("product.name = " + product.getName()); &#125;&#125; 1234SELECT * FROM MEMBER_PRODUCT MP INNER JOIN PRODUCT P ON MP.PRODUCT_ID=P.PRODUCT_IDWHERE MP.MEMBER_ID=? 연결 테이블과 조인해서 데이터를 들고온다. 양방향 다대다의 반대 또한 다대다이므로, @ManyToMany로 연결해주면 된다. 12345678@Entitypublic class Product &#123; @Id private String id; @ManyToMany(mappedBy = "products") private List&lt;Member&gt; members;&#125; mappedBy로 연관관계의 주인만 지정해주면 된다. 사실상 연결 테이블로 관리되는 다대다 관계에서는 연관관계의 주인이 별로 의미가 없다… 물론 위의 상황에서는 Member만이 연관관계를 컨트롤할 수 있지만, 연관관계 편의 메서드만 추가해줘도 양쪽에서 컨트롤 할 수 있게 된다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Entitypublic class Member &#123; @Id private String id; private String username; @ManyToMany @JoinTable(name = "MEMBER_PRODUCT" ,joinColumns = @JoinColumn(name = "MEMBER_ID") ,inverseJoinColumns = @JoinColumn(name = "PRODUCT_ID")) private List&lt;Product&gt; products = new ArrayList&lt;&gt;(); // 연관관계 편의 메서드 public void addProduct(Product product)&#123; if(!this.products.contains(product))&#123; this.products.add(product); &#125; if(!product.getMembers().contains(this))&#123; product.getMembers().add(this); &#125; &#125;&#125;@Entitypublic class Product &#123; @Id private String id; private String name; @ManyToMany(mappedBy = "products") private List&lt;Member&gt; members; // 연관관계 편의 메서드 public void addMember(Member member)&#123; if(!this.members.contains(member))&#123; this.members.add(member); &#125; if(!member.getProducts().contains(this))&#123; member.getProducts().add(this); &#125; &#125;&#125; 연관관계의 주인이 아닌쪽에서 편의메서드를 사용하면 결국 연관관계의 주인쪽에도 추가되므로, 연결 테이블이 영향을 받게 된다. 즉, 편의메서드를 통하면 양쪽에서 다 컨트롤 가능하다. 다대다에서는 연관관계 편의메서드를 작성하지 않는것이 좋아보인다. side effect가 많다. 다대다의 한계 @ManyToMany를 사용하면 연결 테이블을 알아서 관리해주므로 여러모로 편리하지만, 실제 실무에서는 이 정도로만 사용하기에는 한계가 있다. MEMBER_ID와 PRODUCT_ID만 담지 않고, 추가적인 정보를 담는 경우가 많기 떄문이다. (날짜, 수량등을 추가해서 ORDER 테이블로 사용한다거나…) 하지만 이렇게 컬럼을 추가하면 더이상 @ManyToMany를 사용할 수 없게된다. 추가 컬럼을 정의한 연결 테이블에 매핑되는 엔티티를 만들어야하고, 테이블간의 관계도 다대다에서 일대다, 다대일의 관계로 풀어야한다. 123456789101112131415161718192021222324252627282930313233343536373839@Entitypublic class Member &#123; @Id private String id; private String username; @OneToMany(mappedBy = "member") private List&lt;MemberProduct&gt; memberProducts = new ArrayList&lt;&gt;();&#125;@Entitypublic class Product &#123; @Id private String id; private String name; // 여기도 필요에 따라 추가할 수 있다&#125;@Entity@IdClasspublic class MemberProduct&#123; @Id @ManyToOne @JoinColumn(name = "MEMBER_ID") private Member member; @Id @ManyToOne @JoinColumn(name = "PRODUCT_ID") private Product product; private Integer orderAmount; @Temporal(TemporalType.TIMESTAMP) private Date orderDate;&#125; (현재 @IdClass라는 것을 사용해서 복합키를 매핑하였는데, 이는 뒷부분에서 다룬다.) 추가적인 컬럼을 가진 MemberProduct를 정의하였다. 외래키를 직접 관리하므로 이 엔티티가 연관관계의 주인이 된다. @JoinColumn을 선언했음을 볼 수 있다. Member 엔티티는 외래키를 관리하지 않으므로 mappedBy 속성을 줘서 연관관계의 주인이 아님을 명시했다. Product 엔티티에서 직접 MemberProduct를 참조할 일이 없다고 판단해서 연관관계를 추가하지 않았다. 실무(아니 그냥 일반적으로)에서는 위와 같은 방식으로 더 많이 사용된다. 사용하는 방식은 일반적인 다대일, 일대다 관계와 같다.]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>@JoinTable</tag>
        <tag>@OneToMany</tag>
        <tag>@ManyToOne</tag>
        <tag>@OneToOne</tag>
        <tag>@ManyToMany</tag>
        <tag>@JoinColumn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 양방향 연관관계에서 서로간 컨트롤 할수있는 범위]]></title>
    <url>%2Fjpa%2F%EC%96%91%EB%B0%A9%ED%96%A5-%EC%97%B0%EA%B4%80%EA%B4%80%EA%B3%84%EC%97%90%EC%84%9C-%EC%84%9C%EB%A1%9C%EA%B0%84-%EC%BB%A8%ED%8A%B8%EB%A1%A4-%ED%95%A0%EC%88%98%EC%9E%88%EB%8A%94-%EB%B2%94%EC%9C%84%2F</url>
    <content type="text"><![CDATA[전제 12345678910111213141516171819202122232425262728293031@Entityclass Member&#123; @Id @GeneratedValue @Column("id") private Integer id; @Column("name") private String name; @Column("age") private String age; @ManyToOne(optional = false) @JoinColumn(name = "team_id") private Team team;&#125;@Entityclass Team&#123; @Id @GeneratedValue @Column("id") private Integer id; @Column("name") private String name; @OneToMany(mappedBy = "team") private List&lt;Member&gt; members;&#125; 기본기능 Member 외래키 컨트롤 가능 Team 업데이트 가능 Team 외래키 컨트롤 불가능 자신에게 속한 모든 Member들 update 가능 연관관계 편의메서드를 추가함으로써 Member에서도 외래키 컨트롤 가능 remove 연관관계 편의 메서드 + 삭제 기능 을 제공하는 orphanRemoval이 있음 일반적인 연관관계 편의메서드는 delete 까지 수행하진 않고, FK를 null 처리함]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>orphanRemoval</tag>
        <tag>양방향 연관관계</tag>
        <tag>연관관계 편의 메서드</tag>
        <tag>cascade</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] enum 정의 및 활용예제]]></title>
    <url>%2Fjava%2Fenum-%EC%A0%95%EC%9D%98-%EB%B0%8F-%ED%99%9C%EC%9A%A9%EC%98%88%EC%A0%9C%2F</url>
    <content type="text"><![CDATA[enum 정의 http://www.nextree.co.kr/p11686/ 상수 -&gt; interface 내 상수 -&gt; 클래스 -&gt; enum enum 활용사례 https://jojoldu.tistory.com/137]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>enum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] 파티셔닝, 샤딩]]></title>
    <url>%2Fdb%2F%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9D-%EC%83%A4%EB%94%A9%2F</url>
    <content type="text"><![CDATA[파티셔닝 : https://nesoy.github.io/articles/2018-02/Database-Partitioning 샤딩 : https://nesoy.github.io/articles/2018-05/Database-Shard 테이블을 나누는 과정 자체를 파티셔닝이라고 한다 Vertical Partitioning 정규화와는 달리 이미 정규화된 테이블을 세로로 자른다 Horizontal Pratitioning 이를 다른말로 샤딩이라고 한다]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>partitioning</tag>
        <tag>sharding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[process concept]]></title>
    <url>%2Fos%2Fprocess-concept%2F</url>
    <content type="text"><![CDATA[복잡한 컴퓨터 시스템에서 가장 중요한것 == abstraction, decompositon descomposition : 복잡한 문제를 단순한 여러개의 문제로 나누는 방법론 process == program in execution(수행중인 프로그램) OS == 정부. 시민들을 관리한다(법을 만들어 시민들을 통제하는 등) 프로세스 == 시민, 수행의 주체, 자원할당의 주체 프로세스는 OS위에서 프로그램을 실행시키는 주체 OS의 입장에서 가장 중요한 단위 decomposition의 한 유닛이 프로세스이다. 각각의 쪼개진 조각들을 하나하나 실행할 수 있다면 편리하다. decomposition 유닛들이 궁극적으로 수행의 단위까지 된 것 == 프로세스 Program과 Process의 차이점 Program 스토리지만 점유한다. 수동적인 존재이다. Process CPU, memory, IO device 등등을 점유한다. 능동적인 존재이다. Process State 프로그램이 수행되는데 필요한 정보, 수행의 결과로 영향을 받을 수 있는 정보들 Memory Context code segment : 어셈블리어 data segment : 프로그램의 전역변수들 stack segment : 프로그램의 지역변수, 매개변수들 heap segment : 동적할당 Hardware Context cpu register I/O register System Context OS가 여러개의 process들을 관리해야 하다보니 각 프로세스들의 정보들을 저장해둔다. process table open file table page table Executon Stream 프로세스가 수행한 모든 명령어들의 순서(sequence) Multi programming, Multi Processing Multi Programming 메모리의 입장이다. 메인 메모리에 액티브한 프로세스가 여러개 올라와있는 상태를 말한다. 옛날에는 메모리의 용량이 작았기때문에 메인 메모리에 현재 실행되는 프로세스만 올라가고, 사용하지 않는 프로세스는 다른 저장 장치로 내보내는 식으로 멀티 프로그래밍을 구현하였다. 이를 swapping이라고 한다. 요즘은 메모리의 용량이 크므로 swapping 하지는 않는다. Multi Processing Multi Programming을 CPU의 관점에서 바라본 것이다. CPU는 하나의 명령만 수행가능하므로 계속 스위칭 하면서 여러 프로세스들을 실행한다. Sw system 개발 설계 요구사항 명세서 -&gt; 설계(decomposition) -&gt; tasks 구현 tasks -&gt; 구현 -&gt; program 이 program이 OS에 의해 process가 된다.]]></content>
      <categories>
        <category>os</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 연관관계 매핑 기초]]></title>
    <url>%2Fjpa%2F%EC%97%B0%EA%B4%80%EA%B4%80%EA%B3%84-%EB%A7%A4%ED%95%91-%EA%B8%B0%EC%B4%88%2F</url>
    <content type="text"><![CDATA[태아불(엔티티)이 서로 연관관계를 가질 때 드러나는 JPA와 SQL 패러다임 차이가 있다. 바로 조인과 참조이다. SQL은 외래키라는 것을 통해 테이블끼리 관계를 가지고, 조인이라는 것을 통해 두 테이블의 모든 데이터에 접근 가능하다. SQL의 경우 외래키만 있으면 어느쪽에서든 조회가 가능하다. 기본적으로 양방향이다. 하지만 객체에서는 이런 행위가 불가능하다. 엔티티간의 관계는 참조를 통해 형성된다. 클래스의 필드로 다른 클래스를 가지고 있어야하며, 한쪽으로만 접근, 즉 단방향 탐색만 가능하다. 객체 연관관계 vs 테이블 연관관계 객체 1234567891011121314151617181920212223242526272829@Setter@Getterclass Mamber&#123; private String id; private String username; private Team team;&#125;@Setter@Getterclass Team&#123; private String id; private String name;&#125;public void save()&#123; // 연관관계 세팅 Team team = new Team("team1", "어벤져스"); Member member1 = new Member("member1", "멤버1"); Member member2 = new Member("member2", "멤버2"); member1.setTeam(team); member2.setTeam(team); // 연관관계 탐색 Team foundTeam = member1.getTeam(); assertThat(foundTeam.getName(), is("어벤져스"));&#125; 위처럼 참조를 통해 연관관계를 탐색하는 것을 객체 그래프 탐색 이라고 한다. 테이블 1234SELECT T.*FROM MEMBER M INNER JOIN TEAM T ON M.TEAM_ID = T.IDWHERE M.MEMBER_ID = 'member1' 위처럼 외래키를 통해 연관관계를 탐색하는 것을 조인 이라고 한다. 위와같은 패러다임을 풀기위해 나온것이 방향이라는 개념이고, 여기서 단방향, 양방향의 개념이 나온다. 단방향 연관관계 우리는 ORM을 사용중이다. 위의 객체연관 관계를 그대로 활용하되, JPA에게 알려주기만 하면 된다. 12345678910@Setter@Entityclass Member&#123; private String id; private String username; @ManyToOne @JoinColumn(name = "TEAM_ID") private Team team;&#125; 단방향이므로 Team 쪽에 따로 해줄것은 없다. @ManyToOne N:1의 관계라는 것을 나타내주는 어노테이션이다. Teamp 하나에 Member 여러개가 소속될 수 있기 때문이다. 사용할 수 있는 옵션은 아래와 같다. 속성 기능 기본값 optional FK nullable 한지의 여부이다 true referencedColumnName 외래 키가 참조하는 대상 테이블의 컬럼명 참조하는 테이블의 기본키 컬럼명 fetch lazy로딩, eager 로딩을 설정할 수 있다. @ManyToOne = FetchType.EAGER, @OneToMany = FetchType.LAZY optonal 속성에 따른 쿼리 방식 이 값이 true일 경우 JPA는 N쪽 테이블을 조회해온 후, fk 값에 따라 조회를 1쪽 테이블을 추가로 조회하거나(null일 경우 조회하지 않음) LEFT OUTER JOIN을 사용한다. 무작정 INNER JOIN을 하면 fk가 null일 경우 출력되지 않을 것이므로, 당연한 결과다. 반대로 false로 설정하면 바로 INNER JOIN으로 처리한다. 테이블 설계를 FK NOT NULL로 해도 이 속성값이 true일 경우 LEFT OUTER JOIN 등으로 처리하므로, FK NOT NULL일 경우에는 false로 주는 것이 좋다. @JoinColumn 관계에 사용되는 외래키를 작성하는 부분이다. (이 외래키(TEAM_ID)에 해당하는 엔티티는 이것(Team)이다 라고 보면 편하다) 결과적으로 객체를 RDB와 매핑할 것이기 때문에, 이렇게 하나라도 더 알려줘야 탐색의 시간을 줄일 수 있다.(리플렉션으로 객체의 모든 값을 탐색하며 관계를 알아내기에는 너무 낭비이기 떄문에) 사용할 수 있는 옵션은 아래와 같다. 속성 기능 기본값 name 매핑할 외래키 이름 필드명 + _ + 참조하는 테이블의 기본 키 컬럼명 referencedColumnName 외래키가 참조하는 대상 테이블의 컬럼명 참조하는 테이블의 기본키 컬럼명 foreignKey(DDL) 외래키 제약조건 설정 가능 연관관계 사용 저장 12345678910111213public void save()&#123; Team team = new Team("team1", "어벤져스"); em.persist(team); Member member1 = new Member("member1", "멤버1"); member1.setTeam(team); Member member2 = new Member("member2", "멤버2"); member2.setTeam(team); em.persist(member1); em.persist(member2);&#125; 객체간에 관계를 맺고 persist를 땋! 때려주면 1234INSERT INTO TEAM VALUES("team1", "어벤져스");INSERT INTO MEMBER VALUES("member1", "멤버1", "team1");INSERT INTO MEMBER VALUES("member2", "멤버2", "team1"); 처럼 team의 id값이 member의 외래키 값으로 세팅되어 저장된다. 엔티티 저장 시 연관된 모든 엔티티는 영속 상태여야 한다. 존재하는 엔티티라는 것이 보장되어야 하기 때문이다. 이러한 특징 때문에 비효율적이라고 생각할 수 있다. 외부에서 명확한 identity가 넘어왔음에도 불구하고, find로 조회해서 영속성 컨텍스트에 넣어줘야 하기 때문이다. 사실상 외부에서 명확한 identity가 넘어왔음에도 불구하고는 우리의 입장이지, framework는 그것을 모른다. 그러므로 고집을 부릴수는 없는 노릇… 조금 다른 방식으로 풀어볼 수는 있다(em.getReference) 조회 123456public void find()&#123; Member memver = em.find(Member.class, "member1"); Team team = member.getTeam(); assertThat(team.getName(), is("어벤져스");&#125; 객체 그래프 탐색으로 매우 간단하게 찾아갈 수 있다. (또는 JPQL로도 조회 가능하다) 123456-- optional = falseSELECT M.* FROM MEMBER M INNER JOIN TEAM T ON M.TEAM_ID = T.IDWHERE M.ID = "member1"; 수정 123456public void update()&#123; Team team = em.find(Team.class, "team2"); Member givenMember = em.find(Member.class, "member1"); givenMember.setTeam(team);&#125; 변경감지가 동일하게 동작하여 update문이 발생하게 된다. 12345UPDATE MEMBERSET TEAM_ID = 'team2', ...WHERE ID = 'member1' 연관관계 제거 1234public void remove()&#123; Member givenMember = em.find(Member.class, "member1"); givenMember.setTeam(null);&#125; 위처럼 null로 세팅해 연관관계를 제거해줄 수도 있다. fk인 team_id가 null로 세팅된다. 12345UPDATE MEMBERSET TEAM_ID = null, ...WHERE ID = 'member1' 삭제 123456public void save()&#123; member1.setTeam(null); member2.setTeam(null); em.remove(team);&#125; 연관관계를 제거해주지 않고 삭제할 경우 외래키 제약조건에 걸리므로, 관계 제거를 선행해줘야 한다. 양방향 연관관계 현재는 Member -&gt; Team의 관계만 형성되어있는데(객체지향 관점에서) Team -&gt; Member의 관계까지 추가하면 양방향 연관관계가 성립된다. Member -&gt; Team이 N:1 관계였으므로, Team -&gt; Member는 1:N의 관계를 가진다. 관계는 반대편 관계에 달려있다. 반대편이 1:N 관계일 경우 N:1, 1:1일 경우 1:1 관계를 가진다. 자바에서 1:N의 관계를 표현하려면 배열을 사용해야하는데, JPA에서는 여기서 Collection을 사용한다.(List, Set, Map 등) 1234567891011121314151617@Setter@Entityclass Team&#123; private String id; private String name; @OneToMany(mappedBy = "team") private List&lt;Member&gt; memberList;&#125;public void find()&#123; Team givenTeam = em.find(Team.class, "team"); List&lt;Member&gt; givenMembers = givenTeam.getMembers(); // do something...&#125; mappedBy 속성은 양방향 매핑일 떄 사용하는데 반대쪽 매핑의 필드 이름을 값으로 주면 된다. 연관관계의 주인 SQL의 경우 기본적으로 양방향 연관관계를 가지지만, 위에서도 언급했지만, SQL의 경우 외래키 하나로 양방향 연관관계가 형성된다 객체지향의 경우 양방향 연관관계라는 것이 애초에 없다. 단방향 연관관계 2개를 로직으로 잘 묶어서 양방향 연관관계처럼 보이게 하는 것 일 뿐이다. 근데 이렇게 양방향으로 연관관계를 형성해주면 결과적으로 연관관계를 컨트롤 해줄 수 있는 곳이 2군데가 생기게 된다. 하지만 SQL의 경우 언급했다시피, 연관관계를 컨트롤 하는 곳은 단 한군데(외래키)이다. 즉 이 객체들이 SQL로 매핑되려면, 누가 연관관계를 컨트롤하는지 알려줘야 하고, 컨트롤하는 주체를 연관관계의 주인 이라고 하는 것이다. 말이 거창하지만, 그냥 외래키 관리자를 말하는 것이다. 일반적인 상황에서는 그냥 테이블상에서 외래키를 갖고있는 엔티티가 연관관계의 주인이 된다. 연관관계의 주인은 양방향 연관관계를 가졌을떄만 지정해주면 된다. 단방향으로 지정했을 경우에는 ORM 입장에서 혼동스러울 부분이 없기 때문이다. 아까 위에서 양방향 연관관계를 맺으면서 mappedBy 속성을 사용했는데, 이 속성이 곧 연관관계의 주인을 알려주는 속성이다. mappedBy 속성 지정에는 아래와 같은 룰이 존재하는데, 이를 보면 용도를 알 수 있다. 주인은 mappedBy 속성을 사용하지 않는다 주인이 아니면 mappedBy 속성을 사용해서 연관관계의 주인을 지정해야 한다. 결국 위에서도 mappedBy 속성을 사용함으로써 내가 연관관계의 주인이 아니라고 알려주는 것이다. ORM 입장에서는 @OneToMay에서 참조하는 클래스를 탐색한 뒤, mappedBy에 명시된 필드를 찾아가 외래키 정보를 얻을 것이다(아마도) 1:N 관계에서 외래키를 관리하는 쪽은 N 쪽이기 때문이다. (그래서 @ManyToOne에 mappedBy 속성이 없다) ORM 입장에서도 이 개념이 중요하게 작용하는게, 연관관계의 주인만이 연관관계와 매핑되는 외래키를 관리(등록, 수정, 삭제)할 수 있고, 주인이 아닌 쪽은 읽기만 가능하게 된다. Team의 @OneToMany에 있는 members의 원소들을 백날 더하고 빼고 해봤자 아무일도 일어나지 않는다(ㅋㅋ) 1234567891011public void owner()&#123; Team team = em.find(Team.class, "team1"); Member member1 = em.find(Member.class, "member1"); Member member2 = em.find(Member.class, "member1"); team.getMembers().add(member1); // 무시됨 team.getMembers().add(member2); // 무시됨 member.setTeam(team); // 설정됨 &#125; 연관관계의 주인만이 연관관계(외래키)를 컨트롤 할 수 있다는걸 명심하자. 연관관계 편의 메서드 사실상 위의 행위는 객체지향 관점에서 보면 좀 문제가 있다. 1234567891011121314151617181920public void save&#123; Team team = new Team("team1", "어벤져스"); em.persist(team); Member member1 = new Member("member1", "멤버1"); Member member2 = new Member("member2", "멤버2"); member1.setTeam(team); member2.setTeam(team); em.persist(member1); em.persist(member2);&#125;public void find&#123; Team team = em.find(Team.class, "team1"); List&lt;Member&gt; members = team.getMembers(); assertThat(members.size(), is(2));&#125; Team 내에 있는 List 에 아무도 값을 넣어준적이 없는데 find 메서드가 정상 동작한다. 이는 hibernate라는 애가 중간에 있기 때문인데, 객체지향을 중요시하는 ORM을 사용하면서 위처럼만 놔두게 되면 결국 RDB 와 다를게 없다고 생각된다. (만약 ORM Framework가 중간에 없었다면 심각한 오류를 발생시켰을 것이다.) 그러므로 순수한 객체까지 고려하는, 연관관계 편의 메서드라는 것을 작성해줘야 한다. 12345678910111213class Member&#123; public void setTeam(Team team)&#123; this.team = team; this.team.getMembers().add(this); &#125;&#125;class Team&#123; public void addMembers(Member member)&#123; this.members.add(member); member.setTeam(this); &#125;&#125; N 쪽에 set, 1 쪽에 add가 항상 묶여서 수행되도록 작성했다. 연관관계 편의 메서드 작성 시 주의사항 사실상 위의 연관관계 편의 메서드는 싱크를 완벽히 맞춰주는 메서드는 아니다. 위와 같은 편의메서드를 사용할 경우 아레와 같은 버그를 막을 수 없다. 1234member1.setTeam(team1);member1.setTeam(team2); // team2로 변경Member foundMember = team1.getMember(); // member1이 여전히 조회된다 별로 마주할 일 없는 시나리오라고 생각할 수 있으나, 그렇게 따지면 항상 hibernate를 거치면 되므로 연관관계 편의 메서드 자체도 필요가 없어지게 된다. 하지만… 위에서도 언급했지만 그건 정말 위험한 코드이다. 언제 어디서 예기치 못한 오류가 발생할지 모른다. ORM으로 바라보기 보다 객체지향으로 먼저 바라봐야 한다고 (나는) 생각한다.(ㅋㅋ) 좀 더 완벽하게 만들어보자. 1234567891011121314151617181920212223class Member&#123; public void setTeam(Team team)&#123; if(this.team != null)&#123; this.team.getMembers().remove(this); &#125; this.team = team; if(team != null &amp;&amp; !team.getMembers().contains(this))&#123; this.team.getMembers().add(this); &#125; &#125;&#125;class Team&#123; public void addMember(Member member)&#123; if(!this.members.contains(member))&#123; this.members.add(member); &#125; member.setTeam(this); &#125;&#125; 무한루프에 빠질 수 있는 가성성과 위의 버그를 제거하였다. 양방향 연관관계를 형성할 때는 항상 위처럼 연관관계 편의 메서드를 만들어줘야 한다(필수!).]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>@OneToMany</tag>
        <tag>@ManyToOne</tag>
        <tag>@JoinColumn</tag>
        <tag>연관관계</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS 기본 동작]]></title>
    <url>%2Fetc%2FDNS-%EA%B8%B0%EB%B3%B8-%EB%8F%99%EC%9E%91%2F</url>
    <content type="text"><![CDATA[정의 https://netmanias.com/ko/?m=view&amp;id=blog&amp;no=5353 PC -&gt; local DNS -&gt; Root DNS -&gt; Top level DNS -&gt; Second level DNS… -&gt; PC]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>DNS</tag>
        <tag>DNS 동작 방식</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[failover]]></title>
    <url>%2Fetc%2Ffailover%2F</url>
    <content type="text"><![CDATA[정의 http://www.terms.co.kr/failover.htm 1차 시스템에 장애가 발생했을 때 2차 시스템에서 이를 받아 수행하는, 무중단 서비스를 위한 방식.]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>failover</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jhipster]]></title>
    <url>%2Fetc%2Fjhipster%2F</url>
    <content type="text"><![CDATA[정의 https://inyl.github.io/programming/2016/10/15/JHipster.html Spring Boot + Front end + etc… 등등으로 구성된 프로젝트를 generate 해주는 서비스이다. 현재는 yo jhipster 대신에 jhipster 명령어만 입력해서 실행시킬 수 있다. 이후 나오는 선택사항들을 토대로 프로젝트가 generator 된다(아주 방대한 양의 소스가 generate 된다) 지원하는 기술이 굉장히 많다. msa 프로젝트 초기 구성시에 좋다. msa 프로젝트 generate 예시 service type - Microservice Application project name port package name discovery server - jhipster registry authentication type - jhipster UAA uaa project path database type - SQL(H2, MySQL…) production database - MySQL development database - MySQL cache abstraction - Hazelcast 2nd level cache - Yes build tool - Gradle other technology - OpenAPI generator, Kafka internationalization - Yes native language - Korean additional language - English test framework - Cucumber other generator - No msa 설정 시 추가로 해줘야할 부분 jhipster-registry 설정 msa 구성 시 eureka 서버를 사용해야 하는데 jhipster-registry가 그 역할을 해준다. 그러므로 클론받고 띄워줘야 한다. 1234git clone https://github.com/jhipster/jhipster-registrycd jhipster-registry./mvnw kafka 설정(선택했을 경우) jhipster init 할때 kafka를 설정했을 경우, docker로 kafka를 실행한다 12cd &#123;jhipster_project_home&#125;docker-compose -f src/main/docker/kafka.yml up -d jdl jhipster에서 도메인을 나타낼 떄 사용하는 language이다. https://www.jhipster.tech/jdl/ 작성한 도메인의 관계도는 아래의 https://start.jhipster.tech/jdl-studio/ 에서 가시적으로 확인해볼 수 있고, .jh 확장자로 따로 저장도 가능하다. 작성한 .jh 파일은 아래의 명령을 통해 jhipster에 적용 갸능하다. 1jhipster import-jdl &lt;jdl file path&gt; 위 명령을 수행하면 작성한 내용을 기준으로 entity, repository를 생성하고 생성된 repository를 사용하여 기본적인 CRUD를 호출하는 controller를 만들어준다. (초반 generate 시에 위 파일을 지정해줄수도 있다고 한다) 하지만 생성된 entity, repository를 그대로 사용할수는 없으니(당연하다) 초반 와꾸잡는데만 사용하고, 그 시점이후로는 jdl 파일을 건드리진 않는다.]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>jhipster example</tag>
        <tag>jhipster-registry</tag>
        <tag>jdl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] Spring ApplicationEvent]]></title>
    <url>%2Fspring%2FSpring%20ApplicationEvent%2F</url>
    <content type="text"><![CDATA[http://wonwoo.ml/index.php/post/1070#comment-7184 event를 발생시키는 publisher(ApplicationEventPublish), event를 받는 lister(ApplicationEventLister), event가 있음(ApplicationEvent). publisher에서 event를 생성해서 발생시키면 lister에서 자신이 구현한 event가 맞을 경우 그 이벤트를 받을 수 있다 스프링 4.2 부터는 어노테이션으로 가능하고, 많은 옵션들(spel 등)을 줄 수 있다.]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Spring ApplicationEvent</tag>
        <tag>Spring event</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[회사 용어]]></title>
    <url>%2Fetc%2F%ED%9A%8C%EC%82%AC-%EC%9A%A9%EC%96%B4%2F</url>
    <content type="text"><![CDATA[TBD To be determined. 결정 예정 TBU To be updated, 업데이트 예정 ASAP As soon as possible, 가능한 빨리, ‘아삽’이라고도 읽음]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>회사 용어</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac custom setting]]></title>
    <url>%2Fetc%2Fmac-custom-setting%2F</url>
    <content type="text"><![CDATA[mac을 편리하게 사용하고자 직접 세팅한 내용을 공유하고자 한다. 일반 키보드 레오폴드 키보드 사용 한영 전환 아래의 내용을 btt에서 똑같이 사용했다 https://jojoldu.tistory.com/345?category=798573 alt키를 command key로 사용하기 환경설정 - 키보드 - 보조키에서 option, command 키 바꿀 수 있음 home, end 사용하기 ~/Library/KeyBindins/DefaultKeyBinding.dict를 수정하면 된다 http://junho85.pe.kr/580 key bindings 값 찾기 http://junho85.pe.kr/579 windows의 aero snap 사용하기 btt - keyboard Command + Control + 방향키로 Maximize Window Left, Right, Top Half, Bottom Half 를 설정했다 최대화 btt - keyboard Command + ctrl + A = 현재 위치에서 최대화 Command + ctrl + S = 다른 모니터(듀얼모니터)로 넘기기 Command + ctrl + D = 다른 모니터로 넘기기 + 최대화 일반 마우스 앞으로 가기, 뒤로가기가 있는 버티컬 키보드 사용 앞으로 가기, 뒤로가기 매핑 btt - normal mice button3에 뒤로가기, button4에 앞으로 가기 매핑했다. action을 사용하지 않고 Command + [, Command + ]로 매핑했다. 이렇게 하면 intellij 에서도 마우스로 앞뒤로 왔다갔다 할 수 있다. swipe 매핑 btt - drawings 오른쪽에서 왼쪽으로 그리면 right swipe 왼쪽에서 오른쪽으로 그리면 left swipe mission control btt - drawings 위로 그리면 mission control 닫기 btt - normal mice(chrome) 마우스 중간 버튼을 누르면 ctrl + w가 동작하도록 했다 새로고침 btt - drawings(chrome) 아래로 길~게 내리면 Command + r 이 동작하도록 했다(스마트폰 처럼) 기타 iterm에cmd + 좌우 동작안하고 fn 키로만 됨 https://stackoverflow.com/questions/6205157/iterm-2-how-to-set-keyboard-shortcuts-to-jump-to-beginning-end-of-line 설정에서 hex code 입력으로 바꿔주면 됨 듀얼 모니터 MonitorControl application 으로 밝기, 소리 조절 가능 https://github.com/the0neyouseek/MonitorControl 해결하지 못한 부분 keyboard로 듀얼 모니터간 포커스 전환 개발하다 보면 마우스보다 키보드를 많이 쓰는데, 반대편 모니터에 있는 애플리케이션을 사용하려면 결국 마우스로 클릭해야 한다. Command + tab으로 하나하나 찾기는 너무 답답하고, 반대편 모니터를 마우스로 찍는 행위만 해주면 좋을텐데… 오른쪽 command의 본래 기능 없애기 오른쪽 command를 한영키로 사용하니까 문자키와 한영키를 눌렀을때 command키가 동작해서 난감하다 q -&gt; ㅂ로 바꾸다가 Command + Q가 되어버리는…]]></content>
      <categories>
        <category>etc</category>
      </categories>
      <tags>
        <tag>mac 한영키</tag>
        <tag>mac home/end</tag>
        <tag>mac mouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 엔티티 매핑]]></title>
    <url>%2Fjpa%2F%EC%97%94%ED%8B%B0%ED%8B%B0-%EB%A7%A4%ED%95%91%2F</url>
    <content type="text"><![CDATA[@Entity JPA가 관리하는 엔티티가 되기 위해서 필수로 붙여야 하는 어노테이션이다. 속성 기능 기본값 name JPA에서 사용할 엔티티 이름 지정. 중복되는 이름이 있어선 안된다. 클래스 이름 그대로 사용(e.g. Member) (name은 나중에 JPQL 작성할 때 사용된다.) 엔티티가 될 클래스에는 몇가지 룰이 존재한다. 기본 생성자 필수(public or protected) final class, enum, interface, inner class 에는 사용 불가 저장할 필드에 final 사용 불가 이 제약조건은 proxy 패턴, reflection에서 자주 등장하는 용어이다. proxy 패턴을 사용하려면 대상 클래스를 상속 받아야 하는데, final class, enum, interface, inner class는 상속이 불가능하다. reflection으로 클래스를 생성할 때, constructor는 대부분 사용하지 않는다. 외부에서는 생성자의 매개변수들이 무엇을 의미하는지 알수가 없기 때문이다. 그러므로 대부분 기본 생성자 + setter를 사용하는데, final은 setter 호출이 불가능하다. 아마 이런 이유일거라고 생각하고, JPA는 결국 proxy 패턴과 reflection을 쓴다는 것을 알 수 있다. (틀렸을수도 있음) @Table 엔티티와 매핑할 테이블을 지정할 때 사용한다. 생략하면 엔티티 이름을 테이블 이름으로 사용한다. 속성 기능 기본값 name 매핑할 테이블 이름 엔티티 이름 uniqueConstraints(DDL) DDL 생성 시에 unique constraints 만듬. 2개 이상의 복합 unique constrains도 가능 기본 키 매핑 위에서도 언급했듯이 기본키는 필수값이다. @Id 어노테이션을 사용해서 지정할 수 있다. 12345class Member&#123; @Id @Column(name="id") private String id;&#125; @Id 적용 가능한 타입은 아래와 같다. 자바 기본형 자바 wrapper형 nullable한 컬럼 때문에 primitive 대신 wrapper를 사용하는 것을 권장(?)한다. 상황에 따라 쓰면되긴 하지만, 알관성을 위해서라도 하나만 쓰는 것이… String java.util.Date, java.sql.Date java.math.BigDecimal, java.math.BigInteger 기본키 생성 전략 엔티티의 기본키를 설정하는 방법에는 크게 직접할당과 자동생성이 있다. 직접할당은 말 그대로 기본키 값을 애플리케이션에서 직접 할당하는 방법이다. 자동생성의 경우 직접할당과 반대로 데이터베이스에 의존하는 방식이다. 자동생성을 사용히려면 기본키 컬럼에 @GeneratedValue 어노테이션 + 전략(strategy)을 지정해줘야 하며, 전략들은 아래와 같다. IDENTITY 전략 MYSQL의 AUTO_INCREMENT와 같다고 보면 된다. 아래와 같이 선언해주면 된다. 123@Id@GeneratedValue(strategy = GenerationType.IDENTITY)private Long id; row insert시에 데이터베이스가 자동으로 생성해주는 기본 키 값을 사용하는 방식이다. 이 말인 즉 데이터베이스에 직접 insert 하는 작업이 선행되어야 한다는 뜻이므로, JPA의 쓰기지연이 동작하지 않는다는 의미이다. 원래라면 insert 1회 + select 1회(저장된 로우의 기본키 값을 얻어오기 위해)로 2번 튱산해야 하는데, 하이버네이트는 JDBC3 부터 추가된 Statement,getGeneratedKeys()(저장과 동시에 생성된 기본 키 값을 얻어오는 메서드)를 사용함으로써 데이터베이스와 한번만 통신한다(최적화) SEQUENCE 전략 ORACLE의 SEQUENCE와 같다고 보면 된다. 기본적으로 SEQUENCE가 생성되어 있어야한다. 그리고 아래와 같이 선언해주면 된다. 1234567891011121314@Entity@SequenceGenerator( name = "BOARD_SEQ_GENERATOR", // 사용할 sequence 이름 sequenceName = "BOARD_SEQ", // 실제 데이터베이스 sequence 이름 initialValue = 1, allocationSize = 1)public class Board&#123; @Id @GeneratedValue( strategy = GenerationType.SEQUENCE, generator = "BOARD_SEQ_GENERATOR" // 위의 sequence 이름 ) private Long id;&#125; 이제 id 식별자 값을 얻어올 때 마다 BOARD_SEQ 시퀀스에서 식별자를 조회해오게 된다. 조회한 식별자를 엔티티에 할당한 후, 영속성 컨텍스트에 저장한다. 이후 flush가 발생하면 엔티티를 데이터베이스에 저장한다. (identity와 달리 insert를 선행할 필요 없어므르 쓰기지연을 사용할 수 있다) 하지만 결과적으로 보면 데이터베이스와 2번 통신하는 셈이다(select 1회 + insert 1회) @SequenceGenerator를 통해 생성기를 등록해야 한다. 속성 기능 기본값 name 식별자 생성기 이름 필수 sequenceName 데이터베이스에 등록되어 있는 시퀀스 이름 hibernate_sequence initialValue DDL 생성 시에만 사용됨, 시퀀스 DDL을 생성할 때 처음 시작하는 수를 지정한다. 1 allocationSize 시퀀스 한 번 호출에 증가하는 수(성능 최적화에 사용됨) 50 initialValue는 sequence 초기값을 설정할 떄 사용하는 옵션인데, 이말인 즉 sequence도 미리 생성해놓지 않으면 자동 생성 가능하다는 것이다(DDL 자동생성을 on 하면 됨) allocationSize는 시퀀스 한번 호출에 증가하는 수이다. default 값이 50인데, 이는 최적화를 위해서이다. 1-50까지의 sequence 값을 한번에 받고 메모리에 저장해서 할당해주다가, 51번째 sequence가 필요할 떄 데이터베이스 sequence에서 51-100의 sequence를 조회해오는 식으로 동작한다. TABLE 전략 sequence를 흉내내는 전략이다. table을 하나 만들어 name과 sequence 값을 저장해둔다. table을 사용하므로 모든 데이터베이스에서 사용 가능하다. 사용법은 sequence와 거의 동일하다. @TableGenerator를 통해 생성기를 등록해야 한다. 1234567891011121314@Entity@TableGenerator( name = "MY_BOARD_SEQ_GENERATOR", // 사용할 table sequence 이름 table = "MY_BOARD_SEQ", // 실제 데이터베이스 table 이름 pkColumnValue = "BOARD_SEQ", allocationSize = 1)public class Board&#123; @Id @GeneratedValue( strategy = GenerationType.TABLE, generator = "MY_BOARD_SEQ_GENERATOR" // 위의 sequence 이름 ) private Long id;&#125; 기본적으로 테이블은 sequence_name, next_val 의 컬럼을 가진 형태로 생성되고, 로우의 내용은 아래와 같다. (이름이 맘에 안들면 pkColumnName=XXX, valueColumnName=XXX 의 형태로 지정해주면 된다.) sequence_name next_val BOARD_SEQ 2 MEMBER_SEQ 7 PRODUCT_SEQ 50 보다시피 하나의 테이블로 관리하므로, pkColumnValue로 어떤 sequence_name을 사용할지 지정해줘야 한다. 참고로 table key는 양이 많아질수록 성능이 급격히 안좋아지므로, 사용하지 않는 것이 좋다. https://vladmihalcea.com/why-you-should-never-use-the-table-identifier-generator-with-jpa-and-hibernate/ AUTO 전략 선택한 데이터베이스 dialect에 따라 전략(IDENTITY, SEQUENCE, TABLE)을 자동으로 선택해주는 방식이다. 예를 들면 오라클은 SEQUENCE, MYSQL은 IDENTITY가 선택된다. @GeneratedValue의 기본값은 AUTO이다. auto_increment랑 sequence를 양쪽 다 지원하는 데이터베이스가 있을 경우(PostgreSQL) sequence를 우선적으로 선택한다고는 하는데, db마다 다를 수 있을 듯 하다. 참고로 @GeneratedValue를 사용한 Id 컬럼에 대해서는 wrapper형을 써주는 것이 좋다. primitive 타입의 경우 초기화 하지 않을 경우 값이 0인데, 이는 명시적이지 않기 때문이다. (0으로 세팅한건지, 값이 세팅하지 않은건지 모호함) 물론 기본 auto_increment가 1부터 시작하기 때문에 0일 경우 ORM이 id를 generate 해줘야겠다고 판단하겠지만, 그래도 명시적인게 좋다고 생각한다. 엔티티 모델링에서 boxing/unboxing 비용은 큰 관심사가 아니다. 식별자 권장 전략 기본키의 형태는 크게 자연키(비즈니스 의미가 있는 키. e.g. 주민등록번호) 와 대리키(임의로 만들어진 키)가 있는데, 외부풍파에 쉽게 흔들리지 않는 대리키를 사용하는 것아 좋다. 비즈니스라는 것은 내 생각보다 훨씬 쉽게 변하기 때문이다. 필드 매핑 @Column 객체를 필드 테이블에 매핑할 때 사용한다. 가장 많이 사용된다. 속성 기능 기본값 name 필드와 테이블 이름 매핑 객체 필드 이름 nullable(DDL) null 값 허용 여부. false 설정하면 DDL 생성 시 not null이 붙는다 true unique(DDL) 컬럼 하나에 unique constraints 지정할 때 사용. 여러개 지정하려면 @Table의 uniqueConstraints를 사용해야 함 length(DDL) 문자 길이 제약조건. 명시적으로 길이를 볼 수 있는 장점도 있다 255 @Column을 생략해도 엔티티의 필드는 전부 자동으로 테이블과 매핑된다. 이 때 몇가지 특징이 있다. 이름은 어떻게 매핑되는가? @Column의 기본값과 동일하게 컬럼명으로 사용된다. 근데 여기서 딜레마가 하나 있다. java는 naming을 관례적으로 camel case를 사용하고, database는 naming을 관례적으로 under score를 사용한다는 것이다. 이떄 persistence.xml에 1&lt;property name="hibernate.ejb.naming_strategy" value="org.hibernate.cfg.ImprovedNamingStrategy"&gt;&lt;/property&gt; 전략을 주게 되면 위의 딜레마를 해결 가능하다(서로간에 자동 변환) nullable 속성 123456int id; // not null로 생성됨. primitive에는 null이 들어갈 수 없기 때문.Integer id; // nullable true로 생성됨@Columnint id; // @Column의 기본값인 nullable=true가 적용되서 nullable=true로 생성됨. 주의해야함 보다시피 3번쨰 방법은 nullable=true 임에도 불구하고 null을 넣을 수 없다. 이런 상황을 위해 그냥 primitive 대신 wrapper형을 써주는 것이 좋다. (not null에는 primitive, nullable에는 wrapper형을 쓸수도 있지만 통일시키지 않아서 오는 불편함이 더 클것이다) @Enumerated java의 enum 타입을 매핑할 때 사용된다. 유용하게 사용 가능하다. 12@Enumerated(EnumType.STRING)private RoleType roleType; 이렇게 주면 enum의 값 그대로(문자열) 데이터베이스에 저장된다. ORDINAL을 속성을 사용하면 enum의 순서대로 index가 데이터베이스에 저장되는데, 유연하지 못하므로 STRING 속성을 사용하는 것이 낫다. @Temporal 날짜 타입을 매핑할 때 사용된다. 12345678@Temporal(TemporalType.DATE)private Date date; // date date 생성@Temporal(TemporalType.TIME)private Date time; // time time 생성@Temporal(TemporalType.TIMESTAMP)private Date timestamp; // timestamp timestamp 생성 자바의 Date 타입에는 년월일 시분초가 있지만, 데이터베이스에서는 date, time, datetime 3가지 타입이 존재한다. 그러므로 @Temporal을 생략하였을 시, 가장 비슷한 timestamp가 지정된다. @Temporal은 Date, Calendar에만 붙이는 속성이고, java8 부터 등장한 LocalDate, LocalTime, LocalDateTime에는 @Temporal 속성을 붙일 수 없다. java8 날짜 타입들은 jpa가 바로 인식하지 못하므로 추가적인 조치가 필요하다 https://homoefficio.github.io/2016/11/19/Spring-Data-JPA-에서-Java8-Date-Time-JSR-310-사용하기/ @Lob BLOB, CLOB 타입에 매핑된다. 12345@LobString lob; // CLOB으로 매핑. mysql에선 longtext로 생성됨@Lobbyte[] lob; // BLOB으로 매핑. mysql에선 longblob으로 생성됨 @Transient 매핑하지 않을 필드에 설정한다. 임의로 값을 보관하고 싶을때 등에 사용한다. @Access JPA가 엔티티 데이터에 접근하는 방식을 지정한다. 필드 접근: AccessType.FIELD로 지정한다. 필드에 직접 접근한다. 접근 권한이 private이어도 접근할 수 있다. 프로퍼티 접근: AccessType.PROPERTY로 지정한다. Getter를 사용한다. 설정하지 않으면 @id의 위치를 기준으로 접근 방식이 설정된다. 123456789@Entity@Access(AccessType.FIELD)public class Member&#123; @Id private String id; private String data1; private String data2;&#125; @id가 필드에 있으므로 @access(AccessType.FIELD)로 설정한 것과 같다. @access 생략가능 12345678910111213141516171819202122@Entity@Access(AccessType.PROPERTY)public class Member&#123; private String id; private String data1; private String data2; @Id public String getId()&#123; return id; &#125; @Column public String getData1()&#123; return data1; &#125; public String getData2()&#123; return data2; &#125;&#125; @id가 프로퍼티에 있으므로, @access 생략가능 1234567891011121314151617@Entitypublic class Member&#123; @Id private String id; @Transient private String firstName; @Transient private String lastName; private String fullName; @Access(AccessType.PROPERTY) public String getFullName()&#123; return firstName + lastName; &#125; @id가 필드에 있으므로 기본은 필드 접근 방식 사용, @getFullName()만 프로퍼티 접근방식을 사용한다. 결과적으로 회원 엔티티를 저장하면 회원 테이블의 FULLNAME 컬럼에 firstName + lastName 결과가 저장된다. @Access를 사용하는 이유 https://stackoverflow.com/questions/13874528/what-is-the-purpose-of-accesstype-field-accesstype-property-and-access]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>@Entity</tag>
        <tag>@Table</tag>
        <tag>@Column</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PUT, PATCH의 차이]]></title>
    <url>%2Frest%2FPUT-PATCH%EC%9D%98-%EC%B0%A8%EC%9D%B4%2F</url>
    <content type="text"><![CDATA[정의 https://stackoverflow.com/questions/28459418/rest-api-put-vs-patch-with-real-life-examples PUT은 전체 엔티티를 전달해줘야하고, PATCH는 변경하고자 하는 속성만 전달해주면 된다. 123456789101112// PUT&#123; "name" : "joont", "age" : 27, "sex" : "male", // send all data&#125;// PATCH&#123; "name" : "joont92" // send only data you want to change&#125; PUT에 전달한 엔티티에 일부 속성이 누락될 경우 해당 속성은 API 호출 후 값이 유실된다.(매우 중요) PUT은 대체한다는 개념으로 보면 된다. 없으면 생성도 된다. 사례 123API : item/optionscontent : option list행위 : 기존 item에 있는 option들을 전부 지우고, 전달받은 option들로 전부 다시 인서트함 위처럼 자식의 내용을 다 지우고 다시 인서트 하는 형태의 API의 경우 PUT이 좀 더 바람직하다. 교체의 개념과 딱 맞기 떄문이다. 근데 여기서 전달받은 options의 element에 식별자가 있으면 update 한다고 할 경우, 이건 PUT이 맞을까?]]></content>
      <categories>
        <category>rest</category>
      </categories>
      <tags>
        <tag>rest put patch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swagger 2.0 문법]]></title>
    <url>%2FopenAPI%2Fswagger-2-0-%EB%AC%B8%EB%B2%95%2F</url>
    <content type="text"><![CDATA[문법 기본 구조 https://swagger.io/docs/specification/2-0/basic-structure/ paths에 api url, definitions에 object를 정의해서 서로 사용 가능 data types https://swagger.io/docs/specification/data-models/data-types/ 기본적으로 string, number, integer, boolean, array, object 타입을 가지며 2.0에서 저 많은 옵션들을 다 지원하는지는 모르곘다. array는 항상 items 옵션을 가진다. request https://swagger.io/docs/specification/2-0/describing-parameters/ path, query, formData type들을 받을 수 있으며 required, default, minimum 등 여러가지 옵션을 줄 수 있다. https://swagger.io/docs/specification/2-0/describing-request-body/ body 형태의 파라미터도 받을 수 있다. path 파라미터를 제외하고 하나만 받을 수 있다. response https://swagger.io/docs/specification/2-0/describing-responses/ response code, header, response data 등을 줄 수 있다. response data로 definitions에 정의한(definitions는 그냥 이름을 뿐임) object를 줄 수 있다. enum https://swagger.io/docs/specification/2-0/enums/ 기본적으로 enum은 string array 형태로 작성한다. 그리고 definitions 처럼 따로 선언해서 재사용하게 할 수 있다. parameters에서는 &amp;ENUM을 사용하지만 object 내에서는 #/definitions/ENUM형태로 선언해야 한다. 1234567891011121314CategoryType: type: string enum: &amp;CATEGORYTYPE - MAIN - SUBCategoryDTO: title: CategoryDTO type: object properties: externalId: type: string type: enum: *CATEGORYTYPE 이런식으로 선언할 경우 code-gen에서 이상한 TypeEnum 형태의 inner enum을 generate 한다.]]></content>
      <categories>
        <category>openAPI</category>
      </categories>
      <tags>
        <tag>swagger</tag>
        <tag>swagger 2.0</tag>
        <tag>swagger 문법</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[jpa] 영속성 관리]]></title>
    <url>%2Fjpa%2F%EC%98%81%EC%86%8D%EC%84%B1-%EA%B4%80%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[엔티티란? 간단하게 DB 테이블에 매핑되는 자바 클래스를 얘기한다. 이 클래스의 인스턴스가 결국 RDB의 레코드 하나로 매핑될 수 있다. 영속성 관리 영속성이란 간단하게 영구히 저장되는 성질을 얘기한다. ORM 이기 떄문에 영구히 저장되는 환경은 당연히 RDB이고, 영구히 저장할 대상은 엔티티이다. JPA에서는 이 행위를 엔티티 매니저라는 애가 수행한다. 엔티티 매니저 이름 그대로 엔티티를 관리하는 관리자이다. 엔티티와 관련된 모든 작업(삽입, 수정, 삭제 등)을 수행할 수 있다. 엔티티 매니저를 생성하는 플로우는 아래와 같다. 매타정보 입력 123456789&lt;persistence-unit name="test"&gt; &lt;properties&gt; &lt;property name="javax.persistence.jdbc.driver" value="org.h2.Driver"/&gt; &lt;property name="javax.persistence.jdbc.user" value="sa"/&gt; &lt;property name="javax.persistence.jdbc.password" value=""/&gt; &lt;property name="javax.persistence.jdbc.url" value="jdbc:h2:~/test"/&gt; &lt;property name="hibernate.dialect" value="org.hibernate.dialect.H2Dialect" /&gt; &lt;/properties&gt;&lt;/persistence-unit&gt; 엔티티 매니저가 사용할 메타정보이다. database 접속 정보 등이 설정되어 있다. META-INF/persistence.xml에 입력하면 자동으로 스캐닝한다. 엔티티 매니저 팩토리 등록 엔티티 매니저는 엔티티 매니저 팩토리를 통해 생성할 수 있다. 1EntityManagerFactory emf = Persistence.createEntityMangerFactory("test"); 앞서 등록한 메타정보를 통해 엔티티 매니저 팩토리를 생성한다. 보다시피 팩토리이므로 어플리케이션 실행 시 한번만 생성해서 공유하도록 하면 된다.(여러 스레드가 접근해도 안전함) 이 시점에 connection pool을 init 한다. J2SE에서는 엔티티 매니저 팩토리 생성 시 커넥션 풀을 생성하고, J2EE의 경우 컨테이너가 제공하는 데이터 소스를 사용한다. 엔티티 매니저 생성 등록한 엔티티 매니저 팩토리에서 생성하면 된다. 12345EntityManager em = emf.creteEntityManger();em.persist(entity); // 등록em.find(entity); // 조회em.remove(entity); // 삭제 보다시피 팩토리에서 매번 생성해서 사용하는 구조이며, 생성 시 마다 connection을 하나 준다고 생각하면 된다. (엔티티 매니저를 생성했다고 바로 커넥션을 얻는 것은 아니고, 정말 필요할 시점에 커넥션을 획득한다) 여기서 생성된 엔티티 매니저는 데이터베이스에 대한 직접적인 하나의 커넥션이므로, 쓰레드간에 절대 공유해서는 안된다. 개발자 입장에서는 엔티티 매니저는 엔티티를 저장하는 가상의 데이터베이스라고 생각하면 된다. 영속성 컨텍스트 용어를 정의하면 엔티티를 영구 저장하는 환경이다. 엔티티 매니저는 작업을 수행할 때 RDB에 바로 접근하지 않고, 이 영속성 컨텍스트를 통해 작업을 수행한다. 즉 어플리케이션과 RDB 사이에 하나 더 있는 영역인데, 이 영역을 통해 얻는 이점은 아래와 같다.(뭐든 중간에 하나 두면 성능 최적화를 할 요소가 많아진다) 1차 캐시 1차 캐시의 키는 식별자 값이다. em.find()를 호출하면 먼저 1차 캐시에서 엔티티를 찾고 만약 없으면 데이터베이스에서 조회한다. 데이터베이스에서 조회후 1차 캐시에 저장한 후에 영속상태의 엔티티를 반환한다. 이후 한 트랜젝션안에서는 엔티티 인스턴스는 1차 캐시에 있으므로 이 엔티티를 조회시 메모리에 있는 1차 캐시에서 바로 불러오므로 성능상의 이점을 누릴 수 있음 동일성 보장 동일성과 동등성 ==, equal 영속성 컨텍스트에서 관리하는 엔티티 인스턴스는 동일성을 보장한다. 트랜잭션을 지원하는 쓰기 지연 변경 감지 지연 로딩 엔티티 매니저가 생성될 때 하나 생성된다. 보다시피 1차 캐시 + 부가적인 기능들을 묶어서 영속성 컨텍스트라고 부른다. 레이어로 딱 나뉘어져 있는 것은 아니다. 궁금해서 em.persist의 소스를 조금 따라가 보다보니… 1차 캐시에 대한 내용을 약간 확인할 수 있었다. 12345678private Map&lt;EntityKey, Object&gt; entitiesByKey; // 1차 캐시(!) // ... @Overridepublic void addEntity(EntityKey key, Object entity) &#123; entitiesByKey.put( key, entity ); // 여기! getBatchFetchQueue().removeBatchLoadableEntityKey( key );&#125; 보다시피 SaveOrUpdateEventListener 에서 엔티티들을 Map에 저장하고 있다. key, object의 형태로 저장함을 볼 수 있다. (key는 hashCode와 persister(?) 등으로 조합된 클래스이다) 엔티티 키를 만드는 행위를 간단히 보면 아래와 같다. 1final EntityKey key = source.generateEntityKey( event.getRequestedId(), persister ); (엔티티의 식별자값(primary key)을 통해 만들고 있다. 엔티티 매니저에 의해 관리되러면 식별자값은 필수이다!!) 모든 행위들에 대해 이런식으로 저장하고, 트랜잭션이 끝나는 시점에 이런 정보들을 종합하여 최종 SQL을 날린다고 보면 된다.(이 행위를 flush라고 한다) 의문. PK가 아닌 다른 조건으로 조회했을때는 어떻게 되는건가? 영속성 컨텍스트는 1차 캐시의 역할을 하므로, 이미 조회해온 엔티티에 대해서는 추가 조회를 하지 않고 1차 캐시에 있는 엔티티를 돌려준다. 근데 만약… 조회의 조건을 바꿔서 검색했을때는 어떻게 되는걸까? 예를 들어 findByUserNameContaining으로 조회하면 해당 엔티티들이 전부 1차 캐시에 저장될 것이다. 이후에 다른 조건, 예를 들면 findByUserNickNameContaining으로 조회했을 경우 분명 첫번째 조회 결과와 두번째 조회 결과는 겹치는 부분이 있을 것이다. 이 부분에 대해서 그냥 재조회를 하는건지, 아니면 겹치는 부분은 1차 캐시의 엔티티를 반환해주는지 궁금하다. 후자의 경우가 더 비효율적일것 같은데…(리스트끼리 서로 돌면서 여부를 체크해야하기 때문에) 엔티티 생명주기(상태) 위에서 언급한 영속성 컨텍스트에 저장하는 행위에도 결국 일종의 룰이 필요할 것이다. 여기서 등장하는 것이 엔티티 상태 이다. 엔티티 매니저는 엔티티들의 상태를 통해 여러가지 작업들을 수행한다. 이를 엔티티 생명주기라고 한다. 비영속 영속성 컨텍스트와 전혀 관계없는 상태이다. 그냥 엔티티를 생성하면 비영속 상태이다. 1Member member = new Member(); // 비영속 상태! 영속 엔티티 매니저를 통해 엔티티를 영속성 컨텍스트에 저장한 상태를 말한다. (간단하게 말하면 위의 Map에 저장된 상태) 영속상태로 전환하는 법은 간단하다. em.persist나 em.find를 통해 엔티티를 저장하거나 조회하기만 하면 영속 상태가 된다. 영속성 컨텍스트에 저장되고, 엔티티 매니저에 의해 관리된다는 뜻! 준영속 조금 특별한 상태이다. 영속상태였다가 비영속상태로 변환된 엔티티를 준영속 상태라고 한다. em.detach 메서드를 통해 전환할 수 있으며, 전환 시 1차 캐시, 쓰기지연 저장소에 저장된 정보들이 모두 삭제된다. 결과적으로 영속상태가 아니개 되는 것이므로 영속성 컨텍스트에서 제공하는 모든 기능을 사용할 수 없다. 12345em.persist(member); // 영속 상태member = em.detach(member); // 준영속 상태member.setName("changed name"); // update 발생하지 않음 em.detach외에도 em.clear를 통해 영속성 컨텍스트 내의 모든 엔티티를 지워버림으로써 준영속 상태로 만들 수 있고, em.close를 통해 영속성 컨텍스트를 종료해버림으로써 준영속 상태로 만들 수 있다. 비영속 상태와 별 다를것 없지만 하나 확실한 것은, 실존하는 데이터라는게 증명이 된다는 것이다. (영속성 컨텍스트에 들어갔었으면 등록되거나, 조회되어진 데이터이므로) 실제로 개발자가 준영속 상태를 활용할 경우는 거의 없다. 행위 조회 em.find를 통해 엔티티를 조회해올 수 있다. 바로 데이터베이스에서 조회해오는 것은 아니고, 영속성 컨텍스트를 거쳐서 조회한다. 처음 em.find를 통해 오브젝트를 찾으면 먼저 영속성 컨텍스트에 해당 오브젝트(key로 조회)가 있는지 찾고 있으면 db로 가지 않고 그 오브젝트를 바로 리턴하고, 없으면 db에서 조회해온 뒤 영속성 컨텍스트에 저장하고 그 오브젝트를 리턴한다. 어플리케이션 레벨에서 캐싱이 가능하단 뜻이다! 1234Member member1 = em.find(Member.class, "joont92");Member member2 = em.find(Member.class, "joont92");assertSame(member1, member2); // success 하이버네이트와 같은 ORM 프레임워크를 사용하지 않았다면 동일한 레코드임에도 불구하고 쿼리를 두번 날리는 결과가 발생했었을 것이다. 등록 em.persist를 통해 엔티티를 데이터베이스에 등록할 수 있다. (정확히 얘기하면 persist는 해당 엔티티를 영속성 컨텍스트에 등록하라는 의미이다. 뒤에 나올 @GeneratedValue의 특징 떄문에 persist == 저장 이라고 착각할 수 있는데, 이는 틀렸다) 해당 메서드를 실행함과 동시에 데이터베이스에 바로 저장하는 것은 아니고, 먼저 영속성 컨텍스트에 저장한다. 근데 여기서 단순히 영속성 컨텍스트에 저장하는 작업만을 하는것은 아니고, 쓰기지연 SQL 저장소라는 곳에 insert 쿼리를 등록하는 작업까지 동시에 진행한다. 그리고 마지막에 flush가 일어나면 여기에 저장된 SQL을 데이터베이스로 발사!하는 것이다. 이런식으로 쿼리를 바로 날리지 않고 쓰기지연을 수행하는 이유는, 네트워크 통신 횟수를 줄여 이득을 취하기 위함이다. 추후에 나오겠지만 auto_generate key를 사용하는 데이터베이스는 이 flow대로 진행되지 않는다. key 값을 얻어오기 위해 persist와 동시에 insert 쿼리를 실행해버린다. 어떻게 쓰기 지연이 가능할까? 데이터베이스에 트랜잭션이라는 개념이 있기 때문이다. 데이터베이스에 DML을 아무리 날려도 commit을 하지 않으면 적용되지 않는다는 특징을 이용하여 쓰기 지연을 가능하게 할 수 있다. 데이터베이스에 직접 날리지 않고 쿼리를 메모리에 저장해두는 방식으로 가능하다. 수정 수정은 딱히 메서드가 존재하지 않는다. 이는 JPA에 변경감지라는 특징이 있기 때문이다. 엔티티가 처음으로 영속상태에 들어갈 경우, Map에 저장만 하는 것이 아니라 초기 엔티티의 스냅샷이라는 것을 찍어둔다. 그리고 마지막 flush가 일어날 때 영속성 컨텍스트에 저장된 엔티티의 속성 값들과 엔티티 스냅샷의 속성 값들을 비교한다. 메서드 실행 시점에 쿼리를 쌓는 다른 메서드들과는 달리, 엔티티 매니저가 flush될 때 쿼리를 생성한다. 즉, 시점이 다르다. deep 탐색까진 하지 않는다(list의 member들까지 탐색하지는 않음) 그리고 변경이 일어났을 경우 update 메서드를 생성하여 이를 데이터베이스에 발사한다.(변경 감지) (이러한 로직이므로 따로 dirty check가 필요없다) 생성되는 update문의 형태 실제로 update를 발생시켜보면 알겠지만, 업데이트가 발생하는 특정 속성에 대해서만 업데이트 하는 것이 아니라 전체 오브젝트에 대해 업데이트를 실행하는 쿼리가 생성된다. 이렇게 하면 매번 사용하는 수정 쿼리가 같다는 점을 이용한 것이고, 속성 하나하나에 대해 쿼리를 다 생성해놓지 않아도 된다는 장점이 있다(진짜 장점인가) JPA가 로딩 시점에 업데이트 쿼리를 미리 생성해둔다. 하지만 필드의 내용이 너무 많을 경우 매번 이런식으로 풀 업데이트 쿼리를 날리는 것은 비효율적이다. 기본적으로 모든 컬럼을 다 보내는것 자체가 데이터 전송량 낭비이기 때문이다. 이때는 아래와 같이 @DynamicUpdate 어노테이션을 사용하면 수정된 데이터에 대해서만 update를 실행하는 쿼리를 생성한다. 123456@Entity@Table(name = "MEMBER")@org.hibernate.annotations.DynamicUpdateclass Member&#123;&#125; 상황에 따라 다르지만 필드가 30개가 넘어가면 위와 같이 @DynamicUpdate를 쓰는 것이 좋다고 한다. 그리고 그 이전에, 30개가 넘어가는 테이블이면 정규화가 제대로 되지 않는다는 고민이 선행되어야 할 것이다. 삭제 엔티티를 삭제하려면 먼저 삭제 대상 엔티티를 조회해야 한다. 12Member member = em.find(Member.class, "joont92");em.remove(member); em.detach + delete 쿼리라고 보면 된다. 메서드를 실행하면 해당 엔티티는 영속성 컨텍스트에서 detach 된다(그리고 delete 쿼리를 쓰기지연 SQL 저장소에 저장?) 병합 준영속 상태인 엔티티를 다시 영속상태로 만드는 행위를 말한다. em.merge 메서드를 사용한다. 1Member foundMember = em.merge(member); member는 영속성 상태에서 제거된 준영속 상태이므로 식별자를 가지고 있다. 그러므로 위 메서드 실행 시, 해당 식별자로 조회쿼리가 날라가서 해당 엔티티가 영속화될 수 있는지(실제 존재하는지) 체크하게 된다. 있다면 영속성 컨텍스트에 저장하고, 새로운 엔티티를 리턴한다. 즉, foundMember와 member는 같지 않다. 기존 JPA 명세에 따르면, 전달받은 식별자로 해당 엔티티를 찾을 수 없을 경우(식별자가 전달되지 않은 경우도 마찬가지) IllegalArgumentException이 발생하게 된다. 하지만 hibernate 에서는 이럴 경우 그냥 새로 저장해주고, 영속성 컨텍스트에 등록하여 리턴해준다.(@GeneratedValue일 경우 식별자 generate, 아닐 경우 전달받은 식별자로 등록한다) flush 영속성 컨텍스트의 변경 내용을 데이터베이스에 반영하는 행위이다.(커밋되는 것은 아니다) 쓰기지연 SQL에 저장된 SQL들을 발사!하고, 위에서 언급했듯이 영속성 컨텍스트에 들어있는 오브젝트와 스냅샷을 비교하여 업데이트 쿼리를 생성한 뒤, 업데이트 쿼리를 생성하여 발사!한다. flush를 발생시키는 방법은 3가지 정도가 있다. em.flush() 메서드를 직접 호출 거의 사용할 일이 없다 트랜잭션 커밋 시 플러시 자동 호출 flush 하지 않고 commit 할 경우, SQL이 하나도 실행되지 않은 상태이기 떄문에 아무런 일도 일어나지 않는다. JPA에서는 이런 상황을 방지하기 위해 commit시 flush를 자동으로 호출한다. JPQL 실행 시 플러시 자동 호출 JPQL은 호출시에 SQL로 변환되어 데이터베이스에서 조회해오는데, 이럴려면 레코드들이 이미 데이터베이스에 저장되어 있어야 한다. persist와 JQPL 호출 작업을 한 트랜잭션 내에서 하는 행위를 방지하기 위해 위와 같이 처리한 듯 하다. 플러시 모드를 변경하려면 javax.persistence.FlushModeType을 사용하면 된다. FlushModeType.AUTO : 커밋이나 쿼리를 실행할 때 플러시(default) FlushModeType.COMMIT : 커밋할 때만 플러시 flush를 한다고 해서 영속성 컨텍스트에서 엔티티가 지워지는 것은 아니다!! (이걸 신경쓸 일이 있곘느냐만…)]]></content>
      <categories>
        <category>jpa</category>
      </categories>
      <tags>
        <tag>자바 ORM 표준 JPA 프로그래밍</tag>
        <tag>영속성 컨텍스트</tag>
        <tag>엔티티 매니저</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[27살, 2번째 이직]]></title>
    <url>%2Flife%2F27%EC%82%B4-2%EB%B2%88%EC%A7%B8-%EC%9D%B4%EC%A7%81%2F</url>
    <content type="text"><![CDATA[2018년 6월 말. 다니고 있던 2번째 회사를 퇴사하게 되었습니다. 이번 이직 과정에서 인터넷에 공유된 여러 글들을 통해 많은 도움을 얻었고, 블로그에 이직 관련 글 올리신 분들이 나름 멋있어보여서(…) 이직 성공하면 글을 올려야겠다고 생각했었습니다 ㅎㅎ 그리고 성공적으로 이직을 하였고… 이제 출근한지 4주 정도 되었네요. 너무 간만에 일을 해서(4개월이나 쉬었어요 ㅠㅠ) 정신을 못 차리다가… 이제서야 글을 올립니다! 이번 이직 과정에서 개인적으로 매우 성장했다고 생각되어서, 미흡하지만 제 경험을 공유해보려고 합니다. 보시는 분들에게도 혹시나 좋은 영향을 줄 수 있으면 좋겠습니다. 😊 2018년 4월 이때가 퇴사를 결심하게 된 시기였습니다. 퇴사를 결심하고 미국행 비행기표를 끊어버린 시점이기도 하지요 ㅋ.ㅋ 부끄럽지만 아마도 처음으로 회사에 대해 진지하게 생각해본 때였던 것 같습니다. 이전까지는 별 생각없이 회사에서 붙여주고 조건만 나쁘지 않으면 바로 갔었거든요… 그래서 이때 회사가 2번째 회사였는데, 개발자로 면접을 본 경험은 딱 2번이었습니다. 진지하지도 못하고, 시야도 좁았던거죠. 하지만 시간이 지나고, 여러가지 경험을 함으로써 저에게도 나름의 가치관이 정립되기 시작했고… 원하는 기업의 형태도 생기게 되더라고요. 그래서 많은 고민을 했었고, 결국에는 그만두자는 결론을 내리게 되었습니다. (사실은 3년 쯤 일하니까 1달 정도 쉬고 싶었던 것도 큽니다) 결론을 내림과 동시에 미국행 비행기도 끊어버렸습니다 ㅋㅋㅋ 처음으로 쉬어보는거 였거든요(두번째 회사로 올떄는 텀이 없었음) 출국이 6월 말이었기 떄문에 그 시점 이후로 2달 정도 일을 더 하며 마무리하고, 2018년 6월 중순에 최종적으로 퇴사하게 되었습니다. 원하는 회사 저 시점쯤에 생긴 원하는 회사의 조건은 아래와 같았습니다. 개발자가 많았으면 좋겠다 항상 소규모 개발팀에만 있어서 이런 환상이 좀 존재했었습니다. 개발자가 몇십명 있으면 진짜 어벤져스 느낌도 날 거같고… 그렇게 같이 팀워크하면서 프로젝트 진행하고… 하면 재밌을 거 같단 생각이 들었습니다. (개인적으로 이전 회사에서 혼자서 프로젝트 하는것이 좀 힘들었습니다 ㅠ.ㅠ) 코드리뷰가 있으면 좋겠다 코드리뷰 문화가 좋은것만 알고… 한번도 겪어보지를 못해서 꼭 이런 문화가 있는 기업이었으면 했습니다. 기술 스택이 높았으면 좋겠다 무조건적으로 신기술을 사용했으면 좋겠다는 아니지만, 좋은 것들은 다 썼으면 좋겠다고 생각했습니다! (Java8도 쓰고, JPA도 쓰고, AWS도 쓰고, MSA도 하고…!) IT가 주 수익모델인 IT 기업이면 좋겠다 이래야만 개발 조직에 역량이 집중 될 거라고 생각합니다. 월급을 잘 주면…(많이 주면?!) 좋겠당… 돈 많이 받고 싶은건 누구나 그러니까요… ㅎㅎㅎ (월급 밀리는 경험도 당해봐서 ㅠㅠ) 나의 현위치? 스펙은 그냥… 별 거 없습니다… 대학은 뭐 가자마자 거의 바로 관두고… 학은제로 학사 땄습니다 개발은 부산에서 국비지원 교육을 받으면서 처음 배우고, 수료하고 서울에 취업했습니다. 회사는 총 2개를 다녔고, 총 경력은 3년 정도였습니다. 공공기관 SI/SM(1년 반 정도) 사실… 여기는 경력이라고 하긴 좀 그렇습니다… 개발을 안했거든요…ㅋㅋ;; html이나 좀 고치고, cs 업무 같은거 했던거 같네요… 정말 최악 ㅠㅠ 1년쯤 넘었을때 개발 스터디를 하나 시작하게 되었고, 거기서 다른 개발자분들을 만났고, 현실을 직시하고 바로 퇴사했습니다 맛집 추천 서비스(여기도 1년 반 정도) 여기서 많은걸 배우고, 많은 경험을 했습니다. 사실상 제 경력은 여기 다 있다고 보는게 낫겠네요 ㅎㅎ 백엔드, 프론트엔드를 같이 했었습니다. 미국 갔다와서 정리한번 하고, 취업준비 하려는데 경력도 별거 없고… 실력도 미흡해서 엄청 암울했었던 기억이 나네요 ㅠㅠ 게다가 백엔드 프론트엔드 같이 하다보니 뭔가 잡캐가 된 느낌이었고… 그래도 이직은 해야겠지요! 이미 회사가 없으니까요!!! ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 이력서를 써보자! 리프레쉬 할만큼 하고, 서울 돌아와서 어느정도 정리한 뒤 8월 말쯤부터 이력서를 쓰기 시작했습니다. 막상 쓰려니까 너무 막막하고… 왜 미리미리 안써놨나 생각이 들었습니다. ㅠㅠ 그래서 여기저기 관련 글들을 찾아 읽어보았고, 주니어 개발자를 위한 취업 정보에 올라와 있는 Outsider님, 구인본님, 이한별님의 이력서를 많이 참고하여 아주 마음에 드는 이력서를 작성할 수 있었습니다(다들 너무 감사드립니다 ㅠㅠ) 이력서 작성은 mac pages로 하였습니다. 첫 장에서 바로 제 연락처와 기술스택, 경력을 나열했습니다. 경력직 이직이라 기술스택에 관심이 많을거라 생각했거든요 ㅎㅎ 그리고… 개인적으로 스킬에 레벨을 쓰지는 않았습니다. 매우 주관적인 부분이기도 하고, 그런거 쓰지 말라는 글도 봐서… 그리고 두번째 장에 제 프로젝트에 대해 나름 상세하게 나열했습니다. 프로젝트명, 사용한 기술, 설명등을 작성했습니다. 세번째 장에는 기본적인 제 학력, 교육 수료 내용, 자격증 등을 작성했습니다. 그리고 추가적으로 외부활동 이라고 만들어서 제가 스터디하는 내용을 나열했는데, 이걸 마지막 페이지보다 더 위로 올려야 되나 고민했었습니다. 하지만 도저히 넣을 공간이 안나오더라고요 ㅎㅎ 그래서 저기 뒀습니다. 작성하고나니 정말 만족스러웠습니다. 딱딱한 틀에 갖춰진 이력서도 아닌 자유양식의 이력서를 처음 가져보기도 했고, 막상 써놓고 보니 “아 내가 생각보단 쓸모있는 사람이구나” 라는 생각도 들더라고요 ㅋㅋㅋㅋ 지원하자! 이력서를 최종 작성했으니 이제 지원해야곘지요 ㅎㅎ 대부분의 이력서는 원티드를 통해 지원했고요(좋은 기업도 많고… 돈도 주니까!) 잡플래닛도 간간히 이용했습니다. 잡플래닛에서 공고를 보고 해당 기업의 채용공고 사이트를 들어가 지원했었죠. 그리고 잡코리아는 인재등록만 해두고, 따로 지원하지는 않았습니다. 처음엔 인재등록 후 모든 기업에게 공개되도록 설정해놨었는데, 전화랑 메일이 너무 많이와서 컨트롤이 불가능하더라고요… 게다가 이상한 곳도 너무 많았습니다. 그래서 한 3일 정도 지켜보다가, 헤드헌터만 검색할 수 있도록 설정을 바꿔버렸습니다. 개인적으로 잡코리아에 인재등록 하실꺼면 헤드헌터만 볼 수 있도록 해놓는걸 추천합니다 ㅠㅠ 결과는?! 놀랍게도 아주 많은 곳에서 서류를 통과시켜 주셨습니다. 일단 방금도 잠시 언급했지만 잡코리아… 잡코리아 헤드헌터 님들한테 연락이 엄청 많이 왔습니다. 최소 매일 1통씩은 전화나 메일을 받았던 것 같네요. 좀 놀라웠고, &quot;아 경력이 3년정도면 꽤나 귀해지는구나…&quot;라는 생각도 들었습니다. 헤드헌터님들에게 받은 JD중 마음에 드는 기업들이 있으면 이력서를 드렸고, 대부분 이력서를 내자마자 바로 면접이 잡혔습니다. 그래서 거의 이력서 제출과 동시에 면접을 보러 다니기 시작했습니다… 그렇게 1주일 정도 지났을 쯤, 원티드에 지원한 것들도 슬슬 서류통과/서류탈락 이 결정되기 시작했습니다. 그리고 2주차부터는 면접이 너무 많아 엄청 바빴습니다… (작으면 하루에 1개 정도 봤고, 날짜가 몰려서 3개까지 면접을 봤던 적도 있습니다 ㄷㄷ) 나의 면접 전략?! 전략이라니까 뭔가 거창하긴 한데, 제 입장에서는 많은 도움이 되었으므로 일단 써보겠습니다. 이번 면접에 대한 저의 전략은, 간단하지만 “면접을 분석하고, 나를 개선시키자” 였습니다. 그래서 저는 제가 보는 모든 면접을 녹음하고, 집에와서 다시 듣고, 그것을 정리했습니다. 일단 면접을 녹음하고(이게 범법 행위인가…) 집에와서 다시 들어보면, 생각보다 얻는 효과가 매우 큽니다. 일단 내가 붙을지 떨어질지 대략적으로 예측이 가능합니다. 딱 들어보면 “아 여긴 붙겠네”, &quot;아 여긴 망했네&quot;가 느껴지더라고요 제 입장에선.ㅋㅋ 그리고 중요한건, 이것을 하나하나 다 정리하는 행위입니다. 면접의 질문과 내가 한 답변을 정리하면 면접에서 주로 나오는 질문의 패턴들도 파악할 수 있게되고, 내가 어느 부분에서 부족한 답을 하고 있는지도 파악이 가능하게 됩니다. 그렇다면 다시 그 질문을 보면서 기술적인 질문이면 구글링을 통해 찾아보고, 철학적이거나 개인의 가치관적인 질문이면 이것에 대해 다시 생각해볼 수 있겠죠. (개인적으로는 “왜 이런 질문을 한걸까?” 라는 생각을 해보면 더 좋았습니다) 이러면 다음 면접에서 이 질문에 좀 더 잘 대답할 수 있게됩니다. 당연한 소리지만, 면접이 개선이 되는것입니다! 그것도 아주 눈에 띄는 방향으로요. 그리고 여기서 한걸음 더 나아가면 이 모든 일련의 과정들을 진행함으로써 대부분의 기업들이 찾고있는 인재상이 뭔지도 알 수 있게 되고, 내가 어떤 사람인지도 알 수 있게 되고, 앞으로 내가 어떤 방향으로 나아가야 되는지도 알 게 됩니다. 개인적으로 저는 이 과정이 저를 엄청나게 성장시켜 주었다고 생각합니다. 예시를 한번 들어볼까요. 제가 면접에서 받았던 질문들을 예로 들어보면 알고리즘을 풀어보세요, A 아세요? B 아세요? 라는 단순 시험 형태의 질문보다는 나의 과거에 대해 묻는 질문이 더 많았습니다.(물론 기술적인 질문도 많았습니다…) 일단 제 과거를 수집하는 질문들로 부터 시작해서, “저번 프로젝트에서 썼던 기술은 뭔가?” “A라는 기술을 썼던데, 어떤식으로 활용했나?” “저번 프로젝트에서 힘들었던 점은 뭔가?” “왜” 라는 질문으로 이어지기 시작합니다. “그 기술을 왜 쓴건가?” “그 기술을 통해 얻는 장점이 뭔가?” 그리고 얼마나 주도적이었는가도 물어봅니다. “그러한 문제를 해결해보려고 너는 어디까지 해보았는가?” “구체적인 사례를 말해주겠나?” 물론 위 뿐 아니라 여러가지 유형들의 질문이 있었습니다. 하지만 대체적으로 저런 질문들이 핵심이었던것 같네요. 그리고 이런류의 질문을 집에와서 정리하고, 내 과거에 추가적으로 질문을 던지다보면, 앞으로 내가 어떤 사람이 되어야 하겠구나, 앞으로 어떻게 일을 해야겠구나 라는 것이 굉장히 명확해집니다. 이러한 기쁨!!과 최종 결과에 대한 초조함을 이끌고 약 한달정도 면접을 진행했던 것 같네요. ㅎㅎ 그래서 결과는? 저는 총 15군데 정도 지원했고, 12군데 정도에 면접을 보았으며, 5군데 정도에 최종 합격을 하였습니다. 제 생각보다 훨씬 큰 결과였고, 그중에 저를 정말 마음에 들어하셨던 곳도 있었습니다… 그리고 그 중 제가 가장 가고싶었던 기업에 최종 입사를 하게 되었고, 현재 입사한지 1달이 다 되어 가는 상황이네요. 개인적으로 제가 원하던 요구사항도 다 실현이 되었고… 옆에 계신 분들도 다 너무 뛰어나셔서 즐거운 마음으로 회사를 다니고 있습니다 ㅎㅎ (기술 스택도 높고, 출퇴근도 자유고, 밥값도 주고, 운동도 공짜로 하게 해주고…!!!) 벌써 3번째 회사이긴 하지만, 개인적으로 이직을 잘 한것 같아 매우 만족합니다 😊 😊 음… 마무리… 네… 음… 마무리 하겠습니다. 어떻게 쓰다보니 요우님의 포스트 포멧을 많이 따라한 것 같네요 ㅋㅋㅋㅋ (갠적으로 정말 잘 읽었습니다) 이직은 정말 중요한 행위이다. 이직을 준비하는 과정 자체가 개인을 굉장히 성장시킨다 그래서 나는 성장했다!! 이제 앞으로 더 성장할테다!! 뭐 덜 쓴 부분이 있나…? 급히 써서… 하지만 이 포스트가 누군가에게 아주 조금이라도 도움이 되었으면 좋곘습니다. 끝!]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>이직</tag>
        <tag>개발자 이직</tag>
        <tag>개발자 이력서</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] transaction]]></title>
    <url>%2Fdb%2Ftransaction%2F</url>
    <content type="text"><![CDATA[트랜잭션이란 작업을 논리적 단위로 묶어서 원자성(atomic)을 보장하고자 할 때 사용하는 것이다. 원자성 : 논리적 작업 셋에 1개의 쿼리가 있든 2개 이상의 쿼리가 있든 관계없이 논리적인 작업 셋 자체가 100% 적용되거나 적용되지 않는것을 보장해주는 것 MySQL에서는 InnoDB만 지원하고, 나머지(MyISAM 등)은 트랜잭션을 지원하지 않는다. 아래와 같은 쿼리가 있다(3번째 멤버가 중복된 이름이라 에러가 발생할 것이다) 1INSERT INTO MEMBER VALUES(1,'joont'),(2,'junyoung'),(3,'duplicated name'); MyISAM 쿼리 실행시에 오류가 발생하지만 3번 멤버만 저장되지 않을 뿐, 1,2번 멤버는 롤백되지 않고 저장된 상태 그대로 있다. 이러면 나중에 데이터 정합성을 맞추기가 매우 힘들어진다. InnoDB 1,2,3번 모두 저장되지 않는다. 위처럼 꼭 여러개의 작업 단위를 묶지 않아도 된다. 트랜잭션이 성공적으로 수행되면 모두 commit되고, 실패하면 모두 rollback 한다는 사실이 중요한 것이다. 설정법 12345START TRANSACTION; -- 또는 BEGIN;-- do somthingCOMMIT; -- 또는 ROLLBACK; 위처럼 트랜잭션을 시작하고, 커밋이나 롤백하는 시점까지가 하나의 트랜잭션이다. AutoCommit이 설정되어 있으면 쿼리문장 하나하나가 곧 트랜잭션이고, AutoCommit이 설정되어 있지 않으면 COMMIT, ROLLBACK, DDL 이후 시작부터 다시 이 문장을 실행 전까지가 자동적으로 트랜잭션 하나로 잡힌다. 나는 AutoCommit을 off로 설정하면 트랜잭션 테스트가 잘 안되서 AutoCommit을 on으로 설정한 뒤 테스트했다(on은 항상 트랜잭션이 끝남을 보장해주니까) 트랜잭션 설정 주의점 트랜잭션의 단위는 특별한 이유가 없으면 최소한의 단위로 해주는 것이 좋다. 가령 아래와 같은 트랜잭션 설정은 위험하다. 1234567891011121. 처리 시작 ==&gt; DB 커넥션 생성 ==&gt; START TRANSACTION;2. 인가된 사용자인지 확인3. 사용자가 작성한 컨텐츠 Validation4. 첨부파일 서버에 업로드 5. 게시글과 첨부파일 내용을 DB에 저장 6. 게시물 등록에 대한 메일 발송 7. 메일 발송 내역을 DB에 저장 ==&gt; COMMIT;==&gt; DB 커텍션 반납 8. 처리 완료 보다시피 트랜잭션의 범위가 너무 크다. DB에 데이터를 저장하는 구간은 5번과 7번밖에 없는데 인증, Validation, 첨부파일 업로드, 메일 전송과 같은 행위까지 전부 트랜잭션에 같이 포함되어 있다. 즉, 위의 모든 작업들이 끝나기 까지 해당 트랜잭션을 잡고있는 커넥션은 반환되지 않을 것이며, 소유하는 시간이 길어질수록 다른 쓰레드들이 커넥션을 가져가지 못하고 대기하는 상황이 벌어질 것이다. (특히 첨부파일 업로드, 메일 전송과 같은 네트워크 작업을 트랜잭션으로 묶는것은 아주 위험하다. 네트워크 통신이 불가능하다면 트랜잭션 소유시간이 매우 길어지게 된다) 커넥션 뿐만 아니라 CUD가 발생했을 경우 해당 레코드에 대해서 메서드가 끝날때까지 락이 걸리기 때문에 동시처리 성능 또한 매우 떨어지게 된다. 아래와 같이 변경해주는 것이 좋다. 12345678910111213141. 처리 시작 ==&gt; DB 커넥션 생성 2. 인가된 사용자인지 확인3. 사용자가 작성한 컨텐츠 Validation4. 첨부파일 서버에 업로드 ==&gt; START TRANSACTION;5. 게시글과 첨부파일 내용을 DB에 저장 ==&gt; COMMIT;6. 게시물 등록에 대한 메일 발송 ==&gt; START TRANSACTION;7. 메일 발송 내역을 DB에 저장 ==&gt; COMMIT;==&gt; DB 커텍션 반납 8. 처리 완료 트랜잭션을 최소화하는 것이 좋다는 것을 강조하기 위해 7번을 따로 분리하였다. 상황에 따라 크게 달라질 수 있다.]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>트랜잭션</tag>
        <tag>트랜잭션 설정법</tag>
        <tag>트랜잭션 범위</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] git rebase]]></title>
    <url>%2Fgit%2Fgit-rebase%2F</url>
    <content type="text"><![CDATA[https://jupiny.com/2018/05/07/git-rebase-i-option/ -i : interactive mode(대화형) pick : 커밋 그대로 사용 reword : 커밋 메시지 변경 edit : 커밋 메시지 + 작업 내용 변경 squash : 이전 커밋과 합침 fix : squash 처럼 이전 커밋과 합치는데, 커밋 메시지는 합치지 않고 이전 커밋 메시지를 그대로 사용]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>rebase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] MessageConverter]]></title>
    <url>%2Fspring%2FMessageConverter%2F</url>
    <content type="text"><![CDATA[앞서 우리가 HTTP 요청을 모델에 바인딩하고 클라이언트에 보낼 HTTP 응답을 만들기 위해 뷰를 사용했던 방식과는 달리, HTTP 요청 본문과 HTTP 응답 본문을 통째로 메세지로 다루는 방식이다. 주로 XML이나 JSON을 이용한 AJAX 기능이나 웹 서비스를 개발할 때 사용된다. 아래와 같이 스프링의 @RequestBody와 @ResponseBody를 통해 구현할 수 있다. 12345@ResponseBody // 응답@RequestMapping(value= "/hello", method=RequestMethod.POST)public String hello(@RequestBody String param)&#123; // 요청 return "result";&#125; 위와 같은 애노테이션을 명시해두게 되면 스프링은 메세지 컨버터라는 것을 사용하여 HTTP 요청이나 응답을 메세지로 변환하게 된다. 즉 위처럼 파라미터 부분에 @RequestBody를 입력할 경우, 파라미터 타입에 맞는 메세지 컨버터를 선택한 뒤 HTTP 요청 본문을 통째로 메세지로 변환하여 파라미터에 바인딩하는 것이다. 메서드의 상단에 @ResponseBody를 입력할 경우 또한 마찬가지로 리턴 타입에 맞는 메세지 컨버터를 선택한 뒤 리턴 값을 통째로 메세지로 변환한 뒤 리턴해주는 것이다. 참고로 GET 방식의 요청일 경우 HTTP 요청 본문이 없으므로 @RequestBody를 사용할 수 없다. @RequestParam이나 @ModelAttribute를 사용해야 한다. 메세지 컨버터의 종류 이렇게 사용되는 메세지 컨버터는 AnnotationMethodHandlerAdapter를 통해 등록할 수 있고, 이미 디폴트로 4가지의 메세지 컨버터가 등록되어 있다. 아래는 디폴트 메세지 컨버터들이다. ByteArrayHttpMessageConverter 지원하는 오브젝트 타입은 byte[]이고, 미디어타입은 모든 것을 다 지원한다. 즉 파라미터에 @RequestBody byte[] param과 같이 작성하면 모든 요청을 다 byte배열로 받을 수 있다는 말이다. 그리고 리턴타입을 byte[]로 했을 경우 Content-Type이 applcation/octet-stream으로 설정되어 전달된다. 바이너리 정보를 주고받을 경우가 아니라면 그닥 유용해 보이진 않는다. StringHttpMessageConverter 지원하는 오브젝트 타입은 String이고, 미디어타입은 모든 것을 다 지원한다. 파라미터에 사용할 경우 HTTP 본문을 그대로 String으로 가져올 수 있게되고, 리턴에 사용할 경우 단순 문자열을 그대로 전달해줄 수 있다. Content-Type은 text/plain으로 전달된다. FormHttpMessageConverter 지원하는 오브젝트 타입은 MultiValueMap&lt;String, String&gt;이고, 미디어타입은 application/x-www-form-urlencoded만 지원한다. 즉 정의된 폼 데이터를 주고받을 때 사용할 수 있다는 말인데, 폼 데이터는 @ModelAttribute를 사용하는 것이 훨씬 유용하므로 이것 또한 자주 사용할 일은 없다. SourceHttpMessageConvreter 지원하는 오브젝트 타입은 DomSource, SAXSource, StreamSource이고, 미디어타입은 application/xml, application/*+xml, text/xml 세가지를 지원한다. XML 문서를 Source 타입의 오브젝트로 변환하고 싶을 떄 사용할 수 있다. 하지만 요즘은 OXM 기술이 많이 발달되었으므로 이 또한 잘 쓰이지 않는다. 이제 아래는 디폴트가 아닌 메세지 컨버터들이다. 실제로 이 컨버터들이 더 유용하다. 여기서 필요한게 있다면 직접 AnnotationMethodHanlderAdapter의 messageConverters에 등록하고 사용해야 한다. 12345678&lt;bean class="org.springframework...AnnotationMethodHandlerAdapter"&gt; &lt;property name="messageConverters"&gt; &lt;list&gt; &lt;bean class="org.springframework.http.converter.json.MappintJacksonHttpMessageConverter" /&gt; &lt;bean class="org.springframework.http.converter.xml.Jaxb2RootElementHttpMessageConverter" /&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 다른 전략과 마찬가지로 위와 같이 등록 시 디폴트 전략이 모두 무시된다는 점에 주의해야 한다. Jaxb2RootElementHttpMessageConverter JAXB의 @XmlRootElement와 @XmlType이 붙은 클래스를 이용해 XML과 오브젝트 사이의 메세지 변환을 지원한다. 지원하는 미디어 타입은 SourceHttpMessageConvreter와 동일하다. MashallingHttpMessageConverter 스프링 OXM 추상화의 Mashaller와 Unmarshaller를 이용해서 XML과 오브젝트 사이의 변환을 지원한다. 이 컨버터를 등록할 때는 marshaller와 unmarshaller를 설정해줘야 한다. 지원하는 미디어 타입은 SourceHttpMessageConvreter와 동일하다. MappingJacksonHttpMessageConverter Jackson의 ObjectMapper를 이용해서 JSON과 오브젝트 사이의 변환을 지원한다. 지원하는 미디어 타입은 application/json이다. 변환하는 오브젝트 타입의 제한은 없지만 프로퍼티를 가진 자바빈 스타일이나 HashMap을 이용해야 정확한 변환 결과를 얻을 수 있다. From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>토비의 스프링</tag>
        <tag>MessageConverter</tag>
        <tag>@RequestBody</tag>
        <tag>@ResponseBody</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] 실행계획]]></title>
    <url>%2Fdb%2F%EC%8B%A4%ED%96%89%EA%B3%84%ED%9A%8D%2F</url>
    <content type="text"><![CDATA[모든 일에는 계획이 필요하다. 일을 하든… 여행을 가든… 그리고 우리는 그 일을 처리하기 위한 여러가지 계획을 세우고, 그 중에서 어떤 방식이 최적이고 최소의 비용이 소모되는지를 결정하게 된다. 이는 DBMS도 마찬가지이다. 옵티마이저는 쿼리를 실행하기전 여러가지 통계정보를 참조하여 최적의 계획을 세우고, 그 계획대로 쿼리를 실행한다. 옵티마이저 SQL을 가장 빠르고 효율적으로 수행할 최적의 경로를 생성하는 DBMS 내부 핵심엔진이다. 즉 데이터베이스 서버에서 두뇌와 같은 역할을 담당한다. 옵티마이저의 최적화 방법으로는 규칙 기반 최적화(RBO)와 비용 기반 최적화(CBO)가 있는데, 현재는 거의 대부분의 DBMS에서 비용 기반 최적화를 사용하고 있다. MySQL 또한 마찬가지이다. 통계 정보 비용 기반 최적화에서 실행계획 수립 시 가장 중요하게 사용되는 정보이다. 통계 정보가 정확하지 않으면 전혀 엉뚱한 방향으로 쿼리를 실행해 버릴 수 있기 때문이다. 이 통계정보는 ANALYZE 라는 명령어를 사용해 직접 갱신할 수 있는데, MySQL의 경우 사용자가 알아채지 못하는 사이에 자동으로 계속 변경되기 때문에 직접 수동으로 갱신할 일은 별로 없다. (하지만 레코드 건수가 작으면 통계 정보가 부정확할 때가 많음) InnoDB의 경우 ANALYZE를 실행하는 동안 읽기와 쓰기가 모두 불가능하므로 서비스 도중에는 실행하지 않는것이 좋다. 쿼리 실행절차 MySQL 서버에서 쿼리가 실행되는 과정은 크게 3가지로 나눌 수 있다. 요청받은 SQL 문장을 잘게 쪼개서 MySQL 서버가 이해할 수 있는 수준으로 분리한다. 위에서 생성된 SQL 파싱 정보(파스 트리)를 확인하면서 어떤 테이블부터 읽을지, 어떤 인덱스를 이용할지 선택한다. 위에서 선택된 순서와 인덱스를 이용해 스토리지 엔진으로부터 데이터를 가져온다. 1번 단계를 SQL 파싱이라고 하고, MySQL 서버의 SQL 파서라는 모듈로 처리한다. 이 단계에서 만들어진 것을 SQL 파스 트리라고 한다. 2번 단계에서는 1번 단계에서 생성된 SQL 파스 트리를 참조하여 옵티마이저에서 다음 내용을 처리한다. 불필요한 조건 제거 및 복잡한 연산 단순화 조인이 있는 경우, 어떤 순서로 읽을 지 결정 조건과 통계정보를 참조해 사용할 인덱스 결정 임시테이블을 통해 다시 가공해야 하는지 결정 3번 단계에서는 2번 단계에서 수립된 실행 계획대로 스토리지 엔진에 레코드를 읽어오도록 요청하고, 받은 레코드를 MySQL 엔진이 조인하거나 정렬하는 작업을 수행한다. MySQL은 스토리지 엔진과 MySQL 엔진으로 구분된다. 스토리지 엔진은 디스크나 메모리상에서 필요한 레코드를 읽거나 저장하는 역할을 하며, MySQL 엔진은 스토리지 엔진으로부터 받은 레코드를 가공/연산하는 작업을 수행한다. 보다시피 1,2번 단계는 거의 MySQL 엔진에서 처리하며, 3번 단계는 MySQL 엔진과 스토리지 엔진이 동시에 참여해서 처리한다. 실행계획 분석 쿼리에 EXPLAIN이라는 명령어를 추가로 사용하면 MySQL이 수립한 실행계획을 직접 볼수있다. 아래는 실행계획의 예시이다. 표의 각 라인은 사용된 테이블의 개수(임시 테이블 포함)이고, 실행순서는 대체적으로 위에서 아래로 진행된다. 참고로 실행계획은 SELECT 문만 확인 가능하며, DML 문장의 실행계획을 확인하고 싶으면 WHERE 조건절만 같은 SELECT 문을 만들어서 대략적으로 확인해보는 수 밖에 없다. id SELECT 단위 쿼리별로 부여되는 식별자 값이다. 하지만 만약 JOIN을 했을 경우, 레코드는 테이블의 개수만큼 출력되지만 id는 동일하게 부여된다. JOIN 시 먼저(윗 라인)에 표시된 테이블이 드라이빙 테이블, 이후에 표시된 테이블이 드리븐 테이블이 된다. select_type 각 단위 SELECT가 어떤 타입의 쿼리인지 표시하는 칼럼이다. SIMPLE UNION이나 서브 쿼리를 사용하지 않는 단순한 SELECT 쿼리인 경우 표시된다. PRIMARY UNION이나 서브 쿼리가 포함된 SELECT 쿼리의 실행계획에서 가장 바깥쪽에 있는 단위쿼리인 경우 표시된다. UNION UNION이나 UNION ALL로 결합하는 단위 SELECT 쿼리들 중 첫번째를 제외한 두번째 이후부터 표시된다. 첫번째 레코드에는 UNION 대신 DERIVED가 표시된다. 조회된 결과를 UNION으로 결합해 임시테이블을 만들어 사용하기 떄문이다. DEPENDENT UNION UNION/UNION ALL을 사용하는 단위쿼리가 Outer 쿼리에 의해 영향을 받을 경우 표시된다. 12345678910EXPLAIN SELECT e.first_name,(SELECT CONCAT('Salary change count : ', COUNT(*)) AS message FROM salaries s WHERE s.emp_no=e.emp_noUNIONSELECT CONCAT('Department change count : ', COUNT(*)) AS message FROM dept_emp de WHERE de.emp_no=e.emp_no) AS messageFROM employees e; 예외가 조금 억지스럽긴 하다… 보다시피 UNION에서 Outer 쿼리의 emp_no 칼럼을 이용했기 때문에 DEPENDENT UNION이 표시되고 있다. 위와 같은 형태를 서브 쿼리라고 하는데, 일반적으로 서브 쿼리는 Outer 쿼리보다 먼저 실행되며, 속도도 빠르게 처리된다. 하지만 위와 같이 Outer 쿼리에 의존적인 서브쿼리, 즉 DEPENDENT 형태의 경우 절대 Outer 쿼리보다 먼저 실행될 수 없다. 그래서 DEPENDENT 실행계획이 포함된 쿼리는 비효율적인 경우가 많다. SUBQUERY 하나의 단위쿼리가 다른 단위쿼리를 포함했을 경우 이를 서브쿼리 라고 하는데, SUBQUERY select_type은 FROM절 이외에서 사용되는 서브쿼리만을 의미한다. FROM절에 사용된 서브쿼리는 select_type이 DERIVED로 표시된다. DEPENDENT SUBQUERY DEPENDENT UNION과 같이 서브쿼리가 Outer 쿼리에 정의된 컬럼을 사용하는 경우 표시된다. 이 또한 일반 서브쿼리보다 처리속도가 느린 경우가 많다. DERIVED 단위 SELECT 쿼리의 실행 결과를 메모리나 디스크의 임시 테이블을 생성하여 저장할 때 표시된다. MySQL은 FROM절에 사용된 서브쿼리를 제대로 최적화하지 못할 경우가 대부분이다.(인덱스가 전혀 없으므로) 그에 비해 MySQL 5.0 이후로는 조인이 상당히 최적화 된 편이므로, FROM 서브쿼리 대신 조인을 사용하는 것이 좋다. UNCACHEABLE SUBQUERY 옵티마이저는 조건이 똑같은 서브쿼리의 실행결과는 내부적인 캐시 공간에 담아둔 뒤 다시 사용하며 성능을 향상시킨다. SUBQUERY와 DEPENDENT SUBQUERY가 캐시를 사용하는 방법은 다음과 같다. SUBQUERY : Outer 쿼리의 영향을 받지 않으므로 처음 한번만 실행해서 결과를 캐시하고, 필요할 떄 이용한다. DEPENDENT SUBQUERY : Outer 쿼리 컬럼의 값 단위로 캐시해두고 사용한다. UNCACHEABLE SUBQUERY의 경우 캐시를 하지 못하는 경우 표시되는데, 이유는 다음과 같다. 시용자 변수가 서브쿼리에 포함된 경우 NOT_DETERMINISTIC 속성의 스토어드 루틴이 서브쿼리에 사용된 경우 UUID()나 RAND() 같이 호출할 때 마다 달라지는 함수가 서브쿼리에 사용된 경우 UNCACHEABLE UNION 위와 동일하게 UNION 결과를 캐시할 수 없을 경우 사용된다. table MySQL의 실행계획은 SELECT 쿼리 기준이 아니라 테이블 기준으로 표시된다. alias(별칭)를 사용했을 경우 alias가 표시되고, 테이블을 사용하지 않았을 경우 NULL이 표시된다. 그리고 테이블 이름이 &lt; &gt;같이 둘러싸였을 경우, 임시테이블을 의미한다. type MySQL 서버가 각 테이블의 레코드를 어떤 방식으로 읽었는지를 표시해준다. 이 컬럼을 통해 인덱스를 사용했는지, 테이블을 풀 스캔했는지 등을 확인할 수 있다. 인덱스를 효율적으로 사용하는 것은 매우 중요하므로, 이 컬럼은 꼭 확인해야 할 정보이다. 아래는 MySQL에서 부여한 접근속도 순위이다. ALL 타입만 빼고 모두 인덱스를 사용하는 방식이다. system 레코드가 1건만 존재하거나 1건도 존재하지 않는 테이블을 참조할 떄 표시된다. 이는 InnoDB에서는 나타나지 않고, MyISAM이나 MEMORY 테이블에서 사용되는 접근 방식이다. const 테이블 레코드 건수에 관계없이 WHERE 조건절에서 프라이머리 키나 유니크 키 컬럼을 사용하며, 반드시 1건만 반환할 경우 표시된다. 1234EXPLAIN SELECT *FROM employeesWHERE emp_no = 10001; 다중컬럼으로 구성된 프라이머리 키나 유니크 키의 일부 컬럼만 사용할 경우 const 타입의 접근 방법을 사용할 수 없다. eq_ref 여러 테이블이 조인되는 쿼리의 실행계획에서만 표시된다. 조인에서 처음 읽은 테이블의 컬럼 값을 그 다음 읽어야 할 테이블의 프라이머리 키나 유니크 키 컬럼 검색 조건에 사용하고, 그로 인해 두번쨰 테이블에서 출력되는 레코드가 반드시 1건이라는 보장이 있을 경우 표시된다. 12345678EXPLAIN SELECT *FROM dept_emp de INNER JOIN employees e-- 드라이빙 테이블에서 읽은 컬럼을 프라이머리 키 조건에 사용ON e.emp_no = de.emp_no -- employees에서 반드시 1건만 나오게 보장WHERE de.dept_no = 'd005'; ref 인덱스를 Equal 조건으로 검색할 때 사용된다. 조인의 순서와 관계없고, 프라이머리 키나 유니크 키 등의 제약조건도 없다. 반환되는 레코드가 반드시 1건이라는 보장이 없으므로 const나 eq_ref 보다는 느리나, 기본적으로 매우 빠른 조회방법 중 하나이다. ref_or_null ref와 같은데 NULL 비교가 추가된 형태이다. 1234EXPLAIN SELECT *FROM titlesWHERE to_date = '1985-03-01' OR to_date IS NULL; 실무에서 별로 사용되지 않으므로 이 정도만 기억해도 된다. unique_subquery WHERE 조건절에서 사용될 수 있는 IN (subquery) 형태의 쿼리를 위한 접근 방식이다. 서브쿼리에서 중복되지 않은 유니크한 값만 반환될 때 표시된다. index_subquery IN 연산자의 특성상 괄호안에 있는 값의 목록에는 중복이 먼저 제거되어야 한다. index_subquery의 경우 서브쿼리가 중복된 값을 반환할 수 있지만, 인덱스를 이용해 중복을 제거할 수 있을때 표시된다. range 인덱스를 하나의 값이 아니라 범위로 검색하는 경우에 표시된다. 범위 검색 연산자의 경우 &lt;, &gt;, IS NULL, BETWEEN, IN, LIKE 등이 있다. index_merge 2개 이상의 인덱스를 이용해 각각의 검색 결과를 만들어 낸 후, 이를 병합하여 처리하는 방식이다. 12345EXPLAIN SELECT *FROM employeesWHERE emp_no BETWEEN 10001 AND 11000 -- primary key로 조회OR first_name = 'Smith'; -- first_name index로 조회 하지만 index_merge의 경우 이름처럼 효율적으로 작동하는 경우가 그렇게 많지는 않다. index 이름만 보면 아주 좋아보이나, 실제로는 인덱스를 처음부터 끝까지 읽는 인덱스 풀 스캔을 의미한다. 풀 테이블 스캔과 읽는 레코드 수는 같으나, 인덱스가 일반적으로 데이터 파일 전체보다는 크기도 작고 정렬도 되어있으므로 풀 테이블 스캔보다는 빨리 처리된다. 이 방식은 다음의 조건을 충족할 떄 표시된다. range, const, ref와 같은 방식으로 인덱스를 이용하지 못하는 경우 인덱스에 포함된 컬럼으로만 처리할 수 있는 경우 인덱스를 이용해 정렬이나 그룹핑이 가능할 경우 ALL 풀 테이블 스캔을 의미한다. 테이블을 처음부터 끝까지 다 읽는 방식으로, 가장 비효율적인 방법이다. possible_keys 옵티마이저가 최적화 된 실행계획을 만들기 위해 후보로 선정했던 인덱스의 목록이다. 즉, “사용될 뻔 했던 인덱스 목록” 이므로, 아무 도움도 되지 않는다. 그냥 무시하자. key possible_keys와 달리 최종 실행계획에서 선택된 인덱스를 의미한다. 그러므로 쿼리 튜닝 시 의도했던 인덱스가 표시되는지 이곳을 통해 확인하는 것이 중요하다. 2개 이상의 인덱스가 사용될 경우 ,로 구분되어 표시된다. 프라이머리 키의 경우 PRIMARY KEY라는 이름으로 표시된다. key_len 실제 업무에서는 단일 컬럼 인덱스보다 다중 컬럼으로 만들어진 인덱스가 더 많은데, key_len은 쿼리를 처리하기 위해 다중 컬럼으로 구성된 인덱스에서 몇 개의 컬럼까지 사용했는지 알려준다. 정확히는 몇 바이트까지 사용했는지 알려준다. 1234EXPLAIN SELECT * FROM dept_emp where emp_no='10001'; PRIMARY KEY의 4바이트만을 이용했다고 표시되고 있다. emp_no은 INTEGER 타입으로써 저장공간으로 4바이트를 사용한다. 즉, 복합컬럼 인덱스 중 emp_no 컬럼만을 사용했음을 나타낸다. ref Equal 비교 조건으로 어떤 값이 제공되었는지 표시해준다. 일반적으로 이 컬럼은 크게 신경쓰지 않아도 되는데, 컬럼에 func라고 표시될때는 조금 주의해서 살펴봐야 한다. 이는 Function의 줄임말으로 값을 그대로 사용한게 아니라 변환이나 연산을 거친 뒤 값을 사용했다는 뜻이다. 12345EXPLAIN SELECT * FROM employees eINNER JOIN dept_emp deON e.emp_no = (de.emp_no-1); -- 연산 근데 중요한 점은, 위처럼 명시적으로 변환할 때 뿐만 아니라 MySQL 서버가 내부적으로 값을 변경할떄도 func가 출력된다는 점이다. 타입이 일치하지 않는 두 컬럼을 비교할때가 대표적이다. 가능하다면 이런 내부 연산이 발생하지 않도록 타입을 맞춰주는 것이 좋다. rows 해당 쿼리를 처리하기 위해 얼마나 많은 레코드를 디스크로부터 읽고 체크해야 하는지를 의미한다. 이는 통계 정보를 참조해 옵티마이저가 산출한 값이라서 정확하지는 않다. 아래는 rows 컬럼이 실행계획에 영향을 끼친 예시이다. 1234EXPLAINSELECT * FROM dept_empWHERE from_date &gt;= '1985-01-01'; 보다시피 해당 쿼리를 처리하기 위해서는 331,143 개의 레코드를 읽어야 한다고 예측했다. 하지만 dept_emp 테이블의 전체 레코드 개수가 331,603개로, 거의 차이가 나지 않는다. 그래서 옵티마이저는 풀 테이블 스캔이 낫다고 판단하여 ALL로 처리된것을 볼 수 있다. 1234EXPLAINSELECT * FROM dept_empWHERE from_date &gt;= '2002-07-01'; 예측되는 rows를 줄였을 경우 range가 출력됨을 볼 수 있다. Extra 이름과는 달리 실행계획에서 성능에 중요한 내용이 여기 자주 표시된다. 여기에 표출되는 고정된 몇개의 문장들이 있고, 일반적으로 2~3개씩 같이 표시된다. Distinct 아래는 departments 테이블과 dept_emp 테이블에 모두 존재하는 dept_no을 중복없이 가져오기 위한 쿼리이다. 1234EXPLAINSELECT DISTINCT d.dept_noFROM departments dINNER JOIN dept_emp de ON de.dept_no = d.dept_no; 위처럼 Distinct가 출력되면 실제로 아래와 같이 효율적으로 처리됨을 의미한다. DISTINCT 처리를 위해 조인하지 않아도 되는 항목은 무시하고 꼭 필요한 레코드만 읽고 있다. Full scan on NULL key col1 IN(SELECT col2 FROM ...) 형태의 쿼리에서 자주 발생할 수 있는 형태이다. 만약 col1의 값이 NULL이 된다면 결과적으로 NULL IN(SELECT col2 FROM ...)의 형태가 되게 되는데, 이 때 서브쿼리에 대해 풀 테이블 스캔이 발생하게 되고(이유를 정확히 모르겠다…), 이로 인해 상당한 성능저하가 발생하게 된다. 즉 이 메세지는 col1이 NULL을 만나면 풀 테이블 스캔을 사용할 것이라고 알려주는 키워드인 것이다. 만약 col1이 NOT NULL로 정의되었다면 이 메세지는 표시되지 않을 것이다. Impossible HAVING HAVING절의 조건을 만족하는 레코드가 없을 때 표시된다. 쿼리를 잘못 작성한 경우가 대부분이지만, 실제 저장된 데이터 때문에 발생하는 경우도 종종 있다. 쿼리와 데이터를 다시 확인해 보는것이 좋다. Impossible WHERE WHERE절의 조건이 항상 FALSE가 될 수 밖에 없을 때 표시된다. Impossible WHERE noticed after reading const tables WHERE절의 조건이 항상 FALSE가 될 수 밖에 없는데, 테이블을 읽어본 뒤 알았다는 의미이다. 아래와 같은 쿼리가 이에 해당한다. 1234EXPLAINSELECT *FROM employees WHERE emp_no = 0; 이를 통해 실행계획을 만드는 과정에서 옵티마이저가 쿼리의 일부분을 실행해 본다는 사실을 알 수 있다. No matching min/max row MIN()이나 MAX()와 같은 집합 함수가 있는 쿼리의 조건절에 일치하는 레코드가 하나도 없을 때 표시된다. 1234EXPLAINSELECT MIN(dept_no), MAX(dept_no)FROM dept_empWHERE dept_no = ''; No tables used FROM절 자체가 없거나, 상수 테이블을 의미하는 DUAL테이블을 사용할 때 표시된다. Not exists Outer Join을 이용해서 Anti-Join을 수행할 경우 표시된다. Anti-Join A 테이블에는 존재하지만 B 테이블에는 존재하지 않는 값을 조회할 떄 사용하는 기법이다. 일반적으로 NOT IN, NOT EXIST, Outer Join을 통해서 처리하는데 레코드의 건수가 많을 때는 Outer Join이 빠르다. 123456EXPLAINSELECT *FROM departments dLEFT JOIN dept_emp deON d.dept_no = de.dept_noWHERE de.dept_no IS NULL; Range checked for each record (index map: N) 다음과 같은 쿼리가 있다고 하자. 1234EXPLAIN SELECT *FROM employees e1INNER JOIN employees e2 ON e1.emp_no &lt;= e2.emp_no; 레코드를 하나씩 읽을 때 마다 e1.emp_no의 값이 변경되기 때문에 옵티마이저 입장에서는 e2를 인덱스 레인지 스캔으로 읽을지, 풀 테이블 스캔으로 읽을 지 판단하지 못한다. 만약 employees 테이블의 레코드가 1억건이라고 가정했을때, e1.emp_no이 1인 경우는 e2의 모든 레코드를 읽어야하지만, e1.emp_no이 100,000,000인 경우에는 e2의 레코드를 1건만 읽으면 된다. 즉 e1.emp_no이 작은 값일 때는 풀 테이블 스캔이 좋고, 큰 값일 때는 인덱스 레인지 스캔이 좋다. 이 현상을 줄여서 얘기하면 매 레코드 마다 인덱스 레인지 스캔을 체크한다 라고 할 수 있는데, 이게 바로 Range checked for each record 문구인것이다. 참고로 이 문구가 표시될 때 ref 컬럼의 값이 ALL로 표출되는데, 이는 인덱스 사용여부를 검토하고 풀 테이블 스캔을 할 수 있기 때문에 ALL로 표시된것이지 실제로 풀 테이블 스캔을 의미하는 것은 아니다. 그리고 뒤에 (index map: N) 이라는 문구가 추가로 출력되는데, 이는 사용할 인덱스의 후보를 나타내준다. 만약 (index map: 0x19)라는 문구가 표시되었다고 가정하자. 일단 16진수인 0x19를 2진수로 변경해줘야 한다. 11001이 된다. 그리고 생성된 인덱스의 순서를 보기위해 SHOW CREATE TABLE table_name을 입력한다. 그러면 테이블이 가지고 있는 인덱스를 순서대로 확인이 가능한데, 이 순서를 위의 2진수로 체크하면 된다. 즉 현재 11001이라는 값을 얻었으므로, 1번째, 2번째, 5번째 순서에 나열된 인덱스가 사용후보가 된다는 것이다. 매 레코드를 돌면서 위 3개의 사용후보들 가운데 어떤 인덱스를 사용할지 결정하게 되는데, 실제로 어떤 인덱스를 사용했는지는 알 수 없다. Sanned N databases MySQL에는 서버 내에 존재하는 DB의 메타정보를 담은 INFORMATION_SCHEMA라는 DB가 제공되는데, 이 데이터를 읽었을 떄 표시된다. Select table optimized away MIN()이나 MAX() 사용 시 인덱스를 다 읽지 않고 오름차순 또는 내림차순으로 1건만 읽는 형태의 최적화가 적용될 때 표시된다. 1234EXPLAINSELECT MAX(from_date), MIN(from_date)FROM salariesWHERE emp_no = 10001; emp_no과 from_date로 복합 프라이머리 키가 설정되어 있으므로 emp_no이 10001인 레코드를 찾은 뒤 첫 행과 마지막 행만을 읽으면 된다. unique row not found Not exists의 반대라고 볼 수도 있겠다. Outer Join을 수행하는 쿼리에서 아우터 테이블에 조건 조건에 일치하는 레코드가 없을 때 표시된다. Using filesort ORDER BY가 사용된 쿼리에서만 나타날 수 있다. ORDER BY 처리에 인덱스를 사용하지 못했을 경우 나타난다. 인덱스를 사용하지 못할 경우 조회된 레코드를 메모리 버퍼에 복사한 뒤 퀵 소트 알고리즘을 수행한다. 이는 많은 부하를 일으키므로 가능하면 쿼리를 튜닝하거나 인덱스를 생성하는 것이 좋다. Using index 데이터 파일을 전혀 읽지 않고 인덱스만 읽어서 쿼리를 처리할 수 있을 떄 표시된다. 참고로 InnoDB의 테이블은 모두 클러스터링 인덱스로 구성되어 있어서 모든 보조 인덱스들은 데이터의 레코드 주소 값으로 프라이머리 키 값을 가진다. 즉 아래와 같은 쿼리도 Using Index로 처리되는 효과를 낼 수 있다. 123EXPLAIN SELECT emp_no, first_name FROM employees; 현재 first_name 컬럼에 대해서만 인덱스가 생성되어 있지만 클러스터링 인덱스의 특징으로 Using index로 처리 가능하게 된다. Using index for group-by GROUP BY 처리를 위해서는 그룹핑 기준 컬럼을 이용해 정렬 작업을 수행한 뒤 그 결과를 그룹핑하는 고부하 작업을 필요로 한다. 하지만 GROUP BY 처리에 인덱스를 사용할 수 있으면 정렬된 인덱스 컬럼을 읽으면서 그룹핑 작업만 수행하면 된다. 이는 상당히 빠르게 처리되고, 이처럼 GROUP BY에 인덱스가 사용되었을 때 이 메세지가 표시된다. Using join buffer 일반적으로 조인이 되는 컬럼은 인덱스를 생성해야 빠른 처리를 할 수 있다. 이는 MySQL이 조인을 Nested loop 방식으로만 처리하기 때문이다. Nested loop join FROM절에 아무리 테이블이 많아도 조인을 수행할 때 반드시 2개의 테이블이 비교되는 방식으로 처리하는 것이다. 먼저 읽히는 테이블이 드라이빙 테이블이 되고, 뒤에 읽히는 테이블이 드리븐 테이블이 된다. 즉 드라이빙 테이블의 건수만큼 드리븐 테이블이 스캔되므로 드라이빙 테이블이 어떤 테이블이냐가 성능을 많이 좌우한다. 옵티마이저는 두 테이블을 조인할때 각 테이블의 조인 기준 컬럼에 인덱스가 있는지 조사하고, 인덱스가 없는 테이블이 있다면 그 테이블을 드라이빙 테이블로 지정하여 실행한다. 위에서 언급했듯이 드리븐 테이블은 계속해서 탐색되므로 인덱스가 없으면 성능에 영향을 많이 미치기 때문이다. 근데 만약 드리븐 테이블에도 인덱스가 없다면 매번 드리븐 테이블을 풀 테이블 스캔해야 하는데, MySQL에서는 이러한 비효율적인 검색을 보안하기 위해 조인 버퍼라는 것을 사용한다. 드라이빙 테이블에서 읽은 데이터를 조인 버퍼에 저장해두고, 필요할 때 마다 재사용할 수 있게 해준다. 조인 버퍼가 사용되는 실행계획에 Using join buffer 메세지가 표시된다. Using intersect, Using sort_union, Using union index_merge 방식이 사용될 때 두 인덱스의 결과를 어떻게 병합했는지 조금 더 상세하게 설명하기 위한 메세지이다. 출력되는 메세지는 아래의 3개이다. Using intersect : 각각의 인덱스를 사용하는 조건이 AND로 연결될 때 Using union : 각각의 인덱스를 사용하는 조건이 OR로 연결될 때 Using sort_union : Using union으로 처리하기 힘들 정도로 대량의 조건들이 OR로 연결될 때 Using temporary MySQL은 쿼리를 처리하는 동안 중간 결과를 담아두기 위해 임시 테이블을 사용한다. 이 메세지가 표시되면 임시 테이블을 사용했다는 의미인데, 사용된 임시테이블이 메모리에 생성되었는지 디스크에 생성되었는지는 알 수 없다. Using where Using where 메세지는 MySQL 엔진에서 별도의 가공을 해서 필터링 작업을 거쳤을 경우 표시된다. 실제로 가장 흔하게 표시되는 메세지이다. 그러나 이 메세지만으로 정확하게 성능 이슈를 판단하긴 어렵고, MySQL 5.1부터 추가된 Filtered 컬럼과 함께 보아야 성능샹 이슈를 쉽게 체크할 수 있다. Filtered 스토리지 엔진에서 받은 레코드가 MySQL 엔진을 거친 뒤 얼마나 남았는가를 체크해줄 수 있는 컬럼이다. 일반 실행계획에선 볼 수 없고, EXPLAIN EXTENDED라는 명령어를 사용해야 한다. 1234EXPLAIN EXTENDED SELECT * FROM employees WHERE first_name like '%Ab%'; 현재는 스토리지 엔진에서 전달받은 299,113건의 레코드에 대해 MySQL 엔진에서 필터링 된 것 없이 100% 출력되고 있음을 볼 수 있다. EXPLAIN EXTENDED의 추가 기능 EXPLAIN EXTENDED를 실행한 뒤 SHOW WARNINGS 명령을 실행하면 옵티마이저가 다시 재조합한 쿼리 문장을 확인 가능하다. 옵티마이저가 어떻게 쿼리를 해석하고 변환했는지 직접 확인할 수 있으므로 알아두면 도움이 된다. 스토리지 엔진과 mysql 엔진에서 읽어오는 데이터 양 차이의 기준은 뭘까? 참고 : 이성욱, 『Real MySQL』, 위키북스(2012)]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>Real MySQL</tag>
        <tag>MySQL 실행계획</tag>
        <tag>쿼리 실행절차</tag>
        <tag>실행계획 분석</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] 데이터 모델링]]></title>
    <url>%2Fdb%2F%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%AA%A8%EB%8D%B8%EB%A7%81%2F</url>
    <content type="text"><![CDATA[데이터 모델링은 DBMS 사용에 가장 중요한 부분이면서 가장 쉽게 간과되는 부분이기도 하다. 데이터 모델링은 크게 논리 모델링과 물리 모델링으로 나눌 수 있다. 제대로 표현되고 있는 곳은 잘 없지만, 원래 논리 모델링과 물리 모델링의 차이는 테이블/칼럼 등의 이름이 영어냐 한글이냐가 아니라, 모델에 표현하려는 것이 업무냐 시스템이냐의 차이다. 업무를 분석하여 그에 대한 데이터 집합/관계를 중점적으로 표현하는 것이 논리 모델링이고, 그 산출물을 시스템으로 어떻게 표현할지 고려하는 것을 물리 모델링이라고 볼 수 있다. 모델링 용어 ERD상에 표현되는 오브젝트는 논리 모델링이냐 물리 모델링이냐에 따라 각각 이름이 다르게 표현된다. 논리 모델 물리 모델 엔티티(Entity) 테이블(Table) 속성, 어트리뷰트(Attribute) 컬럼(Column) 관계, 릴레이션(Relation) 관계, 릴레이션(Relation) 키 그룹(Key group) 인덱스(Index) 논리 모델링 엔티티 객체지향 언어에서 클래스와 동급의 의미다. 일반적으로 2개 이상의 속성을 가지고, 1개 이상의 레코드를 가지는 오브젝트를 말한다. 엔티티를 도출할 때 가장 중요한 것은 용어의 정의다. 해당 용어가 의미하는 범위가 어디까지인지 명확히 하고, 그에 걸맞는 이름을 부여해야 한다. 그래야만 다음으로 도출할 속성이나 식별자, 관계가 명확해질 수 있다. ERD 에서 엔티티는 이와 같이 표현한다 박스 외부에는 엔티티의 이름을 적고, 박스 내부 상단에는 PK, 하단에는 일반 속성을 나열한다 엔티티 종류 엔티티는 크게 키 엔티티, 메인 엔티티, 액션 엔티티로 구분할 수 있다. 키 엔티티는 대상 데이터 중 가장 최상위에 존재하는 엔티티이며, 일반적으로 메인 엔티티와 액션 엔티티를 만들어내는 부모 역할을 한다 일반적으로 현실에 존재하는 객체를 표현하는 경우가 많다. 사원, 고객, 상품 등의 엔티티는 대표적인 키 엔티티이다. 이러한 키 엔티티간의 작용으로 만들어지는 엔티티를 액션 엔티티라고 한다 예로는 구매, 계약 등이 있다 이러한 액션 엔티티들 중 서비스에서 상당히 중요한 역할을 하는 엔티티들을 메인 엔티티 라고 한다 위에서 언급한 구매, 계약 등은 사실상 메인 엔티티의 대표적인 예시이다 액션 엔티티도 나중에 업무가 변화하고 확장되면 메인 엔티티로 향상될 수 있다. 엔티티 작명 엔티티의 이름은 복수형 표현을 사용하지 않고 별도의 수식어가 없는 단순 또는 복합 명사 형태를 사용한다. 복수형 표현의 예로 목록, 리스트 등의 단어가 있는데, 테이블 자체가 이미 레코드의 목록을 저장하는 객체이므로 이 같은 이름을 사용할 필요는 없다. 엔티티의 이름에 수식어가 있다면 주의해서 검토하고, 필요하다면 통합하는 것이 좋다. ex) 상품(O), 고객용상품(X), 직원용상품(X) 애매모호한 단어도 피하는 것이 좋다. ex) 사원(O), 사원정보(X) 어트리뷰트(속성) 엔티티가 가지고 있는 속성으로써, 더 이상 분리될 수 없는 최소의 데이터 보관 단위이다. 어트리뷰트 원자성 어트리뷰트는 반드시 독자적인 성질을 가지는 하나의 값만을 저장해야 한다. 그런데 값의 최소 단위(하나의 값)라는 것이, 표현되는 서비스에 따라 달라질 수 있다. 예를 들면 주소가 있다 주소를 시군구, 읍면동 단위로 조작하는 행위가 많다면 속성이 시, 군, 구 등으로 잘개 쪼개지겠지만, 그렇지 않다면 굳이 잘게 쪼개어 관리를 어렵게 할 이유는 없다. 위와 같은 이유가 아닌 상태에서 어트리뷰트에 여러 값을 저장하는 행위는 지양해야 한다. 예를 들면 회원 취미 정보를 하나의 어트리뷰트에 구분자로 한꺼번에 저장하는 경우이다. 이는 어트리뷰트의 원자성에 위배되며 물리 모델링(성능)에 나쁜 영향을 미칠 가능성이 크다. 어트리뷰트 작명 어트리뷰트는 그 이름 자체만으로 그 의미를 이해할 수 있게 작명하는 것이 좋다. 사람들은 대부분 어트리뷰트의 이름을 최대한 간단히 작명하려는 경향이 있는데, 이름을 너무 간략히 작성하면 나중에 그 의미를 혼동하기 쉽다. 아래는 잘못된 어트리뷰트 명명의 예시이다. 속성을 하나씩 살펴보자. 번호 어트리뷰트의 이름은 최소한 범위를 한정하는 한정자와 값을 표현하는 명사로 구성해줘야 한다. 번호의 경우 값을 표현하는 명사만 사용되어 가독성이 떨어진다. 누군가는 이를 회원번호가 아니라 전화번호라고 생각할 수도 있다. 회원아이디, 회원일련번호 등으로 변경하는 것이 좋다. 주소 지금은 어느정도 전달력이 있지만 만약 이 외에 사무실 주소라는 어트리뷰트가 추가로 있다면, 이 어트리뷰트가 자택 주소를 나타낸다는 보장이 없어진다. 이러한 경우 때문에 단어 하나로만 구성된 어트리뷰트는 배제하도록 노력해야 한다. 상태 이게 무슨 상태를 나타내는 것인지는 ERD를 설계한 사람만이 알 것이다. 게다가 실수로라도 여기에 설계자가 의도한 값 외의 값이 들어가게 된다면, 이 어트리뷰트는 거의 쓰레기 수준으로 관리될 것이다. 로그인 번호와는 반대로 범위를 한정하는 한정자만 사용된 케이스이다. 로그인 일시를 의미하는건지, 로그인 IP를 의미하는 건지 전혀 알 수 없다. 1번에서 언급하였듯이 어트리뷰트의 이름은 범위를 한정하는 한정자 + 값을 표현하는 명사로 구성하는 것이 가장 이상적이다. 어트리뷰트의 이름이 너무 길어지면 물리모델링 과정에서 까다로워지므로 단어 2~4개 정도를 결합해서 사용하는 것이 좋다. 식별자(프라이머리 키) 식별자는 본질 식별자와 실질 식별자로 나눌 수 있다. 본질 식별자는 엔티티의 레코드가 생성될 수 있는 조건을 알려주는 식별자를 의미한다. 위의 구매 테이블에서 보이듯이, 고객아이디 + 상품코드 + 구매일자가 있어야 레코드가 한건 생성될 수 있으므로, 이 3개의 어트리뷰트가 묶여서 본질 식별자가 되는 것이다 실질 식별자란 실질적으로 테이블에서 식별자로 사용하고 있는 값을 의미하느데, 구매 테이블의 경우 본질 식별자를 그대로 실질 식별자로 사용하고 있다 근데 여기서 문제가 되는게, 보통 구매 엔티티의 경우 주문이력, 상태변화 등 수많은 자식 엔티티를 가질 가능성이 상당히 높다. 근데 현재와 같이 본질 식별자를 식별자로 사용할 경우, 자식 엔티티의 경우 부담해야하는 어트리뷰트의 개수가 계속해서 많아지게 된다. 그러므로 인위적인 값을 생성하여 이를 실질 실별자로 사용하는 경우가 많다. 이때 사용되는 식별자를 인조 식별자라고 한다. 관계(릴레이션) 엔티티간 상호작용을 표현해주는 것을 말한다. 엔티티와 동일하게 매우 중요한 역할을 수행한다. 관계없이 엔티티만 있는 ERD는 ERD로 볼 수 없다. 식별 관계와 비식별 관계 부모 엔티티의 식별자가 자식 엔티티의 식별자로 포함될 경우 식별 관계, 그냥 일반 어트리뷰트로 포함될 경우 비식별 관계라고 한다. 부모 엔티티가 자식 엔티티를 만들어 내는데 필수적인 역할을 하고 있을 경우 식별 관계를 형성할 대상이 될 수 있다. 하지만 그 대상을 모두 식별 관계로 형성하면 자식의 식별자가 너무 많아지므로 관계 중 유일성을 보장할 수 있는 최소한의 대표 관계만 식별 관계로 선택하고, 나머지는 비식별 관계로 선택하는 것이 좋다. 관계의 기수성(Cardinality) 부모 엔티티의 레코드 하나에 자식 엔티티의 레코드가 얼마나 만들어질 수 있는지를 의미한다. 정확히 몇 건이냐를 표시하는 것이 아니라, 0건, 1건, N건(1건 이상) 으로 구분해서 표시한다. 기수성은 관계선의 양쪽 끝에 표시하며, 나타내는 법은 아래와 같다. (이 그림 하나 그린다고 30분을 넘게 썼다… 역시 난 미술이랑 안맞다 ㅋㅋㅋ) 그리고 아래는 실제 표시되는 형태이다. 아래는 간단한 예시이다. 한명의 회원은 한번도 구매를 하지 않을수도, 1번 이상 구매할 수도 있음을 나타낸다. 관계의 형태 모델링에는 수많은 관계들이 나타나겠지만, 대표적으로 많이 나타나는 몇개의 패턴들이 있다. 계층 관계 부모와 자식간의 직선적인 관계가 연속되는 형태를 말한다. 자식 엔티티로 갈수록 식별자의 개수가 많아지므로, 적절한 수준에서 식별자를 인조 식별자로 대체하는 것이 좋다. 업무에 따라 다르지만 보통은 2~4단계에서 대체하는 것이 일반적이다. 순환 관계 하나의 엔티티가 부모임과 동시에 자식이 되는 재귀적인 형태를 말한다. MySQL은 Oracle과 달리 재귀 쿼리가 지원되지 않는데, 이를 이유로 순환 관계를 피하는 모델링은 잘못된 방식이다. 그러한 이유로 순환 관계를 계층 관계로 풀어봤자 나아지는 것은 아무것도 없고, 결국에 더 복잡해질 뿐이다. N:N 관계 보통의 데이터 모델에서는 1:N 관계가 90% 정도를 차지할 정도로 많이 존재히나, 가끔씩 N:N(다대다) 관계도 등장한다. 아래는 N:N 관계의 대표적인 예시다. 학생은 여러개의 과목을 수강할 수 있고, 과목은 여러 학생에 의해 수강될 수 있다 그러나 이런 표기법은 논리모델에서나 가능하고, 물리모델에서는 불가능하다. 즉 물리모델에서는 다른 방식으로 풀어야한다. 이처럼 2개의 1:N 관계로 풀어줘야 한다. 이를 N:N 관계 해소 라고 한다. 수강과 같은 엔티티를 관계 엔티티라고 표현한다. 이는 요즘에 유행하는 SNS의 팔로잉, 팔로워의 대표적인 예시이다. 엔티티 통합 ERD를 작성하다 보면 엔티티를 구성하는 어트리뷰트와 관계가 비슷한 엔티티를 자주 보게 된다. 관계가 비슷하다는 것은 용도가 비슷하다는 의미인데, 이런 엔티티는 통합의 대상이 아닌지 주의깊게 살펴보는 것이 좋다. 아래는 통합의 간단한 예시다. 법인고객과 개인고객이 서로 많은 어트리뷰트를 공유하고 있어 이를 하나의 엔티티로 합친 예시이다. 만약 두 엔티티의 어트리뷰트 차이가 꽤나 난다면, 둘의 공통 속성을 모아서 하나의 통합 엔티티로 만드는 방법도 고려해볼 수 있다. 또한 이 서비스에서 어떤식으로 엔티티나 어트리뷰트에 접근하게 될 지도 고려하면서 통합이나 분리를 선택하는 것이 가장 좋다. 관계 통합 관계 또한 통합하는 과정을 거치는 것이 좋다. 왼쪽 관계의 경우 고객의 수가 늘어나는 등의 요구사항에 대응하기 어려우므로, 오른쪽과 같이 고객들을 별도의 엔티티로 분리해주는 것이 좋다. 참고로 관계를 통합하면 조인이나 저장되는 테이블이 늘어나서 개발이 번거로워질수 있다. 하지만 대체로 관계의 통합은 성능적 이슈보다는 업무에 유연하게 대응하기 위한 것이다. 모델 정규화 만약 데이터를 하나의 테이블에 다 때려넣으면 어떻게 될까? 불필요한 공간 낭비는 기본이고 사람이 관리하기도 매우 힘들것이며 삽입이상, 갱신이상, 삭제이상과 같은 부작용 또한 초래할 수 있게 된다 이상현상(abnomaly) : https://yaboong.github.io/database/2018/03/09/database-anomaly-and-functional-dependency/ 정규화란 중복과 이상현상이 발생하지 않도록 데이터를 적절한 기준으로 나눠서 저장하는 것을 말한다 객체지향 프로그래밍에서 중복을 제거하기 위해 객체들의 관심사를 분리하는 과정과 비슷하다고 보면 된다 (데이터베이스에서 관심사를 발생하지 않아 발생하는 문제의 경우 코드보다 훨씬 치명적이다) 제1정규화(No Reapeating Group) 제1정규화의 요건은 모든 속성은 반드시 하나의 값을 가져야 한다이다. 아래와 같이 하나의 어트리뷰트에 여러개의 값을 저장하거나, 하나의 엔티티에서 똑같은 성격의 어트리뷰트가 여러번 나열되는 것은 제1정규화를 위반한 것이다. 제2정규화(Whole Key Dependent) 제2정규화의 요건은 식별자 일부에 종속되는 어트리뷰트는 제거해야 한다이다. 엔티티의 식별자를 구성하는 어트리뷰트가 2개일떄, 그 엔티티의 모든 어트리뷰트가 식별자 모두에 완전하게 종속적이어야 한다. 친구회원명이라는 어트리뷰트는 식별자 중 친구회원번호에만 종속관계를 가진다. 이는 제2정규화를 위반한 것이다 제3정규화(Non-Key Independent) 제3정규화의 요건은 식별자 이외의 속성간에 종속관계가 존재하면 안된다이다. 직업명은 직업코드에 종속적인 어트리뷰트이므로, 따로 엔티티로 분리하며 제거하였다. 물리 모델링 논리 모델링을 통해 나온 산출물을 RDBMS의 특성에 맞게 변환하는 작업이다. 프라이머리 키 선택 물리 모델링에서는 인덱스라는 존재 때문에 프라이머리 키 선택을 더욱 신중하게 해야 한다. 앞의 논리 모델링에서도 고려했던 인조 식별자 사용 여부를 물리 모델링에서도 고려하게 되는데, 여기서 인덱스의 성능까지 추가적으로 생각해줘야 한다. InnoDB는 프라이머리키에 의해 클러스터링 되는 스토리지 엔진인데, 이러한 엔진에서는 프라이머리 키를 레코드의 주소 대신 사용하기 때문에 프라이머리키를 구성하는 컬럼의 개수가 많아지면 성능적으로 이슈가 발생할 수 있다 알다시피 인덱스는 레코드의 주소를 가지고 있고, 이 말인 즉 프라이머리 키가 많을 경우 인덱스의 크기도 같이 비대해짐을 의미한다 인덱스의 크기가 작을 경우 차이가 미비하지만, 크기가 커질 경우 그 차이가 확연하게 드러나게 된다. 디스크를 차지하는 크기가 커진다는 것은 그만큼 많은 디스크 입출력을 필요로 하고, 메모리에 캐시나 버퍼링을 하기 위해 더 많은 물리적 메모리가 필요하다는 것을 의미한다. 그러므로 프라이머리 키는 적절한 선에서 인조 식별자를 선택해주는 것이 좋다 그리고 추가로, 프라이머리 키 또한 인덱스로 사용되므로 반드시 SELECT의 조건절에 자주 사용되는 컬럼 위주로 순서를 배치해줘야 한다 데이터 타입 선정 물리 모델링에서 칼럼의 데이터 타입은 가능한 한 최소 단위의 타입을 부여해야 한다. 레코드의 개수가 많아지면 데이터 타입 한 바이트라도 많은 차이를 만들어내기 때문이다. 데이터의 타입 데이터의 타입은 저장하려는 데이터의 성격 그대로 타입을 선정하는 것이 가장 좋다. 숫자나 날짜 데이터를 모두 문자열 칼럼에 저장해도 아무런 차이가 없다면 처음부터 MySQL에 이렇게 많은 데이터 타입이 제공되지 않았을 것이다. 만약 저장할 데이터 타입이 명확하지 않고 두개의 데이터 타입 중간쯤 위치해 애매하다면, 그 두 데이터 타입의 장단점을 비교해 선택하는 것이 좋다. 대표적 예시로는 IP주소가 있다. 컬럼의 길이 항상 우리는 칼럼에 저장될 데이터의 최대 길이만을 생각하여 길이를 지정하는 경향이 있는데, 이것보다는 우리가 저장할 데이터가 어떤 특성을 가지느냐에 따라 칼럼의 길이를 결정해야 한다. 예를 들어 URL 데이터를 저장하는 칼럼의 경우, URL의 최대 길이에 집중할 것이 아니라 우리 서비스에 필요한 URL 길이만을 생각하면 된다. 그리고 인조식별자에 INTEGER 대신 BIGINT를 사용하는 것도 어떻게 보면 너무 과도하게 멀리 생각하는 행위라고 볼수도 있다. 문자집합 문자열 타입에서 문자열이 어떤 문자집합을 가지는지도 상당히 중요한 문제이다. 특별히 지정하지 않으면 MySQL 서버의 default-character-set에 지정된 문자집합을 사용한다. 하지만 명확한 기준이 있다면 latin1, utf8를 같이 사용하여 데이터 저장 공간을 줄이는 것도 좋다. 컬럼의 길이나 문자집합을 신경쓰는 이유는 데이터가 디스크를 많이 사용하는 것을 막기 위함이다. MySQL에서는 임시테이블/버퍼작업(정렬, 그룹핑 등)을 위해 별도의 메모리 할당이 필요하다. 이때 MySQL 서버는 실제 저장된 데이터 길이로 메모리를 할당하는 것이 아니라, 데이터 타입에 명시된 길이를 기준으로 메모리를 할당하고 사용한다. 그런데 이 메모리 공간이 일정 크기 이상을 초과하면 메모리가 아니라 디스크에서 처리된다. 즉, 테이블 컬럼이 과도하게 크게 설정되면 메모리로 처리되어야 할 것이 디스크로 처리될 수도 있다. NULL, NOT NULL InnoDB의 경우 NULL이 저장되는 칼럼은 전혀 디스크 공간을 사용하지 않는 특징이 있다. 즉 NULL을 저장함으로써 디스크 공간을 줄일 수 있는 것이다. 하지만 SELECT가 많이 발생하는 컬럼의 경우 NULL을 저장하는 행위를 지양하는 것이 좋다. MySQL에서 NULLABLE 컬럼에 IN 연산등을 했을 경우 굉장히 이상한 비교 작업을 내부적으로 하기 때문이다. 반정규화 정규화는 데이터의 저장 비용을 최소화하는 역할을 담당하기 때문에, 진행할수록 테이블의 개수나 컬럼의 개수가 증가하게 되고, 이는 결국 SELECT의 부담으로 이어진다. 반정규화는 COUNT나 GROUP BY 같은 컬럼들을 미리 집계하여 별도로 저장하여 데이터를 읽어오는 비용을 최소화하는 작업을 말한다. 컬럼 복사 조인을 없애기 위해 원본 컬럼의 값을 변경하지 않고 그대로 다른 테이블로 복사해두는 형태를 말한다. 복사해온 컬럼을 이용해 GROUP BY나 ORDER BY를 인덱스로 할수있다면 성능에 상당히 도움이 될 수 있다. 하지만 복사해온 컬럼이 자주 변경된다면 비효율적인 작업이 될 것이다. 그러므로 읽기와 변경의 비율을 따져보고 컬럼 복사를 진행하는 것이 좋다. 요약 칼럼 어떠한 계산의 결과로 만들어진 값을 저장해두는 컬럼을 의미한다. 대부분 여러 레코드의 건수, 최대값, 최소값등을 미리 계산해서 저장해두는데 사용한다. 계산된 결과가 빈번히 호출되고, 매번 계산하기가 부담스러울 때 사용하면 좋은 방법이다. 하지만 계산의 결과가 빈번히 변경된다면 문제가 될수있다. 이처럼 잦은 데이터의 변경은 문제가 될 수 있으므로, 20-30분에 한번씩 도는 배치를 통해 계산 컬럼을 변경해주는 것이 가장 효율적인 방법이다. 해시 인덱스 해시 인덱스는 칼럼의 원래 값을 인덱싱하는 것이 아니라, 길이를 훨씬 줄인 해시값으로 인덱스를 구성하는 방식이다. 하지만 MyISAM이나 InnoDB에서는 이를 지원하지 않으므로, MD5 함수를 통해 이를 흉내내야 한다. 방식은 간단하다. 별도의 해시 저장용 컬럼을 만들고 그곳에 인덱스를 생성하는 것이다. 그리곤 아래와 같이 사용할 수 있다. 123SELECT *FROM tempWHERE hash_url=MD5('http://....'); url에 직접 인덱스를 생성할 필요도 없고, 만약 url컬럼에 인덱스를 생성하지 못하는 경우에도 유용하게 사용할 수 있다. 기본적으로 InnoDB는 767바이트까지만 인덱스를 생성할 수 있기 때문이다. 참고 : 이성욱, 『Real MySQL』, 위키북스(2012)]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>Real MySQL</tag>
        <tag>데이터 모델링</tag>
        <tag>논리 모델링</tag>
        <tag>물리 모델링</tag>
        <tag>정규화</tag>
        <tag>반정규화</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] mysql dump]]></title>
    <url>%2Fdb%2Fmysql-dump%2F</url>
    <content type="text"><![CDATA[덤프뜨기 전체 database 덤프 123# -A 또는 --all-databases 옵션mysqldump -u[아이디] -p[패스워드] -A &gt; 경로/파일명.sqlmysqldump -u[아이디] -p[패스워드] --all-databases &gt; 경로/파일명.sql 특정 database 또는 특정 table만 덤프 12345# 특정 databasemysqldump -u[아이디] -p[패스워드] [database명] &gt; 경로/파일명.sql# 특정 tablemysqldump -u[아이디] -p[패스워드] [database명] [table] &gt; 경로/파일명.sql 테이블 스키마만 덤프 데이터 없이 테이블 구조(스키마)만 받을 때 사용한다 123# -d 또는 --no-data 옵션mysqldump -u[아이디] -p[패스워드] -d [database명] &gt; 경로/파일명.sqlmysqldump -u[아이디] -p[패스워드] --no-data [database명] &gt; 경로/파일명.sql 특정 조건으로 덤프 특정 database의 특정 table에서 원하는 값만 덤프받고 싶을 경우 사용한다. ex) test db의 employees 테이블에서 emp_no이 1 이상 10이하인 값만 덤프를 받고자 할 때 12# -w 옵션을 사용한다. 조건은 &apos;&apos;로 묶어줘야 한다mysqldump -u[아이디] -p[패스워드] [database명] [table명] -w &apos;emp_no &gt;= 1 and emp_no &lt;= 10&apos; 특정상황 Unkown table ‘COLUMN_STATISTICS’ in information_schema mysql 8 부터 추가된 옵션 떄문이라고 함. mysql dump에 --column-statistics=0 옵션을 추가해주면 됨 https://serverfault.com/questions/912162/mysqldump-throws-unknown-table-column-statistics-in-information-schema-1109 복구하기 1mysql -u[아이디] -p[패스워드] [데이터베이스명] &lt; 덤프파일명.sql 덤프파일내에 database 생성 구문이 있을 경우 지정된 데이터베이스는 무시된다.]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql dump</tag>
        <tag>mysql 덤프</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[linux] 우분투 intellij에서 mac keymap 사용시 이슈]]></title>
    <url>%2Flinux%2F%EC%9A%B0%EB%B6%84%ED%88%AC-intellij%EC%97%90%EC%84%9C-mac-keymap-%EC%82%AC%EC%9A%A9%EC%8B%9C-%EC%9D%B4%EC%8A%88%2F</url>
    <content type="text"><![CDATA[윈도우 키(대시) https://reachlabkr.wordpress.com/2014/12/06/ubuntu-에서-supercommand-key-shortcut-해제/ compiz 설치후에 ubuntu unity plugin launcher에 젤 상단 dash 부분 사용하지 않음으로 변경 난 super space로 바꿨는데, mac spotlight 느낌이 난다 윈도우 w키(창 닫기) 창 관리 - 스케일에 가면 있음. super + tab 으로 바꾼건 윈도우 같다고 함 파라미터 보기 https://askubuntu.com/questions/68463/how-to-disable-global-super-p-shortcut sudo apt-get install dconf-tools dconf-editor 위처럼 따라간 뒤 xrandr 해제 근데 … 좀 늦게 눌러야 인식됨 ㅡㅡ]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu intellij</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] String, StringBuffer, StringBuilder]]></title>
    <url>%2Fjava%2FString-StringBuffer-StringBuilder%2F</url>
    <content type="text"><![CDATA[셋 다 문자열 처리를 위한 클래스이나, 그 처리 방법에서 차이를 보인다. 실제로 사용하는 상황에 따라 성능차이가 발생하니 이를 확실히 정리하고자 한다. String String은 기본적으로 변경이 불가능한 immutable 클래스이다. 이말인 즉 String에 추가적인 연산을 하게 될 경우, 기존 String 클래스의 값이 변경되는 것이 아니라 항상 새로운 클래스가 생성된다는 뜻이다. 12String str1 = "AAA" + "BBB";String str2 = "CCC".concat("DDD"); str1의 경우 힙 영역에 AAA, BBB, AAABBB가 각각 생기게 되는 것이고, str2의 경우 힙 영역에 CCC, DDD, CCCDDD가 생기게 되는 것이다. String이 가지고 있는 각종 mutable해 보이는 연산(substring(), toLowerCase()) 등 은 모두 위와 같이 처리된다. JDK 5.0 미만 버전 한정이다. JDK 5.0 이상부터는 String 연산도 내부적으로 StringBuilder로 변환된다. String을 이렇게 디자인 한 이유는 프로그램 기본 문자열 클래스로 사용하기 위해서이다. 프로그램 작성 시 문자열을 생성하고 참조하는 경우는 많으나, 변경하는 일은 그리 많지 않다. 이럴 경우 위와 같은 immutable 형태로 선언하게 되면 많은 효과를 누릴 수 있다. 일단 thread-safe 하기 때문에 여러 쓰레드 내에서 자유롭게 참조할 수 있고, 한번 생성한 문자열은 같은 문자열에 대해서는 변경을 가하지 않으므로, 요청 시 똑같은 주소값을 반환해 주기 때문에 성능상으로 많은 이점을 누릴 수 있다. 실제로 아래의 연산이 성립한다. 123String str1 = "AAA";String str2 = "AAA";assertTrue(str1 == str2); 두 문자열이 같은 힙 영역을 공유하기 때문이다. StringBuffer 이에 반해 StringBuffer 클래스는 mutable 클래스이다. 위처럼 기존의 문자열에 추가적인 연산을 하게 되면, String처럼 새로운 문자열이 생성되는 것이 아니라 기존의 문자열에 추가적인 연산을 하게 된다. 그러므로 당근 문자열의 연산이 많을 경우에는 String 클래스보다 훨씬 효율적이다. 그러면 아! 그럼 문자열 연산이 많을 때는 무조건 StringBuffer 써야겠구나! 라고 할 수 있지만, StringBuffer는 thread-safe를 위해 내부적으로 synchronized 연산을 수행하게 되므로 문자열의 변경이 잦지 않을 경우는 String 보다 나쁜 성능을 보인다. 그러므로 연산이 많을 경우에 사용하도록 하자~~ StringBuilder JDK 5.0 부터 나왔다. StringBuffer와 동일하나 thread-safe 하지 않다는 점이 차이점이다. 위에서 언급했듯이 JDK 5.0 이후로는 String 연산이 내부적으로 이 StringBuilder로 변환되어 처리된다. 그럼 뭘 쓸까? 문자열 연산이 잦지 않은 경우는 String을 사용하는 것이 좋고, 연산이 잦을 경우에는 thread-safe 여부를 따져서 StringBuffer나 StringBuilder를 사용하면 된다. 어쩌피 JDK 5.0 이후로 연산 시 String이 StringBuilder로 변환된다지만, 문자열을 더할 떄 까지 객체를 계속 추가해야 한다는 사실은 변함이 없으므로, 연산이 많으면 StringBuilder나 StringBuffer를 사용하는 것이 좋다.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>String</tag>
        <tag>StringBuffer</tag>
        <tag>StringBuilder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] Scanner, BufferedReader, StringTokenizer]]></title>
    <url>%2Fjava%2FScanner-BufferedReader-StringTokenizer%2F</url>
    <content type="text"><![CDATA[Scanner VS BufferedReader Scanner를 통한 입력 1234Scanner sc = new Scanner(System.in);String str = sc.nextLine(); // 1 2 3 4 5 6 7 8 9 10String[] result = str.split(" "); BufferedReader를 통한 입력 12BufferedReader br = new BufferedReader(new InputStreamReader(System.in));String[] result = br.readLine().split(" "); 문자열에 최적화 된 BufferedReader에 비해 Scanner는 다양한 기능을 지원하므로 속도가 조금 더 느리다. split VS StringTokenizer split 12345678BufferedReader br = new BufferedReader(new InputStreamReader(System.in));String[] result = br.readLine().split(" ");// A B C D 입력result[0]; // Aresult[1]; // Bresult[3]; // Cresult[4]; // D StringTokenizer 1234567BufferedReader br = new BufferedReader(new InputStreamReader(System.in));StringTokenizer st = new StringTokenizer(br.readLine());st.nextToken(); // Ast.nextToken(); // Bst.nextToken(); // Cst.nextToken(); // D 문자열을 잘라 쓰는건 똑같은 맥락이나, split은 정규식을 기반으로 자르는 로직이므로 내부가 복잡하다. 그에 반면 StringTokenizer의 경우 단순히 공백을 땡기는 것이므로 정규식 처리가 딱히 필요한게 아닌 경우 StringTokenizer가 효율적이다.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>자바 입력</tag>
        <tag>Scanner</tag>
        <tag>BufferedReader</tag>
        <tag>StringTokenizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] 모델 바인딩과 검증]]></title>
    <url>%2Fspring%2F%EB%AA%A8%EB%8D%B8-%EB%B0%94%EC%9D%B8%EB%94%A9%EA%B3%BC-%EA%B2%80%EC%A6%9D%2F</url>
    <content type="text"><![CDATA[메서드에 @ModelAttribute를 파라미터로 선언했을 경우 처리되는 과정은 다음과 같다. 파라미터 타입의 오브젝트를 새로 만든다. 때문에 디폴트 생성자가 필수로 필요하다. @SessionAttributes를 통해 저장된 오브젝트가 있으면 새로 만들지 않고 세션에서 가져온다. HTTP 요청을 생성(혹은 가져온) 오브젝트 프로퍼티에 바인딩 해준다. 이 과정에서 각 프로퍼티에 맞게 타입을 변환해준다. 만약 타입 변환 오류가 발생할 시 BindingResult 오브젝트에 오류를 저장해서 컨트롤러로 넘겨준다. 검증작업을 수행한다. 2번의 과정에서 타입에 대한 검증은 이미 끝냈고, 그 외의 검증은 검증기를 통해 등록할 수 있다. 프로퍼티 바인딩 프로퍼티 바인딩이란 오브젝트의 프로퍼티에 값을 넣는 행위를 말한다. 프로퍼티에 맞게 타입을 적절히 변환하고 해당 프로퍼티의 수정자 메서드를 호출하는 것이다. 스프링에선 크게 두가지의 프로퍼티 바인딩을 지원하는데 첫번째는 애플리케이션 컨텍스트 XML 설정파일로 빈을 정의할 때 사용했던 &lt;property&gt; 태그이다. 이 태그를 통해 빈의 프로퍼티에 값을 주입했었다. 두번째는 HTTP 요청 파라미터를 모델 오브젝트 등으로 변환하는 경우이다. @ModelAttribute 뿐만 아니라 @RequestParam, @PathVariable 등도 해당된다. 근데 잘 생각해보면, 프로퍼티 바인딩이 일반 primitive 타입이 아닌 경우에도 가능했던 적이 있었다. 루트 웹 애플리케이션 컨텍스트에서 dataSource 빈을 설정할 때다. 123&lt;bean id="dataSource" class="org.springframework..SimpleDriverDataSource"&gt; &lt;property name="driverClass" value="com.mysql.jdbc.Driver" /&gt;&lt;/bean&gt; 보다시피 value에 문자열로 클래스명을 전달하고 있다. 그런데 driverClass 프로퍼티는 String 타입이 아닌 Class 타입이다. 하지만 잘 바인딩 된다. 이는 스프링이 제공하는 프로퍼티 바인딩 기능을 사용했기 때문이다. 스프링은 프로퍼티 바인딩을 위해 2가지 API를 제공한다. PropertyEditor 스프링이 기본적으로 제공하는 바인딩용 타입 변환 API이다. PropertyEditor는 스프링 API가 아니라 자바빈 표준에 정의된 API이다. GUI 환경에서 비주얼 컴포넌트를 만들 때 사용하도록 설계되었고, 기본적인 기능은 문자열과 자바빈 프로퍼티 사이의 타입 변환이다. 스프링은 이 PropertyEditor를 문자열-오브젝트 상호변환이 필요한 XML 설정이나 HTTP 파라미터 변환에 유용하게 사용할 수 있다고 판단하여 이를 일찍부터 사용해왔다. 스프링은 20여가지 정도의 PropertyEditor를 만들어 디폴트로 제공하고 있다. 아래의 링크에서 확인할 수 있다. https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/beans/propertyeditors/package-summary.html 이 디폴트 PropertyEditor들은 바인딩 과정에서 파라미터 타입에 맞게 자동으로 선정되어 사용된다. PropertyEditor 구현 디폴트 프로퍼티 데이터에 등록되지 않은 타입을 파라미터로 사용하고 싶을 경우, 직접 PropertyEditor를 만들어 적용할 수 있다. 아래와 같은 enum이 하나 있고, 12345678910111213141516171819public enum Level &#123; GOLD(3), SILVER(2), BASIC(3); private Integer level; Level(Integer level)&#123; this.level = level; &#125; // 숫자를 받으면 해당하는 enum을 리턴 public static Level convert(Integer level)&#123; switch(level)&#123; case 3 : return GOLD; case 2 : return SILVER; case 1 : return BASIC; default : throw new RuntimeException(); &#125; &#125;&#125; 아래와 같이 컨트롤러를 등록하고 /user?level=1 과 같이 호출하면 자동으로 Level enum으로 변환해서 받고 싶다고 하자. (level 파라미터가 Integer이지만 변환이 간단하므로 문제될 것 없다) 1234567@Controllerpublic class UserController&#123; @RequestMapping("/user", method=RequestMethod.GET) public String userSearch(@RequestParam Level level)&#123; // ... &#125;&#125; 현재는 당연히 변환이 불가능하므로 오류가 발생한다. Level 타입에 대한 PropertyEditor를 만들어야 한다. 아래는 프로퍼티 에디터가 변환할 때의 동작 방식이다. setValue(), getValue()는 그냥 getter,setter이기 때문에 손댈 것 없고, 실제로 우리가 구현해야 할 메서드는 setAsText()와 getAsText()이다. 현재 우리한테 필요한 부분은 문자열 -&gt; 오브젝트의 과정이므로 setAsText() 메서드를 구현해서 Level enum에 대한 PropertyEditor를 만들어보겠다. 123456public class LevelPropertyEditor extends PropertyEditorSupport&#123; @Override public void setAsText(String text) throws IllegalArgumentException &#123; this.setValue(Level.convert(Integer.parseInt(text))); &#125;&#125; 이제 이 PropertyEditor를 userSearch 메서드에서 사용할 수 있게 등록해줘야 한다. PropertyEditor 등록 PropertyEditor를 추가하기 전에 먼저 컨트롤러에서 메서드 바인딩이 일어나는 순서를 알아보자. AnnotationMethodHandlerAdapter는 @RequestParam, @PathVariable, @ModelAttribute와 같이 HTTP 요청을 변수에 바인딩하는 애노테이션을 만나면 먼저 WebDataBinder라는 것을 만든다. WebDataBinder는 여러가지 기능을 포함하는데, 여기에 HTTP 요청 문자열을 파라미터로 변환하는 기능도 포함되어 있다. 즉, 우리가 만든 PropertyEditor를 사용하려면 이 WebDataBinder에 직접 등록해줘야 한다. 근데 WebDataBinder의 변환 과정이 외부로 노출되지 않으므로, 직접 등록해 줄 방법은 없다. 그래서 스프링이 제공하는 WebDataBinder 초기화 메서드를 사용해야 한다. @InitBinder 컨트롤러 클래스에 아래와 같이 @InitBinder 애노테이션이 부여되고, WebDataBinder를 인자로 받는 메서드를 하나 생성하자. 123456789101112@Controllerpublic class UserController&#123; @InitBinder public void initBinder(WebDataBinder dataBinder)&#123; dataBinder.registerCustomEditor(Level.class, new LevelPropertyEditor()); &#125; @RequestMapping("/user", method=RequestMethod.GET) public String userSearch(@RequestParam Level level)&#123; // ... &#125;&#125; 그리고 WebDataBinder의 registerCustomEditor 메서드에 PropertyEditor를 적용할 타입과 PropertyEditor 인스턴스를 전달해주면 된다. 이후 다시 /user?level=1을 호출해보면 level 변수에 Level.BASIC 오브젝트가 들어가있는 것을 확인할 수 있다. WebDataBinder 대신 WebRequest를 받을 수도 있다! initBinder 메서드는 클래스내의 모든 메서드에 대해 파라미터를 바인딩하기 전에 자동으로 호출된다. 바인딩 적용 대상은 @RequestParam, @PathVariable, @CookieValue, @RequestHeader, @ModelAttribute의 프로퍼티 이다. 기본적으로 PropertyEditor는 지정한 타입과 일치하면 항상 적용된다. 여기에 프로퍼티 이름을 추가 조건으로 주고, 프로퍼티 이름까지 일치해야만 적용되게 할 수 있다. 이러한 타입의 PropertyEditor는 이미 PropertyEditor가 존재할 경우 사용한다. WebDataBinder는 바인딩 시 커스텀 PropertyEditor가 있을 경우 이를 선적용하고, 없을 경우 디폴트 PropertyEditor를 적용하기 때문이다. 아래는 적절한 예시이다. 12345678910111213141516171819202122232425262728293031323334353637383940public class MinMaxPropertyEditor extends PropertyEditorSupport&#123; Integer min; Integer max; public MinMaxPropertyEditor(Integer min, Integer max) &#123; this.min = min; this.max = max; &#125; @Override public void setAsText(String text) throws IllegalArgumentException &#123; Integer value = Integer.valueOf(text); if(value &lt; min)&#123; value = min; &#125; if(value &gt; max)&#123; value = max; &#125; this.setValue(value); &#125;&#125;public class User&#123; Integer id; Integer age;&#125;@Controllerpublic class UserController&#123; @InitBinder public void initBinder(WebDataBinder dataBinder)&#123; dataBinder.registerCustomEditor(Integer.class, "age", new MinMaxPropertyEditor(1, 50)); &#125; @RequestMapping("/user", method=RequestMethod.POST) public String userAdd(@ModelAttribute User user)&#123; // ... &#125;&#125; 이렇게 해두면 추가하는 유저의 age값은 1~50까지로 제한된다. PropertyEditor를 등록할 때 프로퍼티 이름으로 age를 지정했기 때문에 id에는 적용되지 않는다. 참고로 이 방식은 프로퍼티 이름이 필요하므로 @RequestParam 같은 단일 파라미터 바인딩에는 적용되지 않는다. WebBindingInitializer @InitBinder 방식은 범위가 컨트롤러 하나로만 제한되므로 다른 컨트롤러에서 사용하려면 또 다시 등록해줘야 한다. 만약 해당 PropertyEditor가 모든 곳에 적용해도 될 만큼 필요한 PropertyEditor라면 등록하는 방법을 달리하여 모든 컨트롤러에 적용해줄 수 있다. 먼저 WebBindingInitializer 인터페이스를 구현한 클래스를 작성한다. 1234567public class MyWebBindingInitializer implements WebBindingInitializer&#123; @Override public void initBinder(WebDataBinder binder, WebRequest request) &#123; binder.registerCustomEditor(Level.class, new LevelPropertyEditor()); &#125;&#125; 이제 이 클래스를 빈으로 등록하고 AnnotationMethodHandlerAdapter의 webBindingInitializer 프로퍼티에 DI 해주면 전체적으로 적용된다. 12345&lt;bean class="org.springframework..AnnotationMethodHandlerAdapter"&gt; &lt;property name="webBindingInitializer"&gt; &lt;bean class="MyWebBindingInitializer" /&gt; &lt;/property&gt;&lt;/bean&gt; 프로토타입 PropertyEditor 앞서 작성했던 PropertyEditor 등록 코드들을 보면, 매번 new 키워드로 PropertyEditor를 생성하고 있다. 이 부분이 뭔가 부담스럽게 생각되어 PropertyEditor를 빈으로 등록하는 방식을 생각할 수 있는데, 이는 위험한 상황을 초래한다. 위의 PropertyEditor 동작방식을 다시 살펴보면, 변환과정에서 항상 set -&gt; get의 순서로 2개의 메서드를 사용하고 있음을 볼 수 있다. 이 말인 즉, PropertyEditor는 짧은 시간이나마 상태를 가진다는 것을 의미한다. 상태를 가지는 오브젝트는 절대 빈으로 등록되서는 안된다. 매번 new 키워드로 생성되는 부분이 부담스러워 보일 수 있으나, 실상 PropertyEditor는 워낙 간단한 클래스라 자주 생성되도 별로 문제가 되지 않는다. 그러므로 싱글톤으로 PropertyEditor를 생성하는 실수를 하지 않도록 주의해야 한다. 근데 개발을 하다보면, PropertyEditor에서 다른 빈을 DI 받아야 할 경우가 가끔 생긴다. 예를 들면 아래와 같이 변환할 프로퍼티가 하나의 도메인 오브젝트에 대응하는 경우이다. 123456public class User&#123; Integer id; String name; Code userType; // 코드 테이블에 대응하는 도메인 오브젝트 // ...&#125; 이런 경우, 일반적인 방법으로는 변환할 수 없다. 요청 파라미터는 평범한 문자열이기 때문이다. 이 상황을 해결할 수 있는 방법은 2가지가 있다. 모조 PropertyEditor Code 오브젝트로 변환하되, 완벽하지 않은 오브젝트로 변환하는 방법이다. 123456789public class CodePropertyEditor extends PropertyEditorSupport&#123; @Override public void setAsText(String text) throws IllegalArgumentException &#123; Code code = new Code(); code.setId(Integer.valueOf(text)); this.setValue(code); &#125;&#125; 이런식으로 전달받은 id값만 채운 불완전한 Code 오브젝트를 돌려주는 것이다. 이런 방식을 모조 PropertyEditor라고 부른다. 하지만 이 방식은 조금 위험하다. 다른 프로퍼티들의 값이 모두 null인 불완전한 오브젝트로 변환해주기 때문이다. 이런 오브젝트는 업데이트가 발생하면 심각한 문제를 초래할 수 있으나, 사실상 이러한 코드성 도메인 오브젝트는 다른 테이블에서 참조하는 용도로만 사용하는 것이 대부분이다. 그래서 이런 부분만 유의해주면 매우 유용하게 활용할 수 있다. 프로토타입 도메인 오브젝트 PropertyEditor PropertyEditor를 프로토타입 빈으로 등록하고, 서비스나 DAO 객체를 DI 받아 Code 오브젝트를 조회해오는 방법이다. 123456789101112131415161718192021222324252627@Component@Scope("prototype")public class CodePropertyEditor extends PropertyEditorSupport&#123; @Atuworied codeService; @Override public void setAsText(String text) throws IllegalArgumentException &#123; Code code = codeService.getCode(Integer.valueOf(text)); this.setValue(code); &#125;&#125;@Controllerpublic class UserController&#123; @Inject Provider&lt;CodePropertyEditor&gt; codePropertyEditorProvider; @InitBinder public void initBinder(WebDataBinder dataBinder)&#123; dataBinder.registerCustomEditor(Code.class, codePropertyEditorProvider.get()); &#125; @RequestMapping("/user", method=RequestMethod.POST) public String userAdd(@ModelAttribute User user)&#123; // ... &#125;&#125; 이 방식의 장점은 항상 완전한 도메인 오브젝트를 리턴해주므로, 앞서 제기했던 위험이 없어진다. 단점으로는 매번 DB에서 조회를 해야하므로 성능에 조금 부담을 주는 단점이 있다. 하지만 JPA와 같이 엔티티 단위의 캐싱 기법이 발달한 기술을 사용할 경우, DB에서 조회하는 대신 메모리에서 바로 읽어올 수 있으므로 DB 부하에 대한 걱정은 하지 않아도 된다. Converter PropertyEditor는 근본적인 단점이 있다. 상태를 가지고 있으므로 싱글톤으로 등록할 수 없고, 항상 새로운 오브젝트를 만들어야 한다는 점이다. 스프링 3.0이후로 이러한 PropertyEditor의 단점을 보완해주는 Converter라는 타입 변환 API가 등장하였다. Converter는 PropertyEditor와 달리 변환과정에서 메서드가 한번만 호출된다. 즉, 상태를 가지지 않는다는 뜻이고, 싱글톤으로 등록할 수 있다는 뜻이다! Converter 구현 아래는 Converter 인터페이스이다. 123public interface Converter&lt;S, T&gt;&#123; T convert(s source);&#125; 양방향 변환을 지원하던 PropertyEditor와는 달리, 단방향 변환만을 지원한다. (양방향을 원하면 그냥 반대방향의 Converter를 하나 더 만들면 된다.) 게다가 한쪽 타입이 무조건 String으로 고정되는 불편함 없이 직접 지정 가능하다. 아래는 전달받은 파라미터를 Level 타입으로 변환해주는 Converter이다. 12345public class LevelConverter implements Convert&lt;Integer, Level&gt;&#123; public Level convert(Integer source)&#123; return Level.convert(source) &#125;&#125; 타입을 바로 Integer로 지정함으로써 지저분한 타입변환 코드를 제거할 수 있다. Converter 등록 PropertyEditor처럼 직접 등록할 수 없고, ConversionService 타입의 오브젝트를 통해서 WebDataBinder에 등록해야 한다. ConversionService 타입의 오브젝트를 빈으로 등록하고 이를 DI받아 WebDataBinder에 등록하는 방식이므로 PropertyEditor에 비해 부담이 적다. ConversionService를 등록하는 방법은 2가지가 있다. 첫째로 직접 클래스를 만들고 GenericConversionService를 상속받은 뒤, addConverter() 메서드로 Converter들을 등록하는 방식이다. 이후 빈으로 등록한다. 둘째는 추가할 Converter들을 빈으로 등록해두고 ConversionServiceFactoryBean을 이용해서 Converter들이 추가된 GenericConversionService를 빈으로 등록하는 방식이다. 직접 클래스를 만들지 않고 설정만으로 가능하므로 좀 더 편리하다. 아래는 두번째 방법이다. 12345678&lt;bean class="org.springframework..ConversionServiceFactoryBean"&gt; &lt;property name="converters"&gt; &lt;set&gt; &lt;bean class="LevelConverter" /&gt; &lt;!-- 추가하고 싶은 Converter들... --&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt; 그리고 컨트롤러에서 아래와 같이 해주면 된다. 1234567891011121314@Controllerpublic class UserController&#123; @Autowired ConversionService conversionService; @InitBinder public void initBinder(WebDataBinder dataBinder)&#123; dataBinder.setConversionService(this.conversionService); &#125; @RequestMapping("/user", method=RequestMethod.GET) public String userSearch(@RequestParam Level level)&#123; // ... &#125;&#125; 매번 개별적으로 등록해줘야하는 PropertyEditor와는 달리 하나의 ConversionService에 Converter들을 일괄적으로 지정할 수 있어 매우 편리하다. 때에 따라서는 여러개의 ConversionService를 만들어놓고 사용하기도 한다. 하지만 WebDataBinder는 하나의 ConversionService 타입 오브젝트만 허용한다는 점은 알고있어야 한다. Converter도 PropertyEditor처럼 WebBindingInitializer를 이용해 일괄등록 할 수 있다. 하지만 ConversionService를 등록할 떄는 ConfigurableWebBindingInitializer를 이용하는 것이 더 편리하다. 12345678910111213141516&lt;bean class="org.springframework..ConversionServiceFactoryBean"&gt; &lt;property name="converters"&gt; &lt;set&gt; &lt;bean class="LevelConverter" /&gt; &lt;!-- 추가하고 싶은 Converter들... --&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id="webBindingInitializer" class="org.springframework..ConfigurableWebBindingInitializer"&gt; &lt;property name="conversionService" ref="conversionService" /&gt;&lt;/bean&gt;&lt;bean class="org.springframework..AnnotationMethodHandlerAdapter"&gt; &lt;property name="webBindingInitializer" ref="webBindingInitializer" /&gt;&lt;/bean&gt; 이게 전부 ConversionService를 싱글톤 빈으로 등록할 수 있기에 생겨난 방법들이다. 만약 spring 설정에서 &lt;mvc:annotation-driven /&gt;를 사용했을 경우, 위의 두 방법처럼 ConversionService를 등록하는 것이 불가능하다. 이럴경우 &lt;mvc:annotation-driven conversion-service=&quot;conversionService&quot;/&gt; 처럼 엘리먼트를 이용해서 등록해줘야 하며, 이렇게 등록할 경우 모든 클래스에 자동으로 적용된다. Formatter 위의 두 가지 외에 Formatter라는 타입 변환 API가 하나 더 있다. 근데 이는 스프링에서 기본으로 제공하는 API가 아니라서, 절차가 조금 까다롭다. 일단 Formatter인터페이스는 아래와 같다. 12345678910111213// Formatter interfacepublic interface Formatter&lt;T&gt; extends Printer&lt;T&gt;, Parser&lt;T&gt; &#123;&#125;// Printer interfacepublic interface Printer&lt;T&gt; &#123; String print(T object, Locale locale);&#125;// Parser interfacepublic interface Parser&lt;T&gt; &#123; T parse(String text, Locale locale) throws ParseException;&#125; 이 인터페이스를 구현해서 Formatter를 만들면 된다. 보다시피 Locale을 파라미터로 받을 수 있어 컨트롤러에 사용하기 좀 더 특화되었다고 할 수 있다. 근데… Formatter는 스프링 기본 API가 아니라서 GenericConversionService에 직접 등록할 수 없다. Formatter를 GenericConverter로 포장해서 등록해주는 FormattingConversionService를 통해서만 등록될 수 있다. 그리고 Formatter를 본격적으로 사용하려면 이게 끝이 아니라 애노테이션을 연결시켜야 하므로 AnnotationFormatterFactory도 사용해야 한다. 이래서 굳이 Locale이 타입 변환에 필요한 경우가 아니라면 Converter를 사용하는 편이 낫다. 당장은 FormattingConversionServiceFactoryBean을 통해 FormattingConversionService를 등록하고, 거기서 기본으로 등록되는 Formatter만 사용해도 유용하다. 1&lt;bean class="org.springframework..FormattingConversionServiceFactoryBean" /&gt; 이렇게 빈으로 등록하고 위와 같은 방식으로 conversionService를 주입해주면 된다. 이 또한 만약 spring 설정에서 &lt;mvc:annotation-driven /&gt;를 사용했을 경우 사용이 불가능하다. 참고로 FormattingConversionServiceFactoryBean은 &lt;mvc:annotation-driven /&gt; 사용 시 디폴트로 등록해주는 ConversionService라 위처럼 conversion-service 엘리먼트를 이용해 따로 등록해 줄 필요없다. @NumberFormat 첫째로 사용할 수 있는 애노테이션 기반 포멧터이다. 이는 NumberFormatter, CurrencyFormatter, PercentFormatter 와 연결되어 있다. 엘리먼트로 style과 pattern을 줄 수 있다. style은 Number, Currency, Percent 세 가지를 설정할 수 있고, 각각 위의 Formatter와 연결된다. style에 없는 패턴을 사용하고 싶을 경우 pattern 엘리먼트를 통해 직접 지정할 수 있다. 아래는 pattern엘리먼트를 사용한 예제이다. 123456class Product&#123; @NumberFormat("$###,##0") Long price; // getter, setter&#125; request 인자로 price=$100,000 과 같이 넘겨줘도 price 프로퍼티에서 변환해서 받을 수 있고, 뷰로 내려줄 때에 $100,000의 형태로 내려줄 수 있다. @DateTimeFormat 강력한 날짜, 시간 라이브러리인 Joda Time을 이용하는 애노테이션 기반 포멧터이다. 이는 DateTimeFormatter와 연결되어 있다. 엘리먼트로 style과 pattern을 줄 수 있다. S(short), M(medium), L(long), F(full) 4개의 문자를 날짜와 시간에 대해 1글자씩 사용해 스타일을 지정한다. 12@DateTimeFormat(style="FS")Calenadar birthday; 이는 yyyy'년' M'월' d'일' EEEE a h:mm 포멧으로 매핑된다. 물론 각각의 지역정보에 따라 다르게 출력된다. style에서 지정한 패턴이 마음에 들지 않는 경우, pattern 엘리먼트를 통해 직접 지정 가능하다. 12@DateTimeFormat(pattern="yyyy/MM/dd")Calendar birthday; 바인딩 기술 활용 전략 위의 3가지 방법은 각기 장단점이 있기 때문에 하나만 골라 사용하는 것은 바람직하지 않다. 아래는 어떤 경우에 어떤 바인딩 기술을 활용하는 것이 좋은지에 대한 몇 가지 시나리오이다. 사용자 정의 타입 바인딩을 위한 일괄 적용 : Converter 앞의 Level enum처럼 애플리케이션에서 정의한 타입이면서 모델에서 자주 활용되는 타입이라면 Converter로 만들고 ConversionService로 묶어서 일괄 적용하는 것이 편리하다. 메타정보를 활용하는 조건부 바인딩 : ConditionalGenericConverter 바인딩이 특정 조건(필드, 메서드 파라미터, 애노테이션 등)에 따라 다르게 동작할 때에는 ConditionalGenericConverter를 이용해야 한다. 구현이 까다롭다. 애노테이션을 통한 바인딩 : AnnotationFormatterFactory, Formatter 애노테이션을 통해 바인딩 하고 싶을 경우 사용하면 좋다. 특정 필드에만 바인딩 : PropertyEditor 특정 모델의 특정 필드에 제한해서 바인딩을 적용해야 할 경우 PropertyEditor를 사용하는 것이 편리하다. 필드 이름을 메서드 파라미터로 전달할 수 있기 때문이다. 이렇듯 여러 바인딩 기술들을 등록하다 보면 서로 중복되는 부분이 발생할 것이다. 이럴 경우 우선순위에 의해 바인딩이 적용된다. Custom PropertyEditor &gt; ConversionService &gt; Default PropertyEditor 중복 시 위의 순서로 바인딩된다. 그리고 WebBindingInitializer를 통해 등록한 공통 바인딩은 @InitBinder보다 우선순위가 뒤쳐진다. WebDataBinder 설정 항목 WebDatBinder에는 PropertyEditor, ConversionService 등록 외에도 여러 유용한 바인딩 옵션들이 있다. allowedFields, disallowedFields @ModelAttribute를 사용할 경우 근본적인 보안 문제가 하나 있다. @SessionAttributes를 사용해 변경할 필드만 폼에 표출했다고 하더라도 사용자가 임의로 폼을 조작하여 전달하는 값에 대해서는 변경을 막지 못한다는 점이다. 폼에는 표출하지 않았지만 사용자가 예를 들어 level이라는 필드를 폼에 추가하여 전송할 경우 실제 값이 바뀌는 일이 생길 수도 있다는 것이다. 이를 대비해 폼에 표출한 필드 외에는 모델에 바인딩 되지 않도록 설정할 필요가 있다. 여기에 사용되는 것이 위의 두 속성이다. allowedFields에는 바인딩을 허용할 필드 목록을 넣을 수 있고, disallowedFields에는 바인딩을 금지할 필드 목록을 넣을 수 있다. 1234@InitBinderpublic void initBinder(WebDatBinder dataBinder)&#123; datBinder.setAllowedFields("name", "email", "tel", "*level*");&#125; 이러면 위의 지정한 필드명 외에 다른 필드는 아무리 HTTP 요청으로 보내봐야 바인딩 되지 않는다. 게다가 *level*처럼 와일드카드도 사용할 수 있다. requiredFields 필수 파라미터를 지정할 수 있다. @ModelAttribute의 특성 상 파라미터가 들어오지 않았다고 바로 에러를 발생 시키지 않고 BindingResult에 검증 결과를 저장한다. 하지만 setRequiredFiedls() 메서드로 필수 파라미터를 지정해 줄 경우, 파라미터가 들어오지 않으면 바로 에러가 발생한다. fieldMarkerPrefix input checkbox는 조금 특별한 성질이 있다. 1&lt;input type="checkbox" name="type" value="on" /&gt; 폼에 이와 같은 체크박스가 있다고 했을 때, 이를 체크하고 전달하면 type=on의 형태로 데이터가 전달되지만, 체크하지 않고 전달하면 아예 값을 전달하지 않는다는 점이다. 즉 수정폼에서 기존에 체크되어있던 체크박스를 해제하고 전달할 경우 아무런 값도 전달되지 않기 때문에 사용자는 값을 변경할 수 없는 문제가 발생하게 되는 것이다. 이럴 때 필드마커라는 것을 이용해 해결 할 수 있는데, 아래와 같다. 12&lt;input type="checkbox" name="type" /&gt;&lt;input type="hidden" name="_type" value="on" /&gt; _type의 앞에 붙은 _를 필드마커라고 하는데, 스프링은 이런 필드마커가 있는 필드를 발견할 경우, 필드마커를 제외한 이름의 필드가 폼에 존재한다고 생각한다. 즉, 체크박스를 선택하지 않아 type 파라미터가 전달되지 않았지만, _type필드가 전달 되었으므로 스프링은 type필드가 폼에 있다고 판단하는 것이다. 그리고 이처럼 _type은 전달되고 type은 전달되지 않았을 경우, 체크박스를 해제했기 때문이라 생각하고 해당 프로퍼티 값을 리셋해준다. 리셋 방식은 boolean 타입이면 false, 배열타입이면 빈 배열, 그 외라면 null을 넣어주는 것이다. WebDataBinder의 setFieldMarkerPrifix() 메서드는 이 필드마커를 변경해주는 메서드이다. 기본값은 _이다. fieldDefaultPrefix 필드 디폴트는 히든 필드를 이용해 체크박스의 디폴트 값을 지정하는데 사용한다. 12&lt;input type="checkbox" name="type" value="A"/&gt;&lt;input type="hidden" name="!type" value="Z" /&gt; !type 히든 필드를 지정해서 type필드의 기본값을 지정해줬다. 이럴 경우 체크박스를 선택하지 않아 type필드가 전달되지 않을 경우, 디폴트 값인 Z가 전달된다. 모델 프로퍼티 값이 단순값이 아닐 경우 유용하게 사용할 수 있다. 이 또한 setFieldDefaultPrefix()메서드를 이용해 접두어를 변경해 줄 수 있다. 기본값은 !이다. 검증 @ModelAttribute의 바인딩 작업이 실패로 끝나는 경우는 2가지가 있다. 첫째로 @ModelAttribute가 기본적으로 실행하는 타입 변환에서 오류가 발생했을 경우이고, 둘째로 검증기(validator)를 통과하지 못했을 경우이다. 이는 사용자가 직접 정의하는 부분이다. 사실상 폼의 서브밋을 처리하는 컨트롤러 메서드에서는 검증기를 이용한 검증 작업은 필수이다. 검증 결과에 따라 다음 스텝으로 넘어가든, 다시 폼을 띄워 수정을 요구하든 해야한다. 이 과정에서 쓰이는 API인 Validator, BindingResult, Errors에 대해 알아보자. Validator 오브젝트 검증기를 정의할 수 있는 API이다. @ModelAttribute 바인딩 때 주로 사용된다. 아래는 Validator 인터페이스이다. 12345public interface Validator&#123; boolean supports(Class&lt;?&gt; clazz); void validate(Object target, Errors erros);&#125; supports()는 이 검증기가 검증할 수 있는 타입인지 확인하는 메서드이고, 이를 통과할 경우 validate()를 통해 검증이 진행된다. validate()의 검증과정에서 아무 문제가 없으면 메서드를 정상 종료하면 되고, 문제가 있을 시 Errors 인터페이스에 오류정보를 등록해주면 된다. 이후 이 오류정보를 통해 컨트롤러에서 적절한 작업을 해주면 되는 것이다. 자바스크립트로 입력값을 검증했을 경우 서버에서 검증작업을 생략해도 될까? 안된다. 서버의 검증작업을 생략하면 매우 위험해진다. 브라우저에서 자바스크립트가 동작하지 않게 할수도 있고, 강제로 폼을 조작할수도 있고, Burp suite 같은 것을 사용하여 전달되는 데이터를 변경할 수도 있다. 그러므로 서버 검증작업은 필수로 있어야 한다. 아래는 Validator 구현의 예시이다. 1234567891011121314151617181920212223242526272829public class UserValidator implements Validator&#123; @Override public boolean supports(Class&lt;?&gt; clazz) &#123; // 자식클래스도 검증 가능하게 하기 위해 isAssignableFrom을 사용 return User.class.isAssignableFrom(clazz); &#125; @Override public void validate(Object target, Errors errors) &#123; User user = (User)target; // supports를 통과했으므로 바로 캐스팅하면 됨 // name 필수 if(user.getName() == null || user.getName().length == 0)&#123; errors.rejectValue("name", "name.required"); &#125; // null 체크 &amp; 길이 체크가 귀찮으면 아래와 같이 사용 가능 ValidationUtils.rejectIfEmpty(erros, "name", "name.required"); if(user.getAge() &lt; 0)&#123; // argument도 전달 가능 errors.rejectValue("age", "age.min", new Object[]&#123;0&#125; /* arguments */, null /* default message */); &#125; if(user.getAge() &lt; 20 &amp;&amp; !"M".equals(user.getSex()))&#123; // 필드명을 안 줄수도 있다 errors.reject("cannot.enter.army"); &#125; &#125;&#125; 주석에도 써놓았지만 오류 정보를 등록하는 방법이 다양하다. rejectValue()에 사용된 name은 필드 이름이며, name.required는 에러 코드를 정의한 것이다. (이 에러코드는 messageSource와 함께 사용될 수 있다. 사용 방법 보기) age 필드를 검증할때, 보다시피 에러코드에 파라미터를 전달할수도 있으며 디폴트 메세지도 전달할 수 있다. 제일 아랫부분처럼 2가지 이상의 필드에 대해 검증하는 경우, 필드명을 생략 가능하다. ValidationUtils 같은 유틸리티 클래스도 제공되니 잘 활용하면 좋다. Validator는 싱글톤 빈으로 등록 가능하기 때문에 서비스 로직을 이용하여 검증작업을 진행할 수도 있다. 대표적인 것이 아이디 중복 검사이다. 근데 사실 이 정도 검증까지 가면 좀 모호해지는게 있는데, 검증이 수행되는 계층이다. 검증 작업을 컨트롤러 로직이라고 보는 개발자도 있는 반면, 대부분이 서비스 계층과 연관이 있으니 서비스 계층의 로직이라고 보는 개발자도 있다. 이는 개인이 잘 판단하면 될 문제인 것 같다. 중요한 것은 어느 곳에서 사용하든, 위와 같이 검증로직은 따로 분리되어 있는 것이 좋다. Validator 사용하기 컨트롤러 메서드 내에서 검증 Validator는 빈으로 등록 가능하니 이를 컨트롤러에서 DI 받은 뒤, 각 컨트롤러 메서드에서 validate()를 직접 호출해서 검증을 진행하는 방식이다. (모델 오브젝트의 타입은 굳이 확인할 필요 없으므로 supports()는 생략 가능하다) 123456789101112131415@Controllerpublic class UserController&#123; @Autowired UserValidator userValidator; @RequestMapping("/user", method=RequestMethod.POST) public String userAdd(@ModelAttribute User user, BindingResult result)&#123; this.userValidator.validate(user, result); if(result.hasError())&#123; // 오류 정보가 있을 시 &#125; else&#123; // 오류 정보가 없을 시 &#125; &#125;&#125; @Valid를 이용한 자동 검증 JSR-303의 @javax.validation.Valid 애노테이션을 사용하는 방법이다. 컨트롤러에서 직접 validate()를 호출하여 검증하던 방식과 달리, 바인딩 과정에서 자동으로 검증이 진행되도록 할 수 있다. 1234567891011121314@Controllerpublic class UserController&#123; @Autowired UserValidator userValidator; @InitBinder public void initBinder(WebDataBinder dataBinder)&#123; dataBinder.setValidator(userValidator); &#125; @RequestMapping("/user", method=RequestMethod.POST) public String userAdd(@ModelAttribute @Valid User user, BindingResult result)&#123; // ... &#125;&#125; WebDataBinder에는 보다시피 Validator 타입의 검증용 오브젝트도 등록할 수 있다. 그리고 아래 @ModelAttribute를 사용하는 부분에 추가로 @Valid 애노테이션을 사용해주면 자동으로 검증작업이 수행된다. 개인적으로 위의 방식보다 훨씬 나아 보인다 ㅋㅋ 참고로 @InitBinder 말고 WebBindingInitializer를 이용해 모든 컨트롤러에 일괄 적용할 수도 있다. 서비스 계층에서 검증 자주 사용되지 않지만 Validator가 싱글톤 빈으로 등록되기에 서비스 계층에서도 얼마든지 DI 받아 사용할 수 있다. 서비스 계층에서 반복적으로 같은 검증작업을 수행할 경우 사용하기도 한다. 근데 이럴 경우, BindingResult 타입 오브젝트를 직접 만들어서 validate()에 전달해야 하는데, 이때는 BeanPropertyBindingResult를 사용하는 것이 적당하다. 서비스 계층을 활용하는 Validator Validator는 싱글톤 빈으로 등록될 수 있으므로 다른 빈을 DI받아 사용할 수 있다. 앞서 예시로 들었던 ID 중복 검사처럼, Validator내에서 서비스 계층 빈을 사용하여 검증할 수도 있다. 게다가 이 경우 결과를 BindingResult에 담으면 되므로 서비스 계층에서 번거롭게 예외를 던지던 방식을 제거할 수 있다. 대신 이 방식을 사용하면 컨트롤러에서 서비스 계층을 두번 호출한다는 단점이 있다. 하지만 전체적으로 코드가 깔끔해지고 역할 분담이 확실해지는 장점이 있다. JSR-303 빈 검증 기능 @Valid를 포함하고 있는 JSR-303의 빈 검증 방식도 스프링에서 사용할 수 있다. 1234567public class User&#123; @NotNull String name; @Min(0) int age;&#125; 이런식으로 모델에 특정 애노테이션만 작성해주면 된다. Validator를 직접 구현해서 검증기를 만들필요 없이 간단하게 검증작업을 진행할 수 있다. 이 검증 방식을 사용하려면 LocalValidatorFactoryBean을 사용해야 한다. LocalValidatorFactoryBean이 생성하는 클래스 타입은 Validator이므로 이를 빈으로 등록한 뒤 DI받아 사용하면 된다. (컨트롤러에서 직접 생성해도 되고, WebDataBinder에 등록해도 된다) BindingResult의 에러 코드 앞서 Validator에서 errors.rejectValue(&quot;name&quot;, &quot;name.required&quot;)와 같이 에러코드를 지정하던 작업이 기억날 것이다. 이 정보는 보통 컨트롤러에 의해 폼을 다시 띄울 때 활용된다. 스프링은 등록된 에러 코드를 아래의 같은 파일에서 찾아와 에러 메세지로 활용한다. 1name.required=이름은 필수로 입력하셔야 합니다. 근데 이렇게 지정한 에러코드로 바로 메세지를 찾는것은 아니고, 스프링의 MessageCodeResolver라는 것을 거쳐 에러코드를 확장하는 작업을 한번 거친다. (스프링의 디폴트 MessageCodeResolver는 DefaultMessageCodeResolver 이다.) 이 리졸버를 거치게 되면 우리가 등록한 에러코드 name.required는 아래와 같이 4가지 에러코드로 확장된다. 에러코드.모델이름.필드이름 : name.required.user.name 에러코드.필드이름 : name.required.name 에러코드.타입이름 : name.required.User 에러코드 : name.required 이 4가지 에러코드는 위에서부터 우선순위를 가진다. 즉 메세지 파일이 아래와 같다면, 12name.required.user.name=무언가 잘못된 메세지name.required=이름은 필수로 입력하셔야 합니다. 우리는 에러코드를 분명 name.required라고 지정했지만 계속해서 무언가 잘못된 메세지가 출력될 것이다. 그러므로 에러코드 지정 시 어떤 에러코드로 확장되는지 정확히 알고 있어야 한다. 아니면 위처럼 의도하지 않은 상황이 발생할 수 있기 떄문이다. (개인적으로 좀 혼란스러운 방식이라고 생각한다…) 위는 한가지 예시였을 뿐이고, 검증방식(rejectValue, reject, 타입 오류 등, JSR-303)에 따라 에러코드가 확장되는 룰이 다르니 사용할 떄 주의해야 한다. MessageSource 위에서 확장된 에러코드는 마지막으로 MessageSourceResolver라는 것을 거쳐 실제 메세지로 생성된다. 이 때 사용하는 것이 이 MessageSource이다. 이는 디폴트로 등록되지 않으니 스프링의 빈으로 직접 등록해줘야 한다. MessageSource는 2가지 종류가 있는데 보통 ResourceBundlerMessageSource를 사용한다. 이는 일정시간마다 메세지 파일 변경 여부를 확인해서 메세지를 갱신해주므로 서버가 구동중인 상황에도 메세지를 변경해줄 수 있다. 1&lt;bean id="messageSource" class="org.springframework...ResourceBundleMessageSource" /&gt; 속성값으로 메세지 파일을 지정해주지 않을 경우 디폴트로 messages.properties 파일이 사용된다. MessageSource는 아래의 4가지 정보를 활용해 최종 메세지를 생성한다. 코드 메세지 파일은 키=벨류의 형태로 등록되어 있기 때문에, 메세지를 찾을 키 값은 필수이다. 앞서 우리가 Errors에 등록했던 에러 코드가 이 키 값인 것이다. 메세지 파라미터 배열 앞서 에러코드를 등록하며 Object[] 타입의 파라미터를 넘겨줬던 것을 기억할 것이다. 해당 파라미터는 메세지를 생성하는데 사용될 수 있다. 메세지 파일은 아래와 같이 작성된다. 1field.min=&#123;0&#125;보다 작은 값을 사용할 수 없습니다. 파라미터는 1개 이상 올 수 있기때문에 Object 배열을 사용한다. 디폴트 메세지 코드에 맞는 메세지를 찾지 못하였을때 디폴트 메세지를 지정해줄 수 있다. 에러코드를 충실히 적용했다면 이 부분은 생략하거나 null로 주면 된다. 참고로 코드에 해당하는 메세지도 없고, 디폴트도 없을 경우 예외가 발생하니 주의해야 한다. 지역정보 LocaleResolver에 의해 결정된 현재의 지역정보를 사용할 수 있다. 지역정보에 따라 다른 프로퍼티 파일을 사용 가능하다. 만약 messages.properties 파일을 사용했다면 Locale이 ENGLISH일 경우 messages_en.properties 파일이 사용된다. 이를 통해 다국어 서비스를 적용할 수 있다. 모델의 사이클 모델은 MVC 아키텍쳐에서 정보를 담당하는 컴포넌트이다. 요청 정보를 담기도 하고, 비즈니스 로직에 사용되기도 하고, 뷰에 출력되기도 한다. 또한 이 모델은 아주 여러곳을 거쳐가며 만들어지고, 변형된다. 그러므로 모델의 사이클에 대한 지식은 스프링 MVC를 사용할 떄 가장 중요하다고 할 수 있다. HTTP 요청에서 컨트롤러 메서드까지 왼쪽에서 오른쪽으로 보면 된다. 컨트롤러 메서드에서 뷰까지 오른쪽에서 왼쪽으로 보면 된다. 모델의 생성, 변형, 사용이 한눈에 볼 수 있게 잘 표시되어 있다. From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>@ModelAttribute</tag>
        <tag>토비의 스프링</tag>
        <tag>프로퍼티 바인딩</tag>
        <tag>PropertyEditor</tag>
        <tag>Converter</tag>
        <tag>Formatter</tag>
        <tag>WebDataBinder</tag>
        <tag>Validator</tag>
        <tag>BindingResult</tag>
        <tag>Errors</tag>
        <tag>모델 사이클</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] mysql index]]></title>
    <url>%2Fdb%2Fmysql-index%2F</url>
    <content type="text"><![CDATA[순차 IO / 랜덤 IO 기본적으로 하드는 데이터를 읽을때 원판 플래터를 회전시키며 데이터를 찾는다. 순차 IO란 시작위치에 간 뒤 쭉 읽어서 데이터를 찾는 것을 말하고, 랜덤 IO란 여러 위치를 탐색해서 최종적으로 데이터를 찾는 것을 말한다. 인덱스 레인지 스캔의 경우 랜덤 IO, 테이블 풀 스캔의 경우 순차 IO를 사용한다. 보다시피 당연히 랜덤 IO가 순차 IO 보다 성능이 떨어진다. 하지만 여기서 SSD를 사용하면 얘기가 달라진다. SSD는 HDD와 달리 플래터에 데이터를 기록하지 않고, 플래시 메모리라는 것에 데이터를 저장한다. 이는 원판을 기계적으로 회전시킬 필요가 없으므로 데이터를 매우 빠르게 찾을 수 있다. 그러므로 DBMS용 스토리지에는 SSD가 최적이라고 볼 수 있다. 인덱스란 책을 데이터에 비유한다면, 인덱스는 색인에 비유할 수 있다. 색인에서 키워드와 페이지 번호를 쌍으로 연결해놓았듯이, 인덱스 또한 인덱스 컬럼 값들과 레코드 주소를 키,벨류 형태의 쌍으로 저장해놓은 것을 말한다. 즉 데이터를 검색 할 때 인덱스를 통하는 방식으로 원하는 레코드에 빠르게 접근 할 수 있다. 자료구조로 비교해봤을 때 인덱스는 SortedList, 데이터 파일은 ArrayList 이다. 인덱스는 항상 정렬된 상태로 유지되고, 데이터 파일은 들어온 순서대로 저장된다.(앞이 비어져있을 경우 앞부터 채운다) 이미 정렬된 상태이므로 검색에는 매우 빠르나, 데이터가 추가되거나 삭제될 경우 다시 정렬해줘야 한다. 즉 데이터 조작의 성능을 희생하고 검색의 속도를 높이고자 할 경우 사용하는 것이 좋다. 인덱스가 많으면 많을수록 데이터 조작에서 발생하는 오버헤드가 많으므로, 적절하게 생성해야 한다. 인덱스를 역할로 분류했을 때, primary key와 secondary key로 분류할 수 있다. primary key는 지정과 동시에 바로 인덱스가 생성된다. secondary key는 primary key가 아닌 인덱스들을 얘기한다. unique key의 경우 primary key와 거의 비슷하므로 대체키 라고도 한다. 인덱스 저장 방식으로 분류했을 때, B-Tree 방식(가장 일반적으로 많이 쓰임), 해시 방식, Fractal 방식 등이 있다. 인덱스를 중복 여부로 구분했을 때, 값이 고유한 데이터들에 대한 인덱스와 중복된 값에 대한 인덱스 둘 다 생성할 수 있다. DBMS 입장에서 값이 고유하다는 것은 그 데이터를 찾았을 경우 더이상 탐색하지 않아도 된다는 것을 의미하므로, 성능 부분에서 매우 유리하다. B-Tree 인덱스 가장 일반적으로 사용되는 인덱스 저장 자료구조. 컬럼의 값을 아무런 변형없이 저장한다. 구조 최상단의 루트 노드, 최하단의 리프 노드, 그 둘을 잇는 여러개의 브런치 노드들이 있다.(저장된 데이터가 작을 경우 브런치 노드는 없을 수 있다. 루트랑 리프노드는 항상 존재한다.) 루트 노드들은 각각 자식 노드들의 주소를 가지고, 최하단 리프노드는 저장된 레코드의 주소를 가진다. 인덱스 키 추가 및 삭제 추가 테이블에 레코드를 추가하게 되면 키 값을 이용해 들어갈 리프노드의 위치를 찾고, 추가한다. 만약 리프노드에 더 이상 들어갈 공간이 없으면 리프노드를 하나 더 추가하게 되는데, 여기서 상위 리프노드 또한 조정되어야 한다. 이러한 이유 때문에 B-Tree 인덱스에서는 데이터의 추가 작업 비용이 높다. MyISAM이나 Memory DB의 경우 레코드 추가 -&gt; 인덱스 추가의 작업이 바로바로 이뤄지므로 인덱스에 데이터가 들어가기 전까지 사용자는 결과를 받지 못한다. 이에 비해 InnoDB는 이를 좀 더 유연하게 처리한다. 레코드가 추가되었을 때 리프노드에 들어갈 공간이 남았을 경우 바로 추가하고, 공간이 없을 경우 인서트 버퍼라는 곳에 따로 저장해둔다. 이후 백그라운드 프로세스에서 인덱스를 읽을때나 데이터베이스 서버의 자원이 여유로울 경우 인서트 버퍼 스레드에서 인서트 버퍼를 체크한 뒤 인덱스에 머지한다. 중요한 것은 사용자가 이 작업을 인지하지 않아도 되게끔 투명하게 처리된다는 것이다. 삭제 레코드가 삭제되면 그에 해당하는 인덱스에 삭제마크를 표시한다. 이후 저장되는 레코드는 마지막 리프노드 뒤에 붙을수도 있고, 삭제마크된 부분을 재활용하여 인서트 될 수도 있다. 삭제마크를 표시하는 작업도 인서트와 마찬가지로 버퍼를 이용해 지연 처리할 수 있다(MySql 5.5부터) 변경 인덱스 키 값에 따라 인덱스가 들어갈 리프노드의 위치가 정해지므로, 변경은 불가능하고 삭제 -&gt; 추가의 작업으로 진행된다. 작업 방식은 위의 삭제, 추가 방식과 동일하다. 검색 동등연산, 범위연산, Like(검색어%) 연산에서 인덱스를 사용할 수 있다. 부정연산, Like(%검색어)에서는 인덱스를 사용할 수 없다. 또한 인덱스 키 값에 변형이 일어난 경우(연산, 형변환)에도 인덱스를 이용한 빠른 검색이 불가능하다. 인덱스 사용에 영향을 미치는 요소 기본적으로 MySql에서는 데이터의 저장 공간에 페이지라는 최소 단위를 사용한다. 인덱스의 각 노드들도 하나의 페이지로 볼 수 있다. MySql에서 페이지의 기본 단위는 16KB이다. 변경하려면 소스를 수정하고 컴파일해야 한다. 인덱스 키 값의 크기 인덱스 키 값의 크기가 커지면 자연스럽게 하나의 리프 노드(페이지)에 들어갈 수 있는 데이터의 개수가 작아진다. 만약 인덱스 키 값의 크기가 16바이트이고, 저장된 주소값의 크기가 12바이트 정도라고 하자. 이럴 경우 하나의 리프 노드에 들어갈 수 있는 데이터의 개수는 (16+12)/16KB 해서 585개가 된다. 그리고 만약 인덱스 키 값의 크기가 32바이트라면 (32+12)/16KB 해서 372개가 된다. 이런 상황에서 인덱스를 500개 읽어야 한다고 가정해보면, 리프노드 하나만 읽어도 되었을 것을 리프노드를 2개에 걸쳐 읽어야 하는 상황이 발생한다. 이로 인해 추가적인 I/O가 발생하게 되고, 속도가 느려지게 된다. 노드의 깊이 만약 2억개의 인덱스를 저장해야 하는 상황이 있다고 가정해보자. 인덱스 키 값의 크기가 16바이트일 경우 한 리프당 585개의 데이터가 저장 가능하기 때문에 585^3 = 200,201,625 로 3depth로 2억개의 레코드에 대한 인덱스를 저장 가능하다. 하지만 만약 32바이트일 경우 한 리프당 372개만 저장 가능하기 때문에 372^3 = 51,478,848 밖에 안되므로 3depth로 모든 인덱스를 저장하지 못하고, depth가 깊어지는 상황이 발생한다. 당연하게도 depth가 깊어지면 그만큼 I/O가 늘어나게 되고, 속도가 느려지게 된다. 실제로 depth가 아무리 깊어져도 4-5depth라고 한다. 인덱스 키 값의 크기를 작게해야 한다는 것을 강조하기 위한 약간 극단적인 예시였다. 선택도(분포도) 인덱스에 값의 그룹이 많을 경우 분포도가 좋다고 하고, 값의 그룹이 작을 경우 분포도가 나쁘다고 한다. 예를 들어 성별 같은 경우 값의 그룹이 남,여 뿐이므로 분포도가 상당히 나쁜 편이다. 아래과 같은 상황이 있다고 가정해보자 1234-- index : 단일 인덱스(country)-- 실행할 쿼리SELECT * FROM test_table WHERE country='KOREA' AND city='SEOUL'; 테이블의 데이터는 10,000개라고 가정하고, country='KOREA' AND city='SEOUL'인 레코드가 1건 뿐이라고 가정해보겠다. 저장된 나라가 총 10개뿐일 경우(분포도 낮음) country가 KOREA인 데이터 1000개를 인덱스로부터 읽어온 뒤, 데이터파일에 랜덤 액세스를 하며 city가 SEOUL인 데이터를 찾는다. 총 999건의 불필요한 검색을 하게 된다. 저장된 나라가 총 1,000개일 경우(분포도 높음) country가 KOREA인 데이터 10개를 인덱스로부터 읽어온 뒤, 데이터파일에 랜덤 액세스를 하며 city가 SEOUL인 데이터를 찾는다. 총 9건의 불필요한 검색을 하게 된다. 보다시피 1번과 같은 인덱스는 좋지 않다고 할 수 있다. 어쩌피 모든 데이터 상황에 맞출 수 없으므로 불필요한 검색을 0건으로 만드는 것은 거의 불가능하다. 그래도 최대한 2번 인덱스처럼 검색하여 낭비를 최소화 하도록 해야 한다. 읽어야 하는 데이터의 양 일반적으로 인덱스를 이용해 레코드를 읽는 행위가 레코드를 직접 읽는 행위에 비해 3-4배 정도 비용이 크다고 산정한다. 인덱스의 리프 노드까지 가서 레코드의 주소를 찾고 이 주소로 레코드를 읽는 과정에서 랜덤 I/O가 발생하기 때문이다. 인덱스는 각자의 정렬기준으로 정렬되어 있지만 데이터 파일은 그렇지 않기 때문이다. (인덱스를 통해 3건의 데이터를 찾았을 경우 총 3번의 랜덤 I/O가 발생하는 것이다.) 그래서 테이블 전체 레코드 개수의 20-25%를 넘는 데이터를 인덱스로부터 읽어야 할 경우에 옵티마이저는 그냥 풀 테이블 스캔을 시전한다. (풀 테이블 스캔의 경우 그냥 순차 I/O로 읽어내리기 때문) 여기서 강제로 인덱스를 타게 해봐야 성능상 별로 효과가 없다. 인덱스를 이용해 데이터를 읽는 법 인덱스 레인지 스캔 가장 빠른 스캔 방법이다. 동일 연산자로 하나만 읽으나 범위 연산자로 여러개를 읽으나 모두 인덱스 레인지 스캔으로 분류한다. 루트 노드부터 브랜치 노드를 따라 리프 노드의 데이터(들)를 읽는 방식이다. 이를 탐색한다라고 한다. 인덱스 풀 스캔 인덱스를 처음부터 끝까지 다 읽는 방식이다. 인덱스에 저장된 데이터만으로 모든 것을 처리할 수 있는 경우이거나, 멀티인덱스의 중간 값 부터 조건을 지정하였을 경우 발생한다. 리프 노드의 첫번째 데이터 부터 순차적으로 읽어 내려가며, 하나의 리프노드가 끝났을 경우 해당 리프노드의 링크드리스트를 통해 다음 리프노드로 넘어가 끝까지 읽는 방식이다. 인덱스 풀 스캔은 좋은 방식이 아니다. 루스 인덱스 스캔 말 그대로 루~스하게, 인덱스를 다 읽지않고 듬성듬성 읽는것을 말한다. 중간마다 필요치 않은 인덱스 키 값을 무시하고 다음으로 넘어가는 형태로 처리한다. 예를 들면 아래와 같이 GROUP BY 집합 함수 가운데 MIN, MIX에 대한 최적화를 할 때 사용할 수 있다. 123456-- index : 복합 인덱스(dept_no, emp_no)SELECT dept_no, MIN(emp_no)FROM dept_empWHERE dept_no BETWEEN 'd002' AND 'd004'GROUP BY dept_no; 인덱스는 이미 dept_no, emp_no 의 순서로 정렬되어 있기 때문에 emp_no의 MIN값을 찾고자 할 경우 dept_no 인덱스의 첫번째 컬럼만을 읽으면 된다. 나머지 애들은 굳이 읽을 필요가 없다. DISTINCT도 루스 인덱스 스캔을 사용한다. 처음 한건만 읽으면 되기 때문이다. 다중 컬럼 인덱스 보통 1개의 컬럼으로 인덱스를 생성하기 보단 여러개의 컬럼으로 인덱스를 생성하는 경우가 많다. 다중 컬럼으로 인덱스를 생성할 경우 인덱스의 순서를 신중하게 생각해야 하는데, 이는 인덱스의 정렬이 자신의 앞 인덱스의 정렬에 의존하기 때문이다. 예를 들어 dept_no과 emp_no 컬럼으로 다중 컬럼 인덱스를 생성했다고 가정해보자. 이때 emp_no의 값이 아무리 낮더라도, 짝지어진 dept_no의 값이 높을 경우 해당 데이터는 인덱스의 아래쪽에 쌓이게 된다. 첫번째 컬럼인 dept_no에 의존하기 때문이다. 이 때문에 다중 컬럼 인덱스를 생성할 때에는 순서에 매우 신중해야 한다. 인덱스의 효율(속도)와 연관이 있기 때문. 인덱스 정렬 및 스캔 방향 현재는 어떤지 모르곘으나… MySQL 5.대만 해도 인덱스 생성시 정렬 기준을 주는 것이 불가능했다. 1CREATE INDEX idx_test ON test_table(col1 ASC, col2 DESC) 이렇게 작성해봐야 모두 ASC로 생성된다는 뜻이다. 한쪽 정렬이 적용된 다중 컬럼 인덱스를 가진 테이블에서 컬럼마다 정렬을 지정할 경우, 추가적인 정렬이 일어나기 때문에 절대 빠르게 처리될 수 없다. 이와 같은 상황에서 위와 같이 인덱스를 처리하는게 제일 좋긴하나… 안된다. 그래서 역 값을 줘서 위의 처리가 동작하게 하는 방식을 사용하곤 한다.(col2의 값을 전부 -로 세팅) MySQL 옵티마이저는 기본적으로 ASC, DESC에 대한 개념이 있다. ASC로 요청할 때에는 인덱스의 최소값(위)부터 차례로 읽으면 된다는 것을 알고, DESC로 요청할 때에는 인덱스의 최대값(아래)부터 차례로 읽으면 된다는 것을 알고 있다. 이러한 특성 때문에 우리는 정렬을 공짜로 얻을 수 있다! 효율성 및 가용성 인덱스 레인지 스캔이 불가능한 경우 기본적으로 B-Tree는 왼쪽의 데이터에 의존하는 방식이다. 루트 노드부터 해서 여러 depth를 거쳐 리프 노드를 찾는 방식도 결국 왼쪽의 정렬 기준에 의존하는 것이고, 다중 컬럼에서 인덱스의 저장 구조를 보았을 때도 결국 왼쪽의 정렬 기준에 의존하는 것이다.(N번째 컬럼은 N-1번째 컬럼의 정렬기준에 의존한다는 정의) 이런 상황에서 왼쪽의 기준이 불명확할 경우, 인덱스 레인지 스캔이 불가능해진다. 예를 들어 아래와 같은 쿼리가 있다고 하자. (인덱스 = firstname) 1SELECT * FROM employees WHERE first_name LIKE '%mer'; 왼쪽 값을 기준으로 트리를 형성하는 B-Tree 인덱스인데, 위의 조건은 문자열의 왼쪽값이 정해지지 않았으므로 인덱스를 통한 탐색이 불가능한 쿼리이다. 실제로 돌려보면 테이블 풀 스캔을 한다. 이번에는 다중컬럼 인덱스이다. (인덱스 = dept_no, emp_no) 1SELECT * FROM dept_emp WHERE emp_no &gt;= '11444'; 이 또한 dept_no을 기준으로 인덱스가 정렬되어 있는데 emp_no 부터 조회했으므로 인덱스를 사용하지 못한다. 루트 노드로 들어갈 수 조차 없기 때문이다. 실제로 돌려보면 테이블 풀 스캔을 한다. 인덱스 사용 불가 조건 부정 연산(!=, &lt;&gt;, NOT IN, NOT BETWEEN, IS NOT NULL) 우측 일치 LIKE(LIKE %mer, LIKE _mer) 스토어드 프로시저로 비교할 경우(WHERE col1 = stored_procedure()) 값을 강제로 변경했을 경우(WHERE SUBSTRING(col1, 1, 1) = ‘X’) 타입이 달라 컬럼에 형변환이 일어났을 경우(WHERE ch_col1=1001) 작업 결정 범위 조건, 체크 조건 가용성 dept_no, emp_no 순서의 인덱스와 emp_no, dept_no 순서의 인덱스가 있고, 아래와 같은 쿼리를 실행한다고 가정해보자. 1SELECT * FROM dept_emp WHERE dept_no='d0002' AND emp_no &gt;= '11444'; 첫번째 인덱스의 경우 dept_no, emp_no의 순서로 정렬되어 인덱스에 저장되어 있으므로 단계적으로 작업의 범위를 줄여나가며 스캔이 가능하다. 하지만 두번째 인덱스의 경우 emp_no, dept_no의 순서로 저장되어 있는데, emp_no에서 동등조건이 아닌 범위 조건을 하고 있다. 이러면 이후 dept_no은 범위 조건으로 검색된 결과에 대해 자신의 조건이 맞는지 안 맞는지의 필터 조건밖에 수행하지 못하게 된다. 즉, 작업의 범위를 줄여나가며 스캔을 하지 못했다. 위의 A 처럼 조건을 더 해갈수록 작업의 범위를 줄여주는 조건을 작업 결정 범위 조건이라고 하고, 오로지 가져온 데이터에서 해당 조건이 맞는지 안맞는지만을 체크하는 조건을 체크 조건 이라고 한다. 123456789101112왜 범위 연산이 들어가는 순산부터 작업 범위를 좁히지 못하는 것일까? 생각해봤는데.. a | b | c---------1 | 2 | 31 | 2 | 41 | 2 | 5...이런 형태의 인덱스가 있다고 할 때 a,b,c 를 전부 동등조건으로 검색하면 3가지 값이 일치하는 특정 인덱스 로우를 선택할 수 있지만, c를 범위조건으로 검색하면 a,b 만 일치하는 인덱스를 들고온 뒤 범위로 쭉 긁어오기 때문에 특정 인덱스 로우를 선택하는 작업이 불가능해지는게 아닌가 싶다 아래의 인덱스를 보고 어떻게 하면 작업 결정 범위 조건으로 사용할 수 있는지, 어떻게하면 사용하지 못하는지 살펴보자. 1CREATE INDEX idx_test ON test_table(col1, col2, col3, ... colN) 아래와 같이 사용할 경우 작업 결정 범위 조건으로 사용하지 못한다. col1에 대한 조건이 없는 경우 col1에 대한 조건이 위에 인덱스 사용 불가 조건에 부합할 경우 인덱스의 첫 컬럼은 무조건 검색되어야 한다. 루트 인덱스로 들어갈 통로이기 때문이다. 아래와 같이 사용했을 경우 작업 결정 범위 조건으로 사용할 수 있다.(i = 2이상 N이하) col1에 대한 조건부터 coli-1에 대한 조건까지 모두 동등 연산으로 연결되었을 경우 coli에 대한 조건이 범위(&lt;,&gt;), 좌측 일치 LIKE(문자%)일 경우 위의 두 가지 조건이 성립해야 한다. 동등 조건으로 시작해서 범위조건이나 LIKE(좌측 일치) 조건으로 들어가는 곳 까지가 작업 결정 범위 조건이다. 그 이후로는 모두 체크 조건으로 사용된다. ※ 다중 컬럼 인덱스 사용시 WHERE 조건의 순서는 상관없다.(where절의 실행순서야 옵티마이저에 의해 적절히 잘 파싱된다는걸 말하려는 듯) 12SELECT * FROM test_tableWHERE col1 = 'A' AND col2 = 'B' AND col3 &gt;= 100 AND col4 = 'D' AND col5 LIKE '%test'; (WHERE 절의 순서를 바꿔도 결과는 동일하다) 인덱스 col1 부터 하나씩 타고 가다가 col3 에서 범위조건을 만남으로써, 이후의 값들은 체크 조건으로 사용되게 된다 col1 ~ col3 = 작업 결정 범위 조건, col4 ~ col5 = 체크 조건 이는 모든 B-Tree에 적용되는 조건이므로 다른 RDBMS에서도 적용할 수 있다.]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>Real MySQL</tag>
        <tag>인덱스</tag>
        <tag>B-Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] @SessionAttributes, SessionStatus]]></title>
    <url>%2Fspring%2F%40SessionAttributes%2C-SessionStatus%2F</url>
    <content type="text"><![CDATA[문제상황 아래는 기본적인 회원정보 수정 샘플 코드이다. 1234567891011121314151617181920212223@Controllerpublic class UserController&#123; // 수정 폼 @RequestMapping(value="/user/&#123;uid&#125;", method=RequestMethod.GET) public String modifyForm(@PathVariable Long uid, ModelMap modelMap)&#123; User user = userService.getUser(uid); modelMap.addAttribute("user", user); return "user/modify" &#125; // 수정 @RequestMapping(value="/user/&#123;uid&#125;", method=RequestMethod.POST) public String modify(@PathVariable Long uid, @ModelAttribute User user)&#123; userService.modify(user); return "user/modifySuccess" &#125;&#125; 위 코드는 기본적으로 문제가 있다. 수정 폼을 출력하는 메서드에서 uid로 회원을 조회하고, 결과인 User 오브젝트를 모델로 내린다. 여기까진 문제가 없다. 그러나 이후에 수정을 하는 부분에서 문제가 발생한다. 전달받은 User 오브젝트에서 실제로 사용자가 수정할 수 있는 필드는 제한적이라는 것이다. 회원 수정 폼에서는 수정이 필요한 부분만 input 태그로 입력을 받게되고, 이로 인해 최종적으로 modify 메서드를 호출 시 @ModelAttribute는 수정이 필요한 부분만 값이 찬, 불완전한 User 오브젝트를 받게 된다. 이 상태에서 modify 메서드를 실행할 경우 값이 차지 않은 프로퍼티들로 인해 실제 DB 데이터가 null이나 0으로 업데이트 되는 심각한 상황을 초래하게 된다. 이 상황에서 생각할 수 있는 해결법은 대표적으로 3가지가 있다. 해결법1 - hidden 필드 수정하면 안되는 데이터는 모두 hidden 필드에 넣어주는 것이다. 이럴 경우 서버로 모든 값이 전달될 수 있기 때문에 위의 문제점은 해결된다. 123&lt;input type="hidden" name="id" value="joont" /&gt;&lt;input type="hidden" name="level" value="3" /&gt;... 하지만 이는 더 큰 문제를 초래할 수 있다. 첫째로 이 방식은 데이터 보안에 매우 심각한 문제를 초래한다. hidden 필드의 값은 매우 간단하게 조작될 수 있기 떄문이다. 조작된 값은 서버에서 그대로 업데이트 되기 때문에 이는 매우 심각한 문제를 초래할 수 있다. 둘째로 도메인 오브젝트가 변경될 때 마다 매번 hidden 필드도 관리해줘야 한다는 점이다. 그리고 혹여나 실수로 hidden 필드를 하나라도 누락하게 된다면 또 데이터가 null이나 0으로 업데이트되는 상황이 발생할 것이다. 이는 해결법이라고 보기에는 무리가 있어 권장되지 않는 방식이다. 해결법2 - DB 조회 업데이트 전 DB에서 데이터를 한번 읽어와 도메인 오브젝트에 빈 프로퍼티가 없게 하는 방식이다. 12345678910111213@RequestMapping(value="/user/&#123;uid&#125;", method=RequestMethod.POST)public String modify(@PathVariable Long uid, @ModelAttribute User formUser)&#123; User user = userService.getUser(uid); // 수정할 오브젝트만 반영 user.setNickName(formUser.getNickName()); user.setEmail(formUser.getEmail()); userService.modify(originUser); return "user/modifySuccess"&#125; 기능상으로만 보면 완벽하나, 매 submit 마다 DB에서 다시 데이터를 읽어와야 하는 부담이 있다. 또한 수정할 오브젝트를 일일히 세팅해줘야 하는데, 번거롭고 실수할 가능성이 있다. 해결법3 - 계층간 강한 결합 각 계층의 코드를 특정 작업을 중심으로 긴밀하게 결합시키는 방법이다. 간단하게 말해 기본정보 수정은 modifyDefault, 패스워드 수정은 modifyPassword, 프로필 사진 수정은 modifyProfile 과 같이 작성하여 필요한 부분만 업데이트 치게 하는 것이다. 이게 당장은 편리할지 모르나, 애플리케이션의 규모가 조금만 커져도 단점이 드러난다. 각 메서드는 각각 거의 하나의 화면에서만 사용되고, 각 메서드의 재사용성이 떨어진다. 게다가 메서드들을 각 계층마다 써줘야 한다. 이러면 코드의 중복이 발생하고 수정에도 취약하다. 이러다간 서비스 계층과 DAO 계층을 구분하는 것도 의미가 없어질지도 모른다. 그러므로 이 방식은 권장되지 않는 방법이다. 객체지향성이 떨어지고 데이터 중심으로 코드가 작성되기 때문이다. 각 계층이 서로 독립적이여야 재사용성이 높고, 확장이나 변경에도 유연하게 대응이 가능하다. @SessionAttributes 스프링에선 위와 같이 수정 폼을 다루는 상황에서 깔끔한 해결법을 제공해주는데, 바로 세션을 이용하는 것이다. 수정폼 처리 123456789101112131415161718192021222324@Controller@SessionAttributes("user")public class UserController&#123; // 수정 폼 @RequestMapping(value="/user/&#123;uid&#125;", method=RequestMethod.GET) public String modifyForm(@PathVariable Long uid, ModelMap modelMap)&#123; User user = userService.getUser(uid); modelMap.addAttribute("user", user); return "user/form" &#125; // 수정 @RequestMapping(value="/user/&#123;uid&#125;", method=RequestMethod.POST) public String modify(@PathVariable Long uid, @ModelAttribute User user)&#123; userService.modify(user); return "user/modifySuccess" &#125;&#125; 추가된건 달랑 @SessionAttributes 애노테이션 하나이다. 그러나 이 애노테이션으로 인해 앞의 모든 문제점이 해결되었다. 어떻게 동작하길래 그럴까? @SessionAttributes가 제공해주는 기능은 2가지가 있다. 첫째로 컨트롤러의 메서드가 생성하는 모델정보 중에서 @SessionAttributes에 지정한 이름에 동일한 것이 있으면 이를 세션에 저장한다. 위의 상황에서는 수정폼(modifyForm)이 호출될 때 user 오브젝트가 모델에 저장되는 동시에 세션에도 저장된다. 지정한 이름이 동일하기 때문이다. 둘째로 파라미터에 @ModelAttribute 애노테이션이 있을 때 여기 전달할 오브젝트를 세션에서 가져오는 것이다. 원래는 파라미터에 @ModelAttribute가 있으면 새 오브젝트를 생성한 뒤 HTTP 요청 파라미터를 바인딩한다. 하지만 @SessionAttributes를 사용했을 경우는 다르다. 새 오브젝트를 생성하기 전 @SessionAttributes에 지정된 이름과 @ModelAttribute 파라미터의 이름을 비교하여 동일할 경우 오브젝트를 새로 생성하지 않고 세션에 있는 오브젝트를 사용하게 된다. (참고로 비교하는 @ModelAttribute 파라미터의 이름은 타입의 이름이다. User 오브젝트일 경우 이름은 user 이다. 충돌이 발생하지 않도록 유의해야 한다!) 즉, 수정폼을 거친 뒤 수정을 하게되면 비어있는 프로퍼티가 하나도 없는 상태로 수정을 진행할 수 있게 되는 것이다! 이로인해 앞서 고민했던 문제점이 깔끔하게 해결되었다. 역시 대단하다. 아래는 @SessionAttributes를 사용했을때의 흐름을 그림으로 나타낸 것이다. @SessionAttributes를 지정한 클래스내의 모든 메서드의 모델에 적용되며, 하나 이상의 모델을 세션에 저장할 수 있다. 등록폼 처리 @SessionAttributes은 수정폼외에 등록폼에서도 유용하게 사용할 수 있다. 보통 수정폼의 경우 DB에서 조회한 결과를 폼에 보여줘야 하므로 도메인 오브젝트를 유지해야 하지만, 등록폼의 경우 그럴 필요가 없다. 그래서 대부분, 사용자가 폼을 submit 할 때 도메인 오브젝트를 새로 만들게 한다. 1234567@RequestMapping(value="/user", method=RequestMethod.POST)public String save(@ModelAttribute User user)&#123; // User 오브젝트가 새로 생성된다 userService.save(user); return "user/saveSuccess"&#125; 하지만 이럴 경우 모델을 사용하지 않는 등록폼과, 모델을 사용하는 수정폼으로 폼을 총 2개를 만들어줘야 한다. 게다가 등록폼의 경우 모델 정보를 사용하지 않기 때문에 검증 로직에서 오류가 발생했을 경우 등록과 수정을 왔다갔다 거려야하는 꽤나… 아니 매우 번거로운 상황이 발생한다. 이럴바에는 처음부터 아예 모델을 사용하게 하고, 등록폼에는 빈(empty) 오브젝트라도 출력을 시켜주는 편이 낫다. 이렇게 하면 폼을 굳이 두개로 분리할 필요도 없어진다. 그리고 여기다가 @SessionAttributes를 이용하여 매번 폼이 새로 생성되지 않도록 세션에 저장해두는 것이 좋다. 123456789101112131415@Controller@SessionAttributes("user")public class UserController&#123; @RequestMapping(value="/user/add", method=RequestMethod.GET) public String saveForm(ModelMap modelMap)&#123; User user = new User(); // 이런식으로 기본값이 필요한 경우 지정해줄 수 있다 user.setJoinDt(new Date()); modelMap.addAttribute("user", user); return "user/form"; // modifyForm과 동일한 폼 사용 &#125;&#125; SessionStatus @SessionAttributes를 사용해 세션에 저장한 모델이 더 이상 필요없어질 경우 세션에서 제거해줘야 한다. 세션의 제거 시점은 스프링이 알수없으므로 이를 제거하는 책임은 컨트롤러에게 있다. 세션의 경우 서버의 메모리를 사용하므로 필요없는 시점에 제거해주지 않으면 메모리 누수가 발생할 수 있기때문에 항상 빼먹지 않고 제거해줘야 한다. 세션의 제거는 SessionStatus 오브젝트의 setComplete 메서드로 제거할 수 있다. 123456789101112@RequestMapping(value="/user/&#123;uid&#125;", method=RequestMethod.POST)public String modify(@PathVariable Long uid, @ModelAttribute User user, SessionStatus sessionStatus)&#123; userService.modify(user); // 현재 컨트롤러 세션에 저장된 모든 정보를 제거해준다. 개별적으로 제거할 순 없다 sessionStatus.setComplete(); return "user/modifySuccess"&#125; 여기까지가 @SessionAttributes를 이용한 스프링의 전형적인 폼 처리 방식이다. 겉으로 보여주는 것 없이 애노테이션 하나로만 처리하기 때문에 동작방식을 정확히 이해하고 있어야 한다. From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Session</tag>
        <tag>토비의 스프링</tag>
        <tag>@SessionAttributes</tag>
        <tag>SessionStatus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] @Controller]]></title>
    <url>%2Fspring%2F%40Controller%2F</url>
    <content type="text"><![CDATA[여기서 말하는 @Controller란 빈 자동 스캔시 사용되는 스테레오 타입 애노테이션이 아니라, 애노테이션을 이용해 컨트롤러를 개발하는 방법을 말한다. 즉, AnnotationMethodHandlerAdapter가 실행하는 각 메서드들을 의미한다. 파라미터 개발자가 명시한 애노테이션과 파라미터 타입 등에 따라 AnnotationMethodHandlerAdapter가 적절히 변환하여 제공해줌 HttpServletRequest, HttpServletResponse, ServletRequest, ServletResponse 대게는 좀 더 상세한 파라미터 타입을 사용하면 되지만, 원한다면 직접 HttpServletRequest, HttpServletResponse 타입을 받을 수 있다. ServletRequest, ServletResponse 타입도 가능하다. HttpSession HttpServletRequest에서 얻을 수 있는 HttpSession을 바로 받을 수 있다. HttpSession은 멀티스레드 환경에서 안전성이 보장되지 않으므로 핸들러 어댑터의 synchronizeOnSession 프로퍼티를 true로 줘야한다. Locale java.util.Locale 타입으로 DispatcherServlet의 Locale Resolver가 결정한 Locale 오브젝트를 받을 수 있다. InputStream, Reader HttpServletRequest의 getInputStream()를 통해 받을 수 있는 InputStream과, getReader()를 통해 받을 수 있는 Reader를 바로 받을 수 있다. OutputStream, Writer HttpServletResponse의 getOutputStream()를 통해 받을 수 있는 OutputStream과, getWriter()를 통해 받을 수 있는 Writer를 바로 받을 수 있다. @PathVariable @RequestMapping url에 {}로 들어가는 패스 변수를 받는다. 1234@RequestMapping(value="/post/&#123;postNo&#125;")public String detail(@PathVariable("postNo") Integer postNo)&#123; // ...&#125; 속성으로 받을 패스 변수의 이름을 지정할 수 있으며, 전달받은 패스 변수는 선언한 파라미터 타입으로 형변환 된다. 즉, /post/10 이라고 요청하게 되면 postNo 변수에 Integer 타입으로 형 변환되어 담기게 된다. 만약 /post/notNumber 과 같은 형태로 요청하여 형변환이 불가능 할 경우 400 Bad Request 에러가 발생한다. @RequestParam HttpServletRequest의 getParameter()로 받을 수 있는 파라미터를 바로 받을 수 있다. 전달받은 파라미터는 선언한 파라미터 타입으로 자동 형 변환된다. 또한 필수여부, 디폴트 값 등을 설정할 수 있다. 12345// page라는 이름으로 전달된 파라미터를 받아 Integer 타입으로 변환한다public String list(@RequestParam("page") Integer page)// 필수 여부와 디폴트 값을 줄 수 있다. 필수 여부는 default가 true이다. public String list(@RequestParam(value="page", required=false, defaultValue="1") Integer page) 파라미터 타입을 Map으로 선언하면 모든 파라미터를 맵으로 받을 수 있다. 1public String list(@RequestParam Map&lt;String, String&gt; params) @CookieValue 쿠키값을 받아올 수 있다. 속성으로 쿠키의 이름을 지정해주면 된다. 123456// 쿠키 name이 auth인 것을 가져온다public String list(@CookieValue("auth") String auth)// @RequestParam과 마찬가지로 필수 여부와 디폴트 값을 줄 수 있다.// 필수 여부 default는 true이다.public String list(@CookieValue(value="auth", required=false, defaultValue="NONE") String auth) @RequestHeader 헤더값을 받아올 수 있다. 속성으로 헤더의 이름을 지정해주면 된다. @RequestParam, @CookieValue와 마찬가지로 required, defaultValue를 설정해 줄 수 있다. 1public String list(@RequestHeader("Host") String host) Model, ModelMap, Map 모델 정보를 담을 수 있는 Model과 ModelMap 객체를 파라미터 레벨에서 바로 받을 수 있다. Map도 앞에 특별한 애노테이션이 없다면 모델 정보를 담는데 사용할 수 있다. 하지만 갠적으로 좀 헷갈린다… 안써야지 123456public String list(ModelMap model)&#123; model.addAttribute("key", "value"); // collection에 담긴 모든 오브젝트를 모델에 추가할 수 있다(자동 이름 생성 방식을 통해) model.addAllAttribute(collection);&#125; @ModelAttribute 이름에 Model이 들어가 있긴 하지만 우리가 일반적으로 사용하는 모델과는 조금 의미가 다르다. 컨트롤러가 받는 요청정보 중에서, 하나 이상의 값을 가진 오브젝트 형태로 만들 수 있는 정보를 @ModelAttribute 모델이라고 부른다. @ModelAttribute라고 별다를 건 없다. 기존과 똑같이 파라미터를 받는데, 그걸 메서드에서 1:1로 받으면 @RequestParam인거고 도메인 오브젝트나 DTO에 바인딩해서 받으면 @ModelAttribute 인 것이다. 사용자가 리스트에서 검색할 떄 사용하는 파라미터를 한번 생각해 보자. 기본적으로 전달될 파라미터는 검색 키워드겠고, 그 외에도 검색 타입, 페이지 번호 등이 전달 될 수 있다. 이를 기존의 @RequestParam으로 표현하면 아래와 같이 된다. 1234567public String search( @RequestParam("q") String q, @RequestParam("type") String type, @RequestParam(value="page", required=false, defaultValue="1") Integer page)&#123; service.search(q, type, page);&#125; 일단 서비스 메서드 부터 문제가 있다… 저런식으로 파라미터를 나열할 경우 변경에 매우 취약하게 되며, 같은 타입의 파라미터가 여러개면 실수할 가능성이 매우 높아진다. 이럴 경우에는 아래와 같이 오브젝트를 하나 만들어 전달하는 편이 낫다. 1234567public class Search&#123; private String q; private String type; private Integer page; // getter, setter&#125; 서비스 메서드는 이 오브젝트를 사용하며 해결이 가능한데, 오브젝트를 매번 초기화 해줘야 한다는 귀찮음이 따른다. 이럴떄 사용할 수 있는것이 @ModelAttribute이다! 1234public String search(@ModelAttribute Search search)&#123; service.search(search);&#125; 이제 요청 파라미터들은 자동으로 Search 오브젝트에 바인딩 되어 들어오게 된다(타입 변환도 자동으로 된다). 코드가 매우 깔끔해지고 위에서 언급한 문제점 또한 단번에 해결할 수 있다. @ModelAttribute는 위와 같이 사용할 수도 있지만 보통은 폼의 데이터를 받을 때 훨씬 유용하게 사용할 수 있다. 게다가 @ModelAttribute의 기능중에 하나가 전달받음과 동시에 컨트롤러가 리턴하는 모델에 자동으로 추가해준다는 점이다. 이로인해 사용자가 입력을 잘못했을 경우에도 입력한 모델을 다시 출력해주며 잘못 입력한 정보에 대해 재입력을 요구하는 기능을 쉽게 구현할 수 있게 된다. Errors, BindingResult @ModelAttribute와 같이 사용하는 파라미터 들이다. 기본적으로 @ModelAttribute는 파라미터를 처리할 떄 @RequestParam과는 달리 검증 작업이 추가적으로 진행된다. 검증작업이란 기본적으로 진행되는 타입 변환 외에도 필수 정보 입력 여부, 길이 제한, 값 허용 범위 등 다양한 기준이 적용될 수 있다. BidingResult와 Errors는 이러한 검증작업의 결과가 담겨지는 곳이다. 컨트롤러에서는 이 두 오브젝트의 결과를 통해 사용자에게 적절한 조치를 취할 수 있게 되는 것이다(검증 에러가 난 부분에 대해 재입력 요구 등). 이러한 특성 때문에 @ModelAttribute는 바인딩에서 타입 변환이 실패하는 오류가 발생해도 400 에러를 발생시키지 않는다. 타입 변환 실패 또한 검증의 한 단계로 보고 BindingResult에 그에 해당하는 결과만을 담을 뿐이다. BindingResult의 검증결과에서 오류가 없다고 나오면 그제서야 로직을 통과시키고, 오류가 있을 경우 사용자에게 계속 수정을 요구해야 한다. 1234567public String add(@ModelAttribute User user, BindingResult result)&#123; if(result.hasError())&#123; // 재입력 요구 &#125; else&#123; userService.add(user); &#125;&#125; BindingResult는 반드시 @ModelAttribute 뒤에 나와야 한다. 현재 위의 메서드로는 기본적인 검증인 타입 변환 검증만을 수행하는 상태이다. 모델 바인딩과 검증 바로가기 SessionStatus @SessionAttributes를 통해 저장된 현재 세션을 다룰 수 있는 오브젝트이다. @SessionAttributes, SessionStatus 바로가기 @RequestBody 이 애노테이션이 붙은 파라미터에는 HTTP 요청의 본문 부분이 그대로 전달된다. XML이나 JSON 기반으로 요청하는 경우 매우 유용하게 사용된다. AnnotationMethodHandlerAdapter에는 HttpMessageConverter타입의 메세지 변환기가 여러개 등록되어 있다. @RequestBody가 붙은 파라미터가 있으면 요청의 미디어 타입을 먼저 확인한 후, 메세지 변환기들 중에서 이를 처리할수 있는것이 있다면 HTTP 요청 본문 부분을 통째로 변환하여 파라미터로 전달해준다. 12345public String test(@RequestBody String body)&#123; System.out.println(body); // http body가 그대로 출력 return "test";&#125; StringHttpMessageConverter가 http body를 그대로 받고 있다. @Value 시스템 프로퍼티나 다른 빈의 프로퍼티 값, SpEL등을 이용하는데 사용된다. 파라미터 변수 뿐 아니라 필드 변수에도 사용할 수 있다. 123456// 필드로 받기@Value("#&#123;'systemProperties['os.name']'&#125;") String osName;// 파라미터로 받기public String hello(@Value("#&#123;systemProperties['os.name']&#125;") String osName)&#123; // ...&#125; 상황에 따라 적절히 선택해서 사용하면 된다. @Valid @ModelAttribute 검증에 사용되는 애노테이션이다. 모델 바인딩과 검증 바로가기 리턴 파라미터 뿐만 아니라 리턴 타입도 다양하게 사용할 수 있다. 각 리턴 타입에 대해 다양한 결과를 얻어낼 수 있다. 참고로 어떤 방식으로 리턴하든 마지막에는 ModelAndView로 만들어져 DispatcherServlet에 전달된다. 모델에 자동으로 추가되는 오브젝트 리턴 타입을 알아보기 전에 굳이 명시하지 않아도 모델에 자동으로 추가되는 오브젝트들 부터 살펴보자. @ModelAttribute 파라미터 파리미터에서 @ModelAttribute로 받은 오브젝트는 자동으로 모델에 추가된다. 모델 오브젝트의 이름은 기본적으로 파라미터 타입 이름을 따른다. 이름을 직접 지정하고 싶으면 @ModelAttribute(&quot;모델이름&quot;) 의 형태로 지정해주면 된다. Map, Model, ModelMap 파라미터에 Map, Model, ModelMap 타입의 오브젝트를 사용하면 미리 생성된 모델 맵 오브젝트를 전달받을 수 있다. 이후 추가하고 싶은 모델 오브젝트가 있으면 여기에 추가하면 된다. @ModelAttribute 메서드 파라미터를 오브젝트로 받는 @ModelAttribute의 기능보단 공통적으로 사용되는 모델 오브젝트를 정의하기 위해 유용하게 사용되는 방식이다. 1234@ModelAttribute("countries")public List&lt;Country&gt; countries()&#123; return commonService.getCountries();&#125; 이런식으로 클래스 내에 별도로 정의해놓으면 클래스 내의 다른 메서드들의 모델에 자동으로 추가된다. 같은 클래스 내의 메서드들의 모델에는 항상 &quot;countries&quot; 이름의 List&lt;Country&gt; 오브젝트가 추가되어있는 것이다. &lt;select&gt; 태그를 써서 선택 가능한 목록을 보여주는 경우가 대표적이다. BidingResult @ModelAttribute와 같이 사용하는 BindingResult도 모델에 자동으로 추가된다. 모델 맵에 추가될때의 키는 'org.springframework.validation.BindingResult.모델이름' 이다. ModelAndView 컨트롤러가 리턴해야 할 정보를 담고 있는 가장 대표적인 클래스이다. 하지만 이것보다 편한 방법이 훨씬 많으므로 자주 사용되진 않는다. 1234567public ModelAndView test()&#123; ModelAndView mv = new ModelAndView(); mv.addObject("key", "value"); mv.setViewName("test"); return mv;&#125; 참고로 ModelAndView를 리턴하더라도 Map, Model, ModelMap 파라미터는 모델에 자동 추가된다. String 문자열을 리턴하면 이는 뷰 이름으로 사용된다. 모델은 Model, ModelMap 파라미터를 이용한다. 12345public String test(ModelMap modelMap)&#123; modelMap.addAttribute("key", "value"); return "test";&#125; void 아예 아무것도 리턴하지 않을경우 RequestToViewNameResolver에 의해 자동으로 뷰 이름이 생성된다. 뷰 이름을 일관되게 통일해야 하므로 규칙을 잘 정해야 한다. 모델 오브젝트 RequestToViewNameResolver를 사용해서 뷰 이름을 자동생성하고, 모델에 추가할 오브젝트가 하나뿐이라면 모델 오브젝트를 바로 반환해도 된다. 스프링은 리턴 타입이 단순 오브젝트이면 이를 모델 오브젝트로 인식해서 모델에 자동으로 추가해준다. 모델명은 모델 오브젝트의 타입 이름을 따른다. 1234public List&lt;Post&gt; getPostList()&#123; return postService.getPostList(); // return List&lt;Post&gt;&#125; Map/Model/ModelMap 메서드에서 직접 Map/Model/ModelMap 오브젝트를 생성해서 리턴하면 모두 모델로 사용된다. 하지만 모델은 파라미터로 받을 수 있기 때문에 이 방식은 잘 사용되지 않는다. 여기서 한가지 주의해야 할 점이 있다. 바로 Map 오브젝트이다. 서비스 메서드에서 결과로 Map을 내려주는 경우가 있는데, 이를 모델 오브젝트라고 생각하고 바로 리턴했다가는 원치않은 결과를 얻게 된다. Map은 모델 오브젝트가 아닌 모델 맵으로 인식되기 때문에, Map의 모든 속성이 모델에 추가되는 상황이 발생한다. 123456// 잘못된 코드!!public Map getUser()&#123; Map user = userService.getUser(); return user; // user의 모든 속성이 모델에 추가되어 리턴된다.&#125; View 뷰 이름대신 직접 View 오브젝트를 넘겨도 된다. 12345public View getPostListByExcel(ModelMap modelMap)&#123; // model add.. return excelView; // excel view&#125; @ResponseBody @ReuqestBody와 비슷하게 동작한다. 메서드 레벨에 이 애노테이션이 붙어있으면, 리턴하는 오브젝트가 뷰를 통해 결과를 만들어내는데 사용되지 않고 메세지 컨터버를 통해 바로 HTTP 응답으로 반환된다. 1234@ResponseBodypublic String test()&#123; return "succeed"; // 문자열 그대로 반환&#125; @ResponseBody에 의해 succeed가 viewName으로 사용되지 않고 HTTP 응답으로 반환된다. From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>@RequestMapping param</tag>
        <tag>@RequestMapping return</tag>
        <tag>@ModelAttribute</tag>
        <tag>토비의 스프링</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] @RequestMapping]]></title>
    <url>%2Fspring%2F%40RequestMapping%2F</url>
    <content type="text"><![CDATA[@RequestMapping은 DefaultAnnotationHandlerMapping에서 컨트롤러를 선택할 때 대표적으로 사용하는 애노테이션이다. url당 하나의 컨트롤러에 매핑되던 다른 핸들러 매핑과 달리 메서드 단위까지 세분화하여 적용할 수 있으며, url 뿐 아니라 파라미터, 헤더 등 더욱 넓은 범위를 적용할 수 있다. 속성 DefaultAnnotationHandlerMapping은 클래스와 메서드에 붙은 @RequestMapping 애노테이션 정보를 결합해 최종 매핑정보를 생성한다. 기본적인 결합 방법은 클래스 레벨의 @RequestMapping을 기준으로 삼고, 메서드 레벨의 @RequestMapping으로 세분화하는 방식으로 사용된다. @RequestMapping에 사용할 수 있는 속성들은 아래와 같다. String[] value URL 패턴을 지정하는 속성이다. String 배열로 여러개를 지정할 수 있으며, ANT 스타일의 와일드카드를 사용할 수 있다. 1234@RequestMapping(value="/post")@RequestMapping(value="/post.*")@RequestMapping(value="/post/**/comment")@RequestMapping(value=&#123;"/post", "/P"&#125;) {}를 사용하는 URI 템플릿을 사용할 수도 있다. 1@RequestMapping(value="/post/&#123;postId&#125;") {}를 패스 변수라고 부르며 컨트롤러에서 파라미터로 전달받을 수 있다. 참고로 URL 패턴은 디폴트 접미어 패턴이 적용되므로 아래 2개는 동일한 의미이다. 123// 2개는 동일하다!@RequestMapping(value="/post")@RequestMapping(value=&#123;"/post", "/post/", "/post.*"&#125;) RequestMethod[] method RequestMethod는 HTTP 메서드를 정의한 ENUM이다. GET, POST, PUT, DELETE, OPTIONS, TRACE로 총 7개의 HTTP 메서드가 정의되어 있다. @RequestMapping에 method를 명시하면 똑같은 URL이라도 다른 메서드로 매핑해줄 수 있다. 12345// url이 /post인 요청 중 GET 메서드인 경우 호출됨@RequestMapping(value="/post", method=RequestMethod.GET)// url이 /post인 요청 중 POST 메서드인 경우 호출됨@RequestMapping(value="/post", method=RequestMethod.POST) 이로인해 컨트롤러에서 번거롭게 HTTP 메서드를 확인할 필요가 없다. String[] params 요청 파라미터와 값으로도 구분할 수 있다. String 배열로 여러개를 지정할 수 있으며, 아래와 같이 사용 가능하다. 1234567891011// /post?useYn=Y 일 경우 호출됨@RequestMapping(value="/post", params="useYn=Y")// not equal도 가능@RequestMapping(value="/post", params="useYn!=Y")// 값에 상관없이 파라미터에 useYn이 있을 경우 호출됨@RequestMapping(value="/post", parmas="useYn")// 파라미터에 useYn이 없어야 호출됨@RequestMapping(value="/post", params="!useYn") GET 파라미터만이 아닌 POST 파라미터 또한 비교 대상이다. 폼 내에서 input 태그 등으로 넘겨도 위의 매핑을 적용할 수 있다. 근데 만약 아래와 같은 상황이라면 어떻게 될까? 123// 요청 : /post?useYn=Y@RequestMapping(value="/post")@RequestMapping(value="/post", params="useYn=Y") 요청이 2가지 매핑을 다 만족하지만 이럴 경우 구현이 상세한 쪽으로 선택된다. String[] headers 헤더 값으로 구분할 수 있다. 방식은 위의 params와 비슷하다. 1@RequestMapping(value="/post", headers="content-type=text/*") 매핑 핸들러 매핑이란 원래 오브젝트를 결정하는 전략이다. @RequestMapping으로 메서드 레벨까지 세분화하여 작성하긴 했지만, 일관성을 위해 DefaultAnnotationHandlerMapping은 오브젝트까지만 찾아주고, 최종 실행할 메서드는 AnnotationHandlerAdapter가 결정한다. 타입 레벨 + 메서드 레벨 매핑 타입 레벨(클래스 레벨)에서 공통 조건을 지정하고, 메서드 레벨에서 이를 세분화한다. 그리고 URL 요청 시 이 둘을 조합하여 최종 조건이 결정된다. 1234567891011@RequestMapping(value="/post")public class PostController&#123; // /post/add 에 매핑 @RequestMapping(value="/add") // /post/modify 에 매핑 @RequestMapping(value="/modify") // /post/remove 에 매핑 @RequestMapping(value="/remove")&#125; 타입 레벨에 ANT 패턴을 사용했을 경우에도 메서드 레벨과 조합될 수 있다. 12345@RequestMapping(value="/post/**")public class PostController&#123; // /post/**/add 에 매핑 @RequestMapping(value="/add")&#125; 타입 레벨에 url을 주고 메서드 레벨엔 다른 매핑조건을 줄수도 있다. 12345@RequestMapping(value="/post")public class PostController&#123; @RequestMapping(method=RequestMethod.GET) @RequestMapping(method=RequestMethod.POST)&#125; 타입 레벨에도 메서드 레벨과 같이 method나 params등을 줄 수 있다. 12345@RequestMapping(value="/post", params="useYn=Y")public class PostController&#123; // /post?useYn=Y&amp;isForeign=Y 에 매핑됨 @RequestMapping(params="isForeign=Y")&#125; 타입 레벨에서 공통 조건을 지정하고 메서드 레벨에서 세분화 해준다는 개념만 지키면 어떤식의 조합도 가능하다. 메서드 레벨 단독 매핑 매핑 조건에 딱히 공통점이 없는 경우 메서드 레벨에서만 매핑정보를 지정할 수 있다. 하지만 위에서 얘기했듯이 DefaultAnnotationHandlerMapping은 오브젝트까지만 찾아주므로, 타입 레벨에 조건 없는 @RequestMapping을 선언해 매핑 대상으로 만들어야 한다. 123456@RequestMappingpublic class HomeController&#123; @RequestMapping(value="/post") @RequestMapping(value="/magazine") @RequestMapping(value="/notice")&#125; 근데 만약 컨트롤러 클래스에 @Controller 애노테이션을 붙여 빈 자동 스캔으로 등록되게 했다면 타입 레벨 @RequestMapping은 생략할 수 있다. @Controller 애노테이션을 보고 @RequestMapping을 사용한 클래스라고 판단하기 때문이다. 타입 레벨 단독 매핑 핸들러 매핑과 핸들러 어댑터는 서로 독립적인 전략이므로 아래와 같이 조합될 수 있다. 1234567@RequestMapping(value="/post")public class PostController implements Controller&#123; @Override public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // .... &#125;&#125; 상속 @RequestMapping이 적용된 클래스를 다른 클래스에서 상속하여 사용할 수 있다. 상속 시 서브 클래스는 부모의 @RequestMapping을 상속받을 수 있게된다. 근데 만약 서브클래스 내에서 @RequestMapping을 재정의 할 경우 부모의 정보는 무시된다. 부모 @RequestMapping 상속 부모 클래스에 @RequestMapping이 정의되어있고, 자식에서 @RequestMapping을 재정의 하지 않을 경우 @RequestMapping은 그대로 상속된다. 1234567891011121314@RequestMapping(value="/post")public class PostController&#123; @RequestMapping(value="/list") public String list()&#123; // ... &#125;&#125;public class ChildPostController extends PostController&#123; @Override public String list()&#123; // ... &#125;&#125; ChildPostController의 list 메서드는 /post/list에 매핑된다. 부모 @RequestMapping + 자식 @RequestMapping 상속으로 조합도 가능하다. 1234567891011@RequestMapping(value="/post")public class PostController&#123; &#125;public class ChildPostController extends PostController&#123; @RequestMapping(value="/list") public String list()&#123; // ... &#125;&#125; 부모와 자식의 @RequestMapping이 조합되어 /post/list에 매핑된다. 반대로도 가능하다. 1234567891011121314public class PostController&#123; @RequestMapping(value="/list") public String list()&#123; // ... &#125;&#125;@RequestMapping(value="/post")public class ChildPostController extends PostController&#123; @Override public String list()&#123; // ... &#125;&#125; 자식 @RequestMapping 재정의 @RequestMapping을 재정의 할 경우 모든 조건이 다 재정의 된다는 점에 주의하여야 한다. 123456789101112131415@RequestMapping(value="/post")public class PostController&#123; @RequestMapping(value="/list", params="useYn=Y") public String list()&#123; // ... &#125;&#125;public class ChildPostController extends PostController&#123; @Override @RequestMapping(value="/contents") public String list()&#123; // ... &#125;&#125; value만 재정의 한것 처럼 보이나 실제로는 부모 list 메서드의 @RequestMapping 자체를 모두 재정의 한것이 된다. params 속성은 사라지게 된다. 상속과 제네릭스를 이용한 매핑 전략 이제 앞서 나열한 상속 성질들을 이용하면 공통 컨트롤러를 만들 수 있다. 대상으로는 도메인 오브젝트의 CRUD가 적합하다. CRUD의 경우 대부분이 서비스를 호출하는 위임코드가 많다는 점을 이용하면 아래와 같이 코드를 대폭 줄일 수 있다. 1234567891011121314151617181920212223242526272829303132333435public abstract class CrudController&lt;T, K, S&gt;&#123; S s; @RequestMapping(value="/list") public List&lt;T&gt; list(ModelMap map)&#123; // ... &#125; @RequestMapping(value="/detail") public T detail(K id)&#123; // ... &#125; @RequestMapping(value="/add") public void add(T entity)&#123; // ... &#125; @RequestMapping(value="/modify") public void modify(T entity)&#123; // ... &#125; @RequestMapping(value="/remove") public void remove(K id)&#123; // ... &#125;&#125;@RequestMapping(value="/post")public class PostController extends CrudController&lt;Post, Integer, PostService&gt;&#123; @RequestMapping(value="/best") // 공통 외에 필요한것은 따로 정의하면 된다 public List&lt;Post&gt; best()&#123; // ... &#125;&#125; 상속받고 @RequestMapping만 정의해줬을 뿐인데 중복되는 코드를 대폭 줄일 수 있게 되었다! 만약 기본 메서드에 추가할 작업이 있다면 직접 구현해서 새로 작성하면 된다. 그리고 CRUD외에 추가할 메서드가 있으면 best 메서드 처럼 넣어주면 되므로 코드가 매우 간결해지고 개발 생산성이 대폭 상승된다. From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>토비의 스프링</tag>
        <tag>DefaultAnnotationHandlerMapping</tag>
        <tag>AnnotationHandlerAdapter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] 데이터베이스 무결성, 정합성]]></title>
    <url>%2Fdb%2F%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EB%AC%B4%EA%B2%B0%EC%84%B1-%EC%A0%95%ED%95%A9%EC%84%B1%2F</url>
    <content type="text"><![CDATA[정합성에 어긋난다 == 데이터가 일치하지 않는다(같은 성격?의 데이터를 다루는 테이블들 간에) 무결성이 어긋난다 == 말이 안되는 데이터가 들어있다(말이 안되는 값, 부모가 없는 자식 등등) 무결성 &gt; 정합성 참고 : https://dbaguru.tistory.com/432]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>무결성</tag>
        <tag>정합성</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] spring의 예외처리]]></title>
    <url>%2Fspring%2Fspring%EC%9D%98-%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[이번에는, 프로그램을 만들때 중요하지만 대부분의 개발자가 귀찮아 하는 예외처리에 대해 알아보겠습니다. 잘못된 예외처리는 찾기 힘든 버그를 낳을 수 있고, 더욱 난처한 상황을 만들 수 있으므로 항상 신경써줘야 합니다. 잘못된 예외처리 예외 블랙홀1 12345try &#123; // ....&#125; catch(Exception e)&#123; // 아무 처리가 없음&#125; 예외발생을 무시하고 정상적인 상황인 것 처럼 다음으로 넘어가겠다는 분명한 의도가 있지 않은 이상 절대 하면 안됩니다. 위는 예외가 발생했는데 그것을 무시하고 진행하겠다는 의미인데, 이렇게 해봐야 최종적으로 프로그램에 오류가 발생할 것입니다. 하지만 그때는 이미 되돌리기엔 늦어버리고, 원인을 찾기도 힘듭니다. 예외 블랙홀2 1234567try &#123; // ....&#125; catch(Exception e)&#123; System.out.println(e.getMessage()); // 또는 e.printStackTrace();&#125; 예외 블랙홀1과 별다를게 없습니다. 이는 그냥 로그를 출력한 것이지 예외를 처리한 것이 아닙니다. 예외 블랙홀 처럼 굳이 예외를 잡아서 조치를 취할게 없으면 그냥 잡지를 말아야 합니다. 차리라 throws로 책임을 전가해버리는 것이 낫습니다. 무차별 throws 말했다고 바로 나오네요… 1234567891011void method1() throws Exception&#123; method2();&#125;void method2() throws Exception&#123; method3();&#125;void method3() throws Exception&#123; // ...&#125; method3을 개발한 개발자는, 예외를 잡아봤자 처리할 것도 없고 매번 발생가능한 예외를 찾아내서 throws로 선언하기 귀찮아서 그냥 최상위 예외인 Exception을 throws로 던져버리고 있습니다. method2를 개발한 개발자도 똑같은 행위를 하고 있네요. 일단은, 이 처리 자체가 위의 예외 블랙홀 보다는 낫지만 결국 무책임한 처리이긴 합니다. 그리고 만약 method1을 개발하는 개발자가, 발생하는 예외를 처리하려 했다고 합시다. 그래서 method2를 봤더니… 왠열 걍 throws Exception이네요. 뭐지… 하고 method3까지 들어갔더니 또 throws Exception입니다. 이건 뭐… 왜 예외가 발생하는지 하나하나 다 까봐야겠네요 ㅡㅡ 이처럼 무차별 throws는 정확한 예외상황을 제공해주지 않는 무책임한 예외처리 기법입니다. 예외처리 방법 일단 예외를 처리하는 일반적인 방법은 아래와 같습니다. 예외 복구 예외 상황을 파악하고 문제를 해결하여 정상 상태로 돌려놓는 방법입니다. 말그대로 예외를 catch로 잡아서 적절한 처리를 진행하는 의미죠. 예외 블랙홀은 절대 예외 복구가 아닙니다. 예외 회피 예외를 직접 처리하지 않고 throws를 통해 호출한 쪽으로 던지거나, catch로 잡아서 로그를 찍고 throw로 다시 던지는 방법입니다. 무차별 throws가 되지 않도록 주의해야 합니다. 예외 전환 예외 포장 예외 회피 처럼 호출한 쪽으로 예외를 던지는 것이긴한데, 그냥 던지는 것이 아니라 좀 더 의미있는 예외로 변환하여 던집니다. 12345@Testpublic void add()&#123; dao.add(user1); dao.add(user1);&#125; 예를 들어 위와 같은 테스트 코드는 분명 에러를 발생시킬 겁니다. 똑같은 데이터를 2번 넣고 있기 때문에 기본키 제약조건에 걸리거든요. 근데 중요한건, 여기서 발생하는 예외가 SQLException 이라는 겁니다. SQLException은 SQL 관련해서 오류만 낫다하면 항상 발생하는 매우 모호한 Exception 입니다. 굳이 기본키 제약조건에 걸려 데이터가 중복되는 상황만이 아닌 모든 SQL 관련 오류에 발생하는 Exception 이라는 거죠. (JDK 1.6부터 조금씩 변화하고 있지만 아직 대부분이 SQLException으로 처리되고 있습니다.) 이래서는 dao의 add를 호출한 쪽에서 예외를 처리하는것도 애매해집니다. 그냥 SQLException이라고 오면 이게 무슨 예외인지 어찌 알겠습니까… 무차별 throws 기억나시죠? 비슷합니다. 그래서 예외전환을 통해 DAO의 add를 아래와 같은 방식으로 전환해주는 겁니다. 1234567891011public void add(User user) throws DuplicateUserIdException&#123; try&#123; // add 작업 &#125; catch(SQLException e)&#123; if(e.getErrorCode()==중복코드)&#123; throw new DuplicateUserIdException(e); &#125; else&#123; throw e; &#125; &#125;&#125; 모호한 SQLException이 아니라 DuplicateUserIdException과 같은 좀 더 확실한 의미의 예외로 전환해서 던져주는 겁니다. 이로써 호출하는 쪽에서 예외처리 또한 수월해지게 됩니다. getErrorCode JDBC는 데이터 처리중에 발생하는 다양한 예외를 그냥 SQLException 하나에 담아버립니다. getErrorCode는 SQLException에 정의된 메서드로써, 예외가 발생된 에러 코드를 반환해줍니다. 해당 코드를 통해 발생한 SQLException에 대해 적절한 처리가 가능해집니다. 하지만 에러코드라는게 DB 벤더마다 각각 다르므로 getErrorCode 메서드를 통해 DB에 독립적인 예외처리는 힘들다는 단점이 있습니다… 위에서 비교한 중복코드 라는 것이 DB 마다 다 제각각 이라는거죠. SQLException에서 DB에 독립적인 예외처리를 위해 getSQLState라는 메서드를 제공하고는 있지만, 실상 각 DB 제조사들은 이 상태 코드를 제대로 작성해주지 않은 경우가 많습니다. ㅠ.ㅠ 그래서 DuplicateUserIdException을 전달할 때 SQLException을 넣어서 전달해주는 중첩 예외 방식을 사용하고 있습니다. 중첩예외로 발생시켜줄 경우, 호출하는 쪽에서 Throwable 클래스의 getCause() 메서드로 원인 예외를 확인할 수 있습니다. RuntimeException 전환 위와 같은 포장(wrap)형 예외 전환 말고, 형태(?)를 바꿔주는 예외 전환 방법이 있습니다. 또 다시 SQLException으로 예를 들어보겠습니다. SQLException은 체크 예외입니다. 체크 예외는, 해당 예외 발생 가능성이 있는 곳에서 예외의 처리를 문법적으로 강제합니다. 그런데 중요한점은, SQLException의 경우 대부분이 복구 불가능한 예외라는 점입니다. 즉 대부분이 예외를 잡아도 처리해줄수 있는게 없다는 뜻인데… 이럴경우 그냥 throws로 던져버려야 할까요? 호출하는 쪽에서라도 예외를 받아 처리하면 다행이겠지만, SQLException의 경우 호출하는 쪽에서도 딱히 처리할게 없습니다. 결국 무차별 throws와 같은 현상이 발생하게 됩니다… 이는 매우 무의미합니다. 아무리 예외 처리를 넘기고 넘겨봤자 처리가 불가능하기 때문입니다. 어쩌피 이렇게 처리가 안될 예외이면 가능한 빨리 이를 RuntimeException으로 포장하여, 호출하는 쪽에서 무차별 throws를 선언하지 않도록 해주는 것이 좋습니다. 12345678910try &#123; // DAO 코드&#125; catch (SQLException e) &#123; if(e.getErrorCode() 등을 이용해서 처리할 수 있는 경우)&#123; // 처리 코드 &#125; else&#123; // 로깅, 에러 내용 메일 전송 등의 로직 throw new RuntimeException(e); &#125;&#125; 위와 같이 처리 가능한 SQLExceptoin의 경우 처리해주고, 나머지 예외의 경우 RuntimeException으로 포장하여 호출 메서드들 쪽에서 신경쓰지 않도록 해주는 것이 좋습니다. 대신, 발생시에 로깅이나 메일 전송같은 로직을 발생시켜서 추후에 개발자가 그것을 보고 처리할 수 있도록 하는게 좋습니다. 무작정 throws를 선언하는 것은 올바른 예외처리 방법이 아닙니다. 애플리케이션 예외 이는 위의 RuntimeException 예외 전환과는 반대되는 방식입니다. 시스템 상의 예외가 아닌, 비즈니스 로직에서 발생하는 예외에 대해서는 사용자가 직접 예외를 정의해주기도 합니다. 예를 들자면 잔고가 0인 통장에서 출금을 하려는 경우 등이 있습니다. 적절한 리턴값으로 로직을 구성할 수도 있지만, 리턴값이라는게 표준도 없고 조건문이 많아져서 로직을 파악하기가 더 힘들어집니다. 이럴 경우, 비지니스적인 의미를 가진 예외를 정의하여 발생시켜 주는것이 상대적으로 코드가 더 깔끔해집니다. (예외를 사용하는 경우 catch 블록에 모아 둘수 있기 때문. 리턴값 사용시 로직 안에서 체크하므로 보기 복잡합니다.) 그리고 이런 예외들은 체크 예외로 만들어서 예외 처리를 강제하도록 합니다. 이런 예외를 보통 애플리케이션 예외 라고 합니다. 여러가지 예외처리 방법에 대해 알아보았지만, 정답은 없습니다. 상황에 맞춰 사용해야 하죠. 잘못된 예외처리는 항상 피하고, 예외처리 방법의 특징을 항상 생각하며 프로그램을 작성해야겠네요…(다짐ㅋㅋ) 스프링 예외처리 스프링을 이용하여 DAO 코드를 작성하면, ex) JdbcTemplate 예외 발생 시 스프링에서 제공하는 예외를 던져줍니다. 스프링은 데이터 관련 작업에서 발생하는 수많은 예외들의 대부분을 추상화하고, 재정의하여 제공하고 있습니다. 일단 먼저 알고 가셔야할 것은, 스프링에서 제공하는 데이터 작업 예외들은 모두 RuntimeException 예외들입니다. 서버상에서 발생하는 SQL 관련 예외들은 대부분이 복구 불가능한 예외이기 때문에, 문법적인 불편함을 제거하기 위해 모두 RuntimeException으로 정의한 것입니다. 이제 추상화를 좀 더 상세하게 살펴보겠습니다. 스프링에서는 DB에 독립적인 예외처리가 가능하도록 예외들이 추상화 되어있습니다. JDBC로 DAO 작성했을 경우를 생각해보시면, 언제나 일관된 예외인 SQLException이 발생했었습니다. 이럴 경우 예외만 가지고는 어떤 예외인지 판별이 안되므로, getErrorCode() 메서드를 호출하여 예외처리를 진행해야 했습니다. 하지만 여기서 문제점은 getErrorCode 메서드 같은 경우, DB 제조사별로 제각각의 에러코드를 반환한다는 점 때문에 DB에 독립적인 프로그래밍이 거의 불가능했습니다. 예외처리부분에서 각 DB 제조사별 에러 코드를 확인하고, 처리해야 했으니까요… 그러나 스프링의 경우 각 DB별로 예외클래스와 에러코드 매핑 파일을 두고, 일관된 예외 클래스로 반환하게 해줍니다. 스프링 라이브러리 파일 중에 jdbc 관련 jar 파일을 풀어서 보시면 안에 sql-error-codes.xml 이라는 파일이 보이실겁니다. 해당 파일을 열어보시면 위와 같이 정의가 되어있습니다. MySQL도 있고… Oracle도 있습니다. 이런식으로 약 10여개 정도 DB의 에러 코드에 대해 모두 매핑이 되어있습니다. 그리고 보다시피, 각 에러코드에 대해 일관된 프로퍼티명으로 정의해뒀음을 보실 수 있습니다. 즉, 스프링을 사용하여 DAO를 작성할 경우, DB가 바뀌더라도 일관된 예외로 받을 수 있게 된다는 의미입니다. 이로 인해 DB에 독립적인 프로그래밍이 가능하게 됩니다. 스프링에서 제공하는 JdbcTemplate를 사용해보시면 DB를 바꾸더라도 SQL 에러 발생시 일관된 예외를 던져주는 것을 보실 수 있을 겁니다. 스프링에서는 DAO 구현 기술에 관해서도 독립적인 예외처리가 가능하도록 추상화 되어있습니다. 보통 DAO 작성시, 인터페이스를 통한 전략패턴 형태로 작성합니다. DAO 작성 기술이 JDBC만 있는 것이 아니기 때문이죠. JDBC, Hibernate, JDO, JPA 등등 굉장히 많습니다. 그리고 여기서 또 문제가 되는것은, 각각 기술에 따라 던져지는 예외 또한 다르다는 것입니다. JDBC는 SQLException, Hibernate는 HibernamteException, JPA는 PersistenceException… 이런식이죠. 하지만 스프링은 이러한 것조차도 계층적으로 추상화 해두었다는 겁니다… ㄷㄷ 중복키 예외가 발생했을 떄를 예로 들어보면, JDBC의 경우 DuplicateKeyException, 하이버네이트의 경우 ConstraintViolcationException을 발생시킵니다. 그리고 이 예외들은 공통된 상위 예외로 DataIntegrityViolationException를 가집니다. 즉, 이런식으로 대부분의 모든 예외가 추상화 되어 있기 떄문에, 스프링을 사용하면 DAO의 구현기술, DB의 종류에 독립적인 프로그래밍이 가능해집니다. 하지만… DAO의 구현기술에 관한 추상화는 DB 종류에 관한 추상화보다는 이용가치가 조금 떨어지는 것이 현실입니다. DataIntegrityViolationException 예외의 경우 중복키 예외외에 다른 제약조건 위반상황에서도 발생하기 때문이죠 ㅠ.ㅠ 스프링의 추상화는 정말 대단하지만… 근본적인 한계 때문에 완벽하다고 기대할순 없습니다. 그래도 스프링은 참 대단하네요. 이정도만 해도 ㄷㄷ 간단히 DuplicateKeyException 계층을 보면 위와 같죠. 빨간색으로 표시한 DataAccessException이 스프링에서 제공하는 예외의 최상위 클래스입니다. RuntimeException의 하위클래스인것도 볼 수 있네요. 여기까지 예외처리 방법과 스프링의 예외 추상화에 대해 알아보았습니다… 예외라는게 중요하지만 신경 잘 못쓰게되는 부분인 것 같습니다. 책을 통해 공부하고, 포스팅을 함으로써 다시 한번 중요성을 꺠닫는것 같네요… 다른 분들에게도 도움이 되길 바라겠습니다. 감사합니다.]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring exception</tag>
        <tag>exception bad practice</tag>
        <tag>exception best practice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] mysql sample data]]></title>
    <url>%2Fdb%2Fmysql-sample-data%2F</url>
    <content type="text"><![CDATA[https://github.com/datacharmer/test_db mysql &lt; employees.sql 이거 외엔 딱히 할 필요 없어보이긴 함]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>mysql sample data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] WebApplicationInitializer]]></title>
    <url>%2Fspring%2FWebApplicationInitializer%2F</url>
    <content type="text"><![CDATA[서블릿은 3.0 이후부터 web.xml 없이도 서블릿 컨텍스트 초기화 작업이 가능해졌다. 프레임워크 레벨에서 직접 초기화할 수 있게 도와주는 ServletContainerInitializer API를 제공하기 때문이다. 서블릿 컨텍스트 초기화 web.xml에서 했던 서블릿 등록/매핑, 리스너 등록, 필터 등록 같은 작업들을 말한다. 스프링은 웹 모듈내에 이미 ServletContainerInitializer를 구현한 클래스가 포함되어 있고, 이는 WebApplicationInitializer 인터페이스를 구현한 클래스를 찾아 초기화 작업을 위임하는 역할을 수행한다. 123public interface WebApplicationInitializer&#123; void onStartup(ServletContext servletContext) throws ServletException;&#125; 이 인터페이스를 구현한 클래스를 만들어두면 웹 어플리케이션이 시작할 때 자동으로 onStartup() 메서드가 실행된다. 여기서 초기화 작업을 수행하면 된다. 루트 웹 애플리케이션 컨텍스트 등록(root-context.xml) 기존에는 아래와 같이 web.xml에 리스너 형태로 등록했다. 1234&lt;listener&gt; &lt;!-- /WEB-INF/applicationContext.xml 생성--&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; 리스너의 역할 리스너는 서블릿 컨텍스트가 생성하는 이벤트를 전달받는 역할을 한다. 서블릿 컨텍스트가 만드는 이벤트는 컨텍스트 초기화 이벤트와 종료 이벤트이다. 즉 웹 어플리케이션이 시작되고 종료되는 시점에 이벤트가 발생하고, 리스너를 등록해두면 이를 받을 수 있는 것이다. 스프링은 왜 루트 웹 애플리케이션 컨텍스트를 서블릿 리스너 레벨에서 등록하게 했을까? 이는 위의 리스너의 역할에 보이듯이 루트 애플리케이션 컨텍스트의 생명주기가 서블릿 컨테이너와 일치하기 때문이다. 아래와 같이 리스너를 등록해준다. 12ServletContextListener listener = new ContextLoaderListener();servletContext.addListener(listener); WebApplicationInitializer의 onStartup()은 서블릿 컨텍스트 초기화 시점에 실행된다고 했으니 리스너를 등록하지 않아도 된다고 생각할 수 있으나, 초기화 시점만 잡을 수 있지 종료 시점을 잡을수 없으므로 위와 같이 리스너는 등록해줘야 한다. 종료 시점을 잡아주지 않으면 애플리케이션이 종료되어도 리소스가 반환되지 못하므로 메모리 누수가 일어날 수 있다. 디폴트 루트 웹 애플리케이션 컨텍스트 클래스(XmlWebApplicationContext)나 디폴트 XML 설정파일 위치(/WEB-INF/applicationContext.xml)를 바꾸고 싶을때 web.xml에선 아래와 같이 했었다. 12345678910111213&lt;!-- 애플리케이션 컨텍스트 클래스 변경 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextClass&lt;/param-name&gt; &lt;param-value&gt; org.springframework.web.context.support.AnnotationWebApplicationContext &lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- 설정파일 위치 변경 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLoaction&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/root-context.xml&lt;/param-value&gt;&lt;/context-param&gt; 이는 서블릿 오브젝트의 setInitParameter() 메서드를 통해 지정할 수 있다. 1234// 애플리케이션 컨텍스트 클래스 변경 servletContext.setInitParameter("contextClass", "org.springframework.web.context.support.AnnotationWebApplicationContext");// 설정파일 위치 변경servletContext.setInitParameter("contextConfigLoaction", "/WEB-INF/root-context.xml"); 만약 Java Config를 사용한다면 더 깔끔하게 사용할 수 있다. 1234AnnotationConfigWebApplicationContext rootContext = new AnnotationConfigWebApplicationContext();rootContext.register(RootConfig.class); // RootConfig == root-context java configservletContext.addListener(new ContextLoaderListener(rootContext)); register() 대신 scan() 메서드를 사용하여 패키지를 통째로 스캔할수도 있다. 서블릿 웹 애플리케이션 컨텍스트 등록(servlet-context.xml) 서블릿 웹 애플리케이션 컨텍스트는 서블릿 안에서 초기화되고 서블릿이 종료될 떄 같이 종료된다. 이때 사용되는 서블릿이 DispatcherServlet이다. 기존의 DispatcherServlet 등록은 아래와 같이 작성했었다. 12345678910111213&lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/spring/appServlet/servlet-context.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;sevlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 이를 자바코드로 바꾸면 1234ServletRegistration.Dynamic dispatcher = servletContext.addServlet("appServlet", new DispatcherServlet());dispatcher.setInitParameter("contextConfigLocation", "/WEB-INF/spring/appServlet/servlet-context.xml");dispatcher.setLoadOnStartUp(1);dispatcher.addMapping("/"); 이 또한 루트 애플리케이션 컨텍스트 처럼 Java Config를 사용할 수 있다. 123456AnnotationConfigWebApplicationContext sac = new AnnotationConfigWebApplicationContext();sac.register(WebConfig.class);ServletRegistration.Dynamic dispatcher = servletContext.addServlet("appServlet", new DispatcherServlet(sac));dispatcher.setLoadOnStartUp(1);dispatcher.addMapping("/"); 여기까지가 루트 컨텍스트와 서블릿 컨텍스트를 자바 코드로 분리한 과정이다. 필터나 리스너들도 이런식으로 모두 등록할 수 있고, 그 이후엔 web.xml을 아예 제거할 수도 있다. 그 전까진 WebApplicationInitializer와 web.xml을 같이 사용할 수 있다. 대신 아래와 같이 web.xml에 version을 명시해줘야 한다. 12345&lt;web-app version="3.0"&gt; &lt;!-- .... --&gt;&lt;/web-app&gt; From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>토비의 스프링</tag>
        <tag>web.xml 없애기</tag>
        <tag>WebApplicationInitializer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] HandlerExceptionResolver, LocaleResolve, MulitpartResolver]]></title>
    <url>%2Fspring%2FHandlerExceptionResolver-LocaleResolver-MultipartResolver%2F</url>
    <content type="text"><![CDATA[HandlerMapping, HandlerAdapter, ViewResolver 등 외에도 DispatcherServlet에는 다양한 확장 가능한 전략들이 존재한다. 아래는 남은 전략들 중 중요한 것들만을 나열한 것이다. 핸들러 예외 리졸버 HandlerExceptionResolver는 컨트롤러 작업 중 발생한 예외를 어떻게 처리할 지 결정하는 전략이다. 기본적으로 컨트롤러나 그 뒷 계층에서 발생한 예외는 일단 DispatcherServlet이 전달받은 다음 다시 서블릿으로 던져서 서블릿 컨테이너가 처리하게 된다. 별다른 처리를 하지 않았다면 500 Internal Server Error 같은 메시지가 출력될 것이다. 그런데 핸들러 예외 리졸버가 등록되어있다면 DispatcherServlet은 먼저 이 리졸버가 해당 예외를 처리할 수 있는지 확인한다. 만약 해당 예외를 처리할 수 있으면 예외는 DispatcherServlet 밖으로 던져지지 않고 해당 핸들러 예외 리졸버가 처리한다. 핸들러 예외 리졸버는 HandlerExceptionResolver 인터페이스를 구현해서 생성한다. 123public interface HandlerExceptionResolver&#123; ModelAndView resolveException(HttpServletRequest reqeust, HttpServletResponse response, Object handler, Exception ex);&#125; 구현 메서드인 resolveException의 리턴 타입은 ModelAndView이다. 예외에 따라 사용할 뷰와 모델을 돌려주도록 되어있다. 만약 처리 불가능한 예외라면 null을 리턴한다. 스프링은 이미 4개의 HandlerExceptionResolver 구현 전략을 제공한다. AnnotationMethodHandlerExceptionResolver 예외가 발생한 컨트롤러 내의 메서드 중에서 @ExceptionHandler 애노테이션이 붙은 메서드를 찾아 예외처리를 맡긴다. 123456789101112@Controllerpublic class HelloController&#123; @RequestMapping(value="/hello") public void hello()&#123; // DataAccessException occured! &#125; @ExceptionHandler(DataAccessException.class) public ModelAndView dataAccessExceptionHandler(DataAccessException ex)&#123; return new ModelAndView("error").addObject("msg", ex.getMessage()); &#125;&#125; 특정 컨트롤러의 예외만을 처리하고 싶을때 유용하다. ResponseStatusExceptionResolver 예외를 특정 HTTP 응답 상태코드로 전환해준다. 예외클래스에 @ResponseStatus 애노테이션을 붙이고 value(응답 상태 값)를 지정한 뒤 해당 예외를 발생시키면 ResponseStatusExceptionResolver가 HTTP 응답을 변환해준다. 단순한 HTTP 500 에러 대신 의미있는 응답 상태를 클라이언트에 전달하고자 할 떄 유용하게 사용된다. 1234@ResponseStatus(value=HttpStatus.SERVICE_UNVAILABLE, reason="이유도 지정 가능")public class ServiceException extends RuntimeException &#123;&#125; 위와 같이 @ResponseStatus가 붙은 Exception을 정의하고, 1234public void helloService()&#123; // .... throw new ServiceException(); // exception 발생&#125; 이렇게 해당 Exception을 발생시키면 @ResponseStatus에 지정된 형태로 Response를 받을 수 있다. 만약 위처럼 정의할 수 없는 기존 예외가 발생했을 때는 위의 @ExceptionHandler를 사용하면 된다. 12345@ResposneStatus(value=HttpStatus.SERVICE_UNVAILABLE)@ExceptionHandler(DataAccessException.class)public ModelAndView dataAccessExceptionHandler(DataAccessException ex)&#123; return new ModelAndView("error").addObject("msg", ex.getMessage());&#125; DefaultHandlerExceptionResolver 위 두 가지 예외리졸버에서 처리하지 못한 예외를 다루는 예외 리졸버이다. 스프링에서 발생하는 주요 예외를 처리하는 표준 예외처리 로직을 담고 있다. 즉, 이 예외 리졸버는 신경쓰지 않아도 된다. SimpleMappingExceptionResolver web.xml의 &lt;error-page&gt;와 비슷하게 예외를 처리할 뷰를 지정할 수 있게 해준다. (뷰를 찾을때는 뷰리졸버가 사용된다.) 12345678910&lt;bean class="org.springframework.web.servlet.handler.SimpleMappingExceptionResolver"&gt; &lt;property name="mappedHandlers"&gt; &lt;props&gt; &lt;!-- 클래스 이름에 패키지를 쓰지 않아도 됨 --&gt; &lt;prop key="DataAccessException"&gt;error/dao&lt;/prop&gt; &lt;prop key="BusinessLoginException"&gt;error/business&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name="defaultErrorView"&gt;error/default&lt;/property&gt;&lt;/bean&gt; 사실상 실제로 활용하기에 가장 편리한 예외 리졸버이다. 사용자에게 어려운 HTTP 상태코드를 보여주는 것보다는 문구가 있는 예외 페이지를 보여주는 편이 낫기 때문이다. 또한 모든 컨트롤러에서 발생하는 예외에 일괄 적용할 수 있다. 예외페이지를 보여주는 외에 로그를 남기거나 관리자에게 통보를 해야할 경우 SimpleMappingExceptionResolver 보다는 핸들러 인터셉터의 afterCompletion() 메서드를 사용하는 것이 좋다. 에러페이지에 에러로그를 남기는 것은 바람직하지 않기 때문이다. 지역정보 리졸버 LocaleResolver는 애플리케이션에서 사용하는 지역정보를 결정하는 전략이다. 디폴트인 AcceptHeaderLocaleResolver는 HTTP 헤더의 지역정보를 그대로 사용한다. 보통 HTTP 헤더의 지역정보는 브라우저의 기본 설정에 따라 보내진다. 브라우저 설정을 따르지 않고 사용자가 직접 변경하게 하려면 SessionLocaleResolver나 CookieLocaleResolver를 사용하는 것이 편리하다. 해당 리졸버를 사용하면 사용자가 국가 선택 시 쿠키나 세션의 locale 값을 변경하여 해당 지역의 리소스 파일이 사용되게 할 수 있다. 다국어 서비스에 유용하게 활용할 수 있다. LocaleResolver 사용 예(http://yookeun.github.io/java/2015/08/12/spring-i18n/) 멀티파트 리졸버 멀티파트 포맷의 요청정보를 처리하는 전략이다. 멀티파트 리졸버 전략은 디폴트 전략이 없으므로 아래와 같이 빈을 등록해줘야 한다. 123&lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;property name="maxUploadSize" value="100000" /&gt;&lt;/bean&gt; DispatcherServlet은 클라이언트로부터 멀티파트 요청을 받으면 멀티파트 리졸버에게 요청해서 HttpServletRequest의 확장 타입인 MultipartHttpServletRequest로 변환한다. MultipartHttpServletRequest에는 멀티파트를 디코딩한 내용과 이를 참조하거나 조작할 수 있는 기능이 추가되어 있다. 이후 컨트롤러에서는 아래와 같이 MultipartHttpServletRequest로 캐스팅하여 사용할 수 있다. 12345public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response)&#123; MultipartHttpServletRequest multipartRequest = (MultipartHttpServletRequest) request; MultipartFile multipartFile = multipartRequest.getFile("image"); // ....&#125; RequestViewNameTranslator 컨트롤러에서 뷰 이름이나 뷰 오브젝트를 돌려주지 않을 경우 요청 URL 정보를 기준으로 해서 뷰 이름을 생성해준다. 디폴트로 DefaultRequestViewNameTranslator가 등록되어 있다. 잘 활용하면 매번 뷰 이름을 지정하는 수고를 덜어줄 수 있다. From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>토비의 스프링</tag>
        <tag>HandlerExceptionResolver</tag>
        <tag>LocaleResolver</tag>
        <tag>MultipartResolver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] View, ViewResolver]]></title>
    <url>%2Fspring%2FView-ViewResolver%2F</url>
    <content type="text"><![CDATA[뷰는 모델이 가진 정보를 어떻게 표현해야 하는지에 대한 로직을 갖고 있는 컴포넌트이다. 일반적인 뷰의 결과물은 브라우저에서 볼 수 있는 HTML이다. 이 외에도 엑셀, PDF, RSS 등 다양한 결과를 생성할 수 있는 뷰 오브젝트들이 있다. 뷰 뷰는 View 인터페이스를 구현해서 생성한다. 123public interface View&#123; void render(Map&lt;String, ?&gt; model, HttpServletRequest request, HttpServletResponse resposne) throws Exception;&#125; 스프링에서 제공하는 뷰 목록은 아래와 같다. InternalResourceView RequestDispatcher의 forward(), include()를 이용하는 뷰다. 123RequestDispatcher dispatcher = request.getRequestDispatcher("/WEB-INF/view/hello.jsp");request.setAttribute("message", "This is message");dispatcher.forward(request, response); 위는 RequestDispatcher를 사용했던 때의 방식이다. InternalResourceView의 동작 방식도 이와 동일하다고 보면 된다. 아래는 사용 예제의 일부분이다. 123View view = new InternalResourceView("/WEB-INF/view/hello.jsp");return new ModelAndView(view, model); JstlView InternalResourceView의 서브 클래스이다. JSP를 뷰 템플릿으로 사용할 때 JstlView를 사용하면 여러가지 추가 기능을 더 활용할 수 있다.(지역화 메세지 등) RedirectView HttpServletResponse의 sendRedirect()를 호출해주는 기능을 가진 뷰다. 실제 뷰를 생성하진 않고 URL만 만들어 다른 페이지로 리다이렉트 해준다. 모델정보가 있다면 URL 뒤에 파라미터로 추가된다. 1234// 뷰 사용시return new ModelAndView(new RedirectView("/main"));// 뷰 리졸버 사용시return new ModelAndView("redirect:/main"); 보다시피 redirect: 접두어를 사용하여 뷰 리졸버가 인식하게 할 수 있다. redirect 경로는 절대경로(컨텍스트 패스, 서블릿 패스 포함)이어야 하는데, contextRelative 옵션을 true로 주면 컨텍스트 패스는 생략하고 작성할 수 있다. VelocityView, FreeMarkerView JSP 대신 Velocity, FreeMarker 템플릿 엔진을 뷰로 사용할 수 있게 해준다. 둘은 JSP보다 훨씬 문법이 강력하고 속도도 빠른 장점이 있다. 개인적으로 둘은 써보지 않았고 Thymeleaf라는 템플릿 엔진을 써봤었는데, 이것도 추천합니다… MarshallingView, MappingJacksonJsonView 모델의 정보를 xml, json으로 변환시킬 수 있는 뷰다. MashallingView를 사용할 경우 xml, MappingJacksonJsonView를 사용할 경우 json으로 변환된다. 메세지 컨버터를 이용해서도 동일한 작업을 할 수 있다. AbstractExcelView, AbstractJExcelView, AbstractPdfView 엑셀과 PDF문서를 만들어주는 뷰다. Abstract가 붙어있으니 상속을 해서 구현해줘야 한다. 구현하는 부분은 문서를 생성하는 부분이다. 구현한 뷰 클래스는 빈으로 등록하여 컨트롤러에 DI해줘도 되고, 직접 생성해도 된다. 뷰 오브젝트는 멀티스레드 환경에서 공유해도 안전하다. 즉 싱글톤 빈으로 생성해도 문제되지 않는다. AbstractAtomFeedView, AbstractRssFeedView application/atom+xml과 application/rss+xml 타입의 피드 문서를 생성해주는 뷰다. 컨트롤러에서 사용하는 방법은 위의 AbstractExcelView 등과 동일하다. 뷰 리졸버 뷰를 선택하는 것도 컨트롤러의 역할이다. 하지만 위처럼 컨트롤러에서 매번 뷰를 생성하는 것은 비효율적이므로, 스프링에서는 이 작업을 적절히 분리하였다. 컨트롤러는 뷰의 논리적인 이름만을 리턴한 뒤 역할을 종료하고, 이를 DispatcherServlet의 뷰 리졸버가 받아 사용할 뷰 오브젝트를 찾고 생성하는 작업을 진행해준다. 게다가 뷰 리졸버는 보통 뷰 오브젝트를 캐싱하므로 같은 URL의 뷰가 반복적으로 만들어지지 않는 장점도 있다. 뷰 리졸버도 하나 이상 등록해서 사용할 수 있는데, 이때는 핸들러 매핑처럼 order 프로퍼티를 이용해 적용 순서를 적용해주는 것이 좋다. 뷰 리졸버는 ViewResolver 인터페이스를 구현해서 생성한다. 123public interface ViewResolver&#123; View resolveViewName(String viewName, Locale locale) throws Exception;&#125; InternalResourceViewResolver 주로 JSP를 사용할 때 쓰이는 뷰 리졸버이다. 뷰를 생성할 필요없이 논리적인 이름만을 리턴해주면 되는데, 그대로 사용할 경우 풀 패스를 써줘야 하므로 그대로 사용하는 것은 피해야 한다. prefix, suffix 프로퍼티를 이용하면 앞뒤에 붙는 내용을 생략할 수 있다. 1234&lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/view" /&gt; &lt;property name="suffix" value=".jsp" /&gt;&lt;/bean&gt; 컨트롤러에서는 hello 만을 리턴해주면 된다. 이는 나중에 변경에도 용이하다. JSTL 라이브러리가 클래스패스에 존재할 경우 JstlView를 사용하고, 존재하지 않으면 InternalResourceView를 사용한다. VelocityViewResolver, FreeMarkerViewResolver Velocity와 FreeMarker를 사용하게 해주는 뷰 리졸버이다. InternalResourceViewResolver와 같이 prefix, suffix를 사용가능하다. ResourceBundleViewResolver 컨트롤러가 아닌 외부에서 뷰를 결정할 때, 한가지 뷰 만이 아니라 컨트롤러마다 뷰가 달라질 수 있을 때 사용하면 괜찮은 방식이다. ResourceBundleViewResolver를 사용하면 클래스패스의 views.properties 파일에 논리적 이름과 뷰 정보를 정의하여 작성하고, 이를 사용하여 뷰를 선택하게 할 수 있다. 아래는 views.properties 파일 예시이다. 12345hello.(class)=org.springframework.web.servlet.view.JstlViewhello.url=/WEB-INF/view/hello.jspbye.(class)=org.springframework.web.servlet.view.velocity.VelocityViewbye.url=bye.vm 독립된 파일을 통해 뷰를 자유롭게 매핑할 수 있지만, 모든 뷰를 일일히 매핑해줘야 하는 불편도 뒤따른다. 그래서 단독으로 사용하는 것은 추천되지 않고, 다른 뷰 리졸버와 함께하면 유용하게 사용될 수 있다. 123456&lt;bean class="org.springframework.web.servlet.view.ResourceBundleViewResolver"&gt; &lt;property name="order" value="0" /&gt;&lt;/bean&gt;&lt;!-- order 기본값이 Integer.MAX 이므로 굳이 order 프로퍼티를 안줘도 됨 --&gt;&lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver" /&gt; 이렇게 사용하면 view.properties에 뷰가 없을 경우 아래 InternalResourceViewResolver를 사용하게 된다. 이로써 특별한 타입의 뷰가 필요할 때만 view.properties에 작성해주며 사용할 수 있다. XmlViewResolver ResourceBundleViewResolver와 용도는 동일하고, views.properties 대신 /WEB-INF/views.xml을 사용한다. 추가로 이 파일은 서블릿 컨텍스트를 부모로 가지므로 DI가 가능하다는 장점이 있다. BeanNameViewResolver 뷰 이름과 동일한 이름을 가진 빈을 찾아서 뷰로 이용하게 해준다. From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>토비의 스프링</tag>
        <tag>View</tag>
        <tag>ViewResolver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Session 동작방식]]></title>
    <url>%2Fhttp%2FSession%20%EB%8F%99%EC%9E%91%EB%B0%A9%EC%8B%9D%2F</url>
    <content type="text"><![CDATA[개념 http 프로토콜은 매 접속마다 새로운 요청이 이뤄지는, 상태를 저장하지 않는 stateless 형식의 프로토콜이다. 그래서 서버쪽에서 클라이언트의 상태를 기억하기 사용하는 것이 session이다. 서버는 클라이언트의 정보를 내부에 저장하고, 이 저장된 정보에 접근하기 위해 세션 ID(JSESSIONID)를 이용한다. 여기에는 여러가지 정보(사용자 정보 등)를 담을 수 있고, 이 정보는 사용자가 일치하는 세션 ID를 가지고 들어오지 않는 한, 설정한 session timeout 만큼 유지된다. 쿠키와 달리 서버쪽에 저장되는 정보이므로 보안에 좀 더 안전하다. FLOW 아래는 session의 요청 방식을 잘 보여주는 그림이다. 클라이언트가 처음 서버로 http요청을 시도한다. 서버쪽에서는 전달받은 세션 ID(JSESSIONID)가 없으므로 생성한다. 생성한 세션 ID를 쿠키에 넣어 클라이언트에게 전달된다. 여기서 만약 클라이언트가 쿠키를 사용하지 않을 경우에는 URL에 붙여서 전달한다. ex) http://joont92.github.io;JSESSIONID=~~ 서버입장에서는 JSESSIONID를 전달받는 방식(request header에 넣어서 보내는지, URL에 붙여서 보내는지)에 따라 사용자가 쿠키를 사용하느냐 안하느냐의 여부를 알 수 있다. 첫 요청에는 JSESSIONID를 전달받지 않으므로 두 가지 방식으로 모두 보내본다. 클라이언트쪽에서는 받은 세션 ID를 쿠키에 저장한다. 쿠키는 session 종료 시 같이 소멸되는 Memory Cookie로 저장한다. 같은 페이지에 2번째 요청할 때, 클라이언트는 세션 ID를 가지고 있으므로 이를 request header에 넣어 전달한다. 서버는 전달받은 세션 ID가 있으므로 새로 생성하지 않고 해당 세션 ID에 맞는 정보를 전달해준다. 이러한 FLOW를 통해 클라이언트의 상태를 유지할 수 있게 되는것이다.]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>Session</tag>
        <tag>JSESSIONID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] HandlerMapping, HandlerAdapter, HandlerInterceptor]]></title>
    <url>%2Fspring%2FHandlerMapping-HandlerAdapter-HandlerInterceptor%2F</url>
    <content type="text"><![CDATA[HandlerMapping, HandlerAdapter, HandlerInterceptor는 앞서 DispatcherServlet Flow에서 설명했던 확장 포인트 중 컨트롤러와 관련된 부분이다. 핸들러 매핑 HTTP 요청정보를 이용해서 컨트롤러를 찾아주는 기능을 수행한다. HandlerMapping 인터페이스를 구현해서 생성한다. 123public interface HandlerMapping&#123; HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception;&#125; DispatcherServlet은 등록된 HandlerMapping 전략들에게 HttpServletRequest를 전달하면서 매칭되는 오브젝트를 찾는다.(이게 곧 세부 컨트롤러!) 스프링이 제공하는 핸들러 매핑 전략은 총 5가지이다. BeanNameUrlHandlerMapping HTTP 요청 URL과 빈의 이름을 비교하여 일치하는 빈을 찾아준다. 빈 이름에는 ANT패턴이라고 불리는 *, **, ? 를 이용한 패턴을 넣을 수 있다. 12345&lt;!-- hello로 시작하면 모두 여기 매핑된다 --&gt;&lt;bean name="/hello*" class="HelloController" /&gt;&lt;!-- **는 하나 이상의 경로를 매핑 할 수 있다 --&gt;&lt;bean name="/root/**/sub" class="SubController" /&gt; 하지만 컨트롤러의 개수가 많아지면 URL정보가 XML이나 애노테이션에 분산되어 파악하기 어려우므로, 복잡한 애플리케이션에서는 잘 사용하지 않는다. ControllerBeanNameHandlerMapping BeanNameUrlHandlerMapping과 유사하지만 위처럼 빈 이름을 URL 형태로 짓지 않아도 된다는 것이 차이점이다. 빈 이름 앞에 자동으로 /이 붙여져 URL에 매핑된다. 1&lt;bean name="hello" class="HelloController" /&gt; &lt;!-- /hello에 매핑 --&gt; ControllerClassNameHandlerMapping 빈의 클래스 이름을 URL에 매핑해주는 매핑 클래스이다. 기본적으로는 클래스 이름을 모두 사용하지만 클래스 이름이 Controller 로 끝날 경우 Controller를 뺀 나머지 이름을 URL에 매핑해준다. 123public class HelloController implements Controller&#123; // /hello에 매핑 // ...&#125; SimpleUrlHandlerMapping URL과 컨트롤러 매핑정보를 한곳에 모아놓을 수 있는 전략이다. 매핑정보는 SimpleUrlHandlerMapping 빈의 프로퍼티에 넣어준다. 123456789101112&lt;bean class="org.springframework.web.servlet.handler.SimpleUrlHandlerMapping"&gt; &lt;property name="mappings"&gt; &lt;!-- Properties 타입으로 URL과 빈 이름을 넣어준다 --&gt; &lt;props&gt; &lt;prop key="/hello"&gt;helloController&lt;/prop&gt; &lt;prop key="/root/**/sub"&gt;subController&lt;/prop&gt; &lt;!-- ANT 패턴 사용 가능 --&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 빈 이름이 위의 value 와 매핑 --&gt;&lt;bean id="helloController" /&gt;&lt;bean id="subController" /&gt; mappings는 Properties 타입이므로 프로퍼티 타입 포맷을 이용하여 더 간단히 작성할 수 있다. 12345678&lt;bean class="org.springframework.web.servlet.handler.SimpleUrlHandlerMapping"&gt; &lt;property name="mappings"&gt; &lt;value&gt; /hello=helloController /root/**/sub=subController &lt;/value&gt; &lt;/property&gt;&lt;/bean&gt; 매핑정보가 한군데 모여있어 URL을 관리하기 편리하여 대규모 프로젝트에서 선호하기도 한다. 하지만 매핑정보를 직접 작성하므로 오타가 발생할 수도 있다는 단점이 있다. DefaultAnnotationHandlerMapping @RequestMapping이라는 애노테이션을 이용해 매핑하는 전략이다. @RequestMapping은 클래스는 물론 메서드 단위로도 URL을 매핑할 수 있다. 또한 URL외에도 method, parameter, header 등의 정보도 애노테이션을 이용해 매핑에 활용할 수 있다. 굉장히 강력하고 편리한 방법이지만 매핑 애노테이션의 사용 정책, 작성 기준을 잘 마련해놓지 않으면 매핑정보가 금방 지저분해지므로 주의해야 한다. HandlerMapping 공통 설정정보 아래는 핸들러 매핑에서 공통적으로 사용되는 주요 프로퍼티이다. order 핸들러 매핑은 1개 이상을 동시에 사용할 수 있다. 1개 매핑으로 통일하는것이 가장 이상적이긴하나, 그렇지 않을 상황이 종종 있다. 2개 이상의 핸들러 매핑이 등록되었는데 URL이 중복 매치될 경우, order 프로퍼티를 통해 매핑 우선순위를 지정할 수 있다. defaultHandler URL을 매핑할 빈을 찾지 못할 경우 자동으로 디폴트 핸들러(컨트롤러)를 선택하게 한다. 원래 URL을 찾지 못하면 HTTP 404 error가 발생하는데, 이를 디폴트 핸들러로 넘겨 적절한 에러처리를 할 수 있다. 123&lt;bean class="org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping"&gt; &lt;property name="defaultHandler" ref="defaultController" /&gt;&lt;/bean&gt; alwaysUseFullPath URL 매핑은 기본적으로 애플리케이션 컨텍스트 패스, 서블릿 패스를 제외한 나머지만 가지고 비교한다. 즉 애플리케이션이 /test에 배포되고, DispatcherServlet URL mapping이 /app/*일 경우 전체 URL은 /test/app/hello 와 같은 형태지만, 핸들러 매핑은 /hello만을 대상으로 삼는다는 의미이다. 이는 애플리케이션이나 서블릿이 변경되어도 애플리케이션이 영향을 받지 않게 하기 위해서이다. 하지만 alwaysUseFullPath 옵션을 true로 주면 이를 해제하고 모든 URL을 대상으로 변경할 수 있다. detectHandlersInAncestorContexts 기본적으로 애플리케이션 컨텍스트는 계층형 구조를 가지므로, 자식 컨텍스트는 부모 컨텍스트를 참조할 수 있고, 그 반대는 안된다. 즉 루트 컨텍스트 -&gt; 서블릿 컨텍스트 의 부모-자식 형태의 경우 서블릿 컨텍스트에 선언된 빈은 루트 컨텍스트에 선언된 빈을 DI할 수 있지만, 루트 컨텍스트에 선언된 빈은 서블릿 컨텍스트에 선언된 빈을 DI할 수 없다. 그런데 핸들러 매핑의 경우 이와 좀 다르다. 핸들러 매핑 클래스는 매핑할 클래스를 현재 컨텍스트, 즉 서블릿 컨텍스트 내에서만 찾는다. 컨트롤러는 서블릿 컨텍스트에만 두는 것이 바람직하기 때문이다. detectHandlersInAcestorContexts 옵션을 true로 주면서 이 방식을 바꿔줄 수 있긴한데, 이 옵션은 절 대 사용하지 말자. 그냥 스프링의 극단적인 유연성을 보여주기 위한 옵션일 뿐이다. 핸들러 어댑터 HandlerMapping을 통해 찾은 컨트롤러를 직접 실행하는 기능을 수행한다. 핸들러 어댑터는 HandlerAdapter 인터페이스를 구현해서 생성한다. 1234567public interface HandlerAdapter&#123; boolean supports(Object handler); ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; long getLastModified(HttpServletRequest request, Object handler);&#125; HandlerMapping으로 찾은 오브젝트(컨트롤러)를 등록된 HandlerAdaptor들의 supports 메서드에 대입하며 지원 여부를 살핀다. 부합할 경우 handler 메서드를 실행하여 ModelAndView를 리턴한다! 스프링 MVC가 지원하는 컨트롤러는 총 4개이므로, 핸들러 어댑터도 4개이다. SimpleServletHandlerAdapter(Servlet interface) 표준 서블릿 인터페이스인 javax.servlet.Servlet을 구현한 클래스를 컨트롤러로 사용할 때 사용되는 어댑터이다. 이 방식의 장점은 서블릿 클래스 코드를 그대로 유지하면서 스프링 빈으로 등록할 수 있다는 점인데, 이는 서블릿 코드를 점진적으로 스프링 어플리케이션으로 포팅할 떄 유용하게 사용된다. 이 컨트롤러의 어댑터로는 SimpleServletHandlerAdapter가 사용된다. 참고사항 서블릿 라이프사이클 메서드인 init, destroy는 실행되지 않는다. 스프링 빈의 init-method나 @PostConstruct를 이용해야 한다. 이 컨트롤러는 모델과 뷰를 리턴하지 않는다. 서블릿은 원래 Response에 결과를 넣어주는 방식이기도 하고, ModelAndView의 개념을 모른다. ※ DispatcherServlet은 ModelAndView 리턴 대신 null을 리턴할 경우 뷰를 호출하는 작업을 생략한다. HttpRequestHandlerAdapter(HttpRequestHandler interface) 아래의 HttpRequestHandler 인터페이스를 구현한 클래스를 컨트롤러로 사용할 때 사용되는 어댑터이다. 123public interface HttpRequestHandler&#123; void handleRequest(HttpServletRequest request, HttpServletResponse resposne) throws ServletException, IOException;&#125; 서블릿 인터페이스와 생김새가 유사하다. 서블릿 스펙을 준수할 필요없이 HTTP 프로토콜을 기반으로 한 전용 서비스를 만들려고 할 때 사용한단다… 이정도만 알고 넘어가도 될듯. SimpleControllerHandlerAdapter(Controller interface) 아래의 Controller 인터페이스를 구현한 클래스를 컨트롤러로 사용할 때 사용되는 어댑터이다. 123public interface Controller&#123; ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception;&#125; 스프링의 대표적인 컨트롤러 타입이다.(3.0 전까지) DispatcherServlet과 주고받는 정보를 그대로 파라미터와 리턴값으로 갖고 있다. 하지만 이 인터페이스를 직접 구현해 컨트롤러를 만드는 것은 권장되지 않으며, 필수 기능이 구현된 AbstractController를 사용하거나 직접 확장한 Controller 클래스를 사용하는 것을 권장한다. 아래는 직접 확장의 간단한 예제이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 기반 컨트롤러public abstract class SimpleController implements Controller&#123; private String[] requiredParams; // 필수 파라미터 private String viewName; public void setRequiredParams(String[] requiredParams) &#123; this.requiredParams = requiredParams; &#125; public void setViewName(String viewName)&#123; this.viewName = viewName; &#125; @Override final public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; if(viewName == null)&#123; throw new IllegalStateException(); &#125; Map&lt;String, String&gt; params = new HashMap&lt;String, String&gt;(); for(String param : requiredParams)&#123; String value = request.getParameter(param); if(value == null)&#123; throw new IllegalStateException(); &#125; params.put(param, value); &#125; Map&lt;String, Object&gt; model = new HashMap&lt;String, Object&gt;(); this.control(params, model); // 개별 컨트롤러가 구현할 메서드 return new ModelAndView(this.viewName, model); &#125; public abstract void control(Map&lt;String, String&gt; params, Map&lt;String, Object&gt; model) throws Exception;&#125;// 개별 컨트롤러public class TestController extends SimpleController&#123; public TestController()&#123; this.setRequiredParams(new String[]&#123;"name", "age"&#125;); this.setViewName("/WEB-INF/view/test.jsp"); &#125; @Override public void control(Map&lt;String, String&gt; params, Map&lt;String, Object&gt; model) throws Exception &#123; // make model using params... &#125;&#125; 기계적으로 Controller 인터페이스의 handleRequest 메서드를 사용하지 말고 위와 같이 클래스를 확장하여 사용하는 것이 좋다. AnnotationMethodHandlerAdapter 앞의 핸들러 어댑터들과 달리 호출하는 컨트롤러의 타입이 정해져 있지 않다. 클래스와 메서드에 붙은 애노테이션, 메서드 이름, 파타미터, 리턴타입에 대한 규칙 등을 조합하고 분석해서 컨트롤러를 선별한다. 또한 다른 컨트롤러와 다르게 컨트롤러 하나가 여러 URL에 매핑될 수 있다.(이때까지는 1개의 URL에 1개의 컨트롤러가 매핑되었다) 이는 메서드 단위로 URL매핑이 가능하기 때문이다. 이 AnnotationMethodHandlerAdapter는 다른 핸들러 어댑터와는 다르게 DefaultAnnotationHandlerMapping 핸들러 매핑과 같이 사용해야 한다. 두 가지 모두 동일한 애노테이션을 사용하기 때문이다. 이 방식은 매우 강력하고 작성이 간결한 대신 꽤나 많은 규칙이 존재하고, 이를 잘 숙지하고 사용해야 한다. @RequestMapping 바로가기 핸들러 인터셉터 핸들러 매핑은 요청정보를 통해 컨트롤러를 찾아주는 기능 외에 인터셉터를 적용해주는 기능 또한 제공한다. 핸뜰러 인터셉터는 DispatcherServlet이 컨트롤러를 호출하기 전과 후에 요청, 응답을 가공할 수 있는 일종의 필터이다. 핸들러 매핑은 DispatcherServlet으로 부터 매핑 작업을 요청받으면 등록된 핸들러 인터셉터들을 순서대로 수행하고 컨트롤러를 호출한다. 등록된 핸들러 인터셉터가 없다면 컨트롤러를 바로 호출한다. 구현 핸들러 인터셉터를 만들고자 할 때는 아래의 HandlerInterceptor 인터페이스를 구현해야 한다. 123456789public interface HandlerInterceptor &#123; boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; void postHandle( HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception; void afterCompletion( HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception;&#125; preHandle 컨트롤러가 호출되기 전에 실행된다. 파라미터 handler는 컨트롤러 오브젝트이다. 리턴값이 true이면 다음 인터셉터로 진행되고, false일 경우 다음 인터셉터들을 실행되지 못한다. postHandle 컨트롤러를 호출한 후에 실행된다. ModelAndView가 제공되므로 작업결과를 참조하거나 조작할 수 있다. afterCompletion 모든 작업(뷰 생성까지)이 완료된 후 실행된다. 적용 적용할 핸들러 인터셉터들을 핸들러 매핑 클래스의 속성으로 지정해줘야 하므로, 핸들러 매핑 클래스를 빈으로 등록해줘야 한다. 이후 interceptors 프로퍼티를 이용하여 인터셉터를 등록한다. 1234567891011&lt;bean class="org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping"&gt; &lt;property name="interceptors"&gt; &lt;list&gt; &lt;ref bean="firstInterceptor" /&gt; &lt;ref bean="secondInterceptor" /&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean name="firstInterceptor" class="~~~FirstInterceptor" /&gt;&lt;bean name="secondInterceptor" class="~~~SecondInterceptor" /&gt; 보다시피 인터셉터들은 핸들러 매핑 단위로 등록된다. 즉 작성한 인터셉터들을 여러개의 핸들러 매핑에 적용시키고 싶으면 핸들러 매핑 빈 마다 반복적으로 다 등록해줘야 한다. 물론 인터셉터 빈은 한개만 등록하면 된다. 핸들러 인터셉터 vs 서블릿 필터 보다시피 핸들러 인터셉터는 서블릿 필터와 기능이 유사하지만 조금 차이가 있으니 선택에 주의를 기울여야 한다. 서블릿 필터는 web.xml에 등록하면서 웹 애플리케이션으로 들어오는 모든 요청에 적용할 수 있다는 장점이 있다. 반면에 스프링의 빈이 아니라는 점과, 핸들러 인터셉터보다 정교한 컨트롤이 어렵다는 단점이 있다. 핸들러 인터셉터는 특정 핸들러 매핑에 제한된다는 단점이 있지만, 인터셉터를 스프링 빈으로 등록할 수 있고 ModelAndView를 컨트롤 하는 등 더욱 정교한 컨트롤이 가능하다. 프로젝트의 상황에 따라 적절한 선택을 하는것이 좋다. From]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>토비의 스프링</tag>
        <tag>HandlerAdapter</tag>
        <tag>HandlerMapping</tag>
        <tag>HandlerInterceptor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] final class]]></title>
    <url>%2Fjava%2Ffinal-class%2F</url>
    <content type="text"><![CDATA[관련글 https://hashcode.co.kr/questions/388/자바에서-final-클래스는-어디에-사용하나요 이 클래스는 상속을 염두해두지 않은 클래스라는 것을 알려줌으로써, 해당 클래스를 상속받아 예기치 못하는 일이 일어나는 것을 막을 수 있다.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>final class</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[spring] DispatcherServlet Flow]]></title>
    <url>%2Fspring%2FDispatcherServlet-Flow%2F</url>
    <content type="text"><![CDATA[스프링 웹 기술은 MVC 아키텍쳐를 근간으로 한다. MVC 아키텍쳐 정보를 담은 모델(M), 화면 출력 로직을 담을 뷰(V), 제어 로직을 담은 컨트롤러(C)가 서로 협력하여 하나의 웹 요청을 처리하고 응답을 만들어내는 구조이다. MVC 아키텍처는 기본적으로 프론트 컨트롤러 패턴과 함께 사용된다. 프론트 컨트롤러 패턴 클라이언트의 요청을 먼저 받고 공통적인 작업을 수행 후, 나머지 작업을 세부 컨트롤러로 위임해주는 방식 스프링은 DispatcherServlet 이라는 프론트 컨트롤러를 사용하며, 이는 스프링 MVC의 핵심이다. DispatcherServlet flow 1) DispatcherServlet의 HTTP 요청 접수 들어오는 http 요청이 DispatcherServlet에 할당된 것이라면 이를 받는다. 대체로 아래와 같이 web.xml에 정의되어 있다. 12345678&lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 요청을 받은 DispatcherServlet은 공통적으로 진행해야 할 전처리 작업이 있다면 이를 먼저 수행한다. 2) DispatcherServlet에서 컨트롤러로 HTTP 요청 위임 DispatcherServlet은 들어온 http 요청 정보(URL, parameter, method 등)을 참고로 해서 어떤 컨트롤러에게 작업을 위임할 지 결정한다. DispatcherServlet은 프론트 컨트롤러이므로 세부 컨트롤러를 직접 찾지않고 HandlerMapping, HandlerAdapter라는 전략에 해당 행위를 위임한다.(자세한 내용은 위 링크에서 확인) 지금은 그냥 전달받은 HttpServletRequest, HttpServletResponse를 넘겨주면 세부 컨트롤러를 찾아준다 정도만 기억해도 된다. 3) 컨트롤러의 모델 생성과 정보 등록 컨트롤러는 사용자 요청을 해석하여 비즈니스 로직을 수행하고 결과를 받아온 뒤, 모델에 넣는다. (모델은 뷰에 뿌려줄 정보를 담은 key, value 형태의 맵이다.) MVC 패턴의 장점은 모델과 뷰가 분리되었다는 것이다. 같은 모델이라도 뷰만 바꿔주면 전혀 다른 방식으로 모델의 정보를 출력시킬 수 있다. 예를 들어 jsp뷰를 선택하면 html, 엑셀뷰를 선택하면 엑셀, pdf뷰를 선택하면 pdf로 모델정보를 출력할 수 있다. 4) 컨트롤러의 결과 리턴 : 모델과 뷰 모델이 준비되었으므로 뷰를 선택한다. 뷰 오브젝트를 직접 선택할 수도 있지만 보통은 뷰 리졸버를 사용므로 뷰의 논리적인 이름만을 리턴하면 된다. 컨트롤러가 최종적으로 리턴해주는 정보는 모델과 뷰, 2가지이다. 이를 리턴해주고 컨트롤러의 역할은 끝이난다. 5-6) DispatcherServlet의 뷰 호출과 모델 참조 다시 DispatcherServlet으로 넘어왔다. 뷰 오브젝트에 모델을 넘겨주며 최종 결과물을 생성해달라고 요청한다. 생성된 최종 결과물은 HttpServletResponse 오브젝트에 담긴다. 7) HTTP 응답 돌려주기 DispatcherServlet은 공통적으로 진행해야 할 후처리 작업이 있는지 확인하고 이를 수행한다. 수행이 끝나면 HttpServletResponse에 담긴 최종 결과를 서블릿 컨테이너에게 돌려준다. 컨테이너는 이 정보를 HTTP 응답으로 만들어 사용자의 브라우저나 클라이언트에 전송하고 작업을 종료한다. DispatcherServlet의 변경 가능한 전략 위에 나와있듯이, DispatcherServlet은 하나의 요청에 대해 상당히 많은 작업들을 거친다 (하나의 요청에 HandlerMapping 거치고 HandlerAdapter 거치고 ViewResolver 거치고 등등…) 이 작업들을 전략이라고 부른다. 기본적으로 default 전략들이 있고, 별다른 설정을 하지 않는다면 이 기본 전략들을 사용하여 요청을 수행하게 된다 중요한것은, 스프링의 DispatcherServlet은 전략패턴이 잘 적용되어 있으므로 이 전략들을 아주 쉽게 확장(변경)할 수 있다는 점이다(!!) 간단하게 확장하고 싶은 전략들을 빈으로 등록만 해두면, DispatcherServlet이 플로우를 수행하면서 해당 전략을 기본전략 대신해서 실행하게 된다(등록된 확장 전략들이 있는가 체크하고 있으면 수행, 없으면 기본 전략 수행) DispatcherServlet은 스프링이 관리하는 오브젝트가 아니므로 직접 DI 하는 방식으로 전략이 확장되는 것은 아니다. DispatcherServlet은 기본적으로 DispatcherServlet.properties 파일을 통해 설정을 초기화하고, 내부적으로 가지고 있는 어플리케이션 컨텍스트를 통해 확장 가능한 전략이 있나 찾은 뒤, 이를 가져와 디폴트 전략을 대신해서 사용하는 방식이다. 아래는 확장 가능한 전략들과, default로 등록된 전략들이다. HandlerMapping URL과 요청 정보를 기준으로 어떤 컨트롤러를 사용할지 결정한다. default BeanNameUrlHandlerMapping DefaultAnnotaionHandlerMapping HandlerAdapter 핸들러 매핑으로 선택된 컨트롤러를 직접 호출한다. default HttpReqeustHandlerAdapter SimpleControllerHandlerAdapter AnnotaionMethodHandlerAdapter HandlerExceptionResolver 예외가 발생했을 때 이를 처리하는 로직을 갖고 있다. 예외 발생 시 에러페이지 출력, 로그 전송등의 작업은 개별 컨트롤러가 아닌 프론트 컨트롤러의 역할이다. DispatcherServlet은 등록된 HandlerExceptionResolver중에서 발생한 예외에 적합한 것을 찾아 예외처리를 위임한다. default AnnotaionMethodHandlerExceptionResolver ResponseStatusExceptionResolver DefaultHandlerExceptionResolver ViewResolver 컨트롤러가 리턴한 논리적인 뷰 이름을 참고하여 적절한 뷰 오브젝트를 찾아준다. default InternalResourceViewResolver UrlBasedViewResolver LocaleResolver 지역정보를 결정해주는 전략이다. 디폴트는 헤더 정보를 보고 지역정보를 설정한다. 파라미터나 쿠키, xml 설정등을 통하게 할 수 있다. default AcceptHeaderLocaleResolver RequestViewNameTranslator 컨트롤러에서 뷰 오브젝트나 이름을 제공하지 않았을 경우 URL 요청정보를 참고해서 자동으로 뷰 이름을 생성해주는 전략이다. default DefaultRequestToViewNameTraslator 전략의 변경을 위해 빈을 직접 등록하게 되면 해당 전략의 default 전략들이 모두 무시되므로, 유의해야 한다. 또한 디폴트 전략에 추가 옵션을 주고 싶으면 해당 빈을 등록하면서 옵션을 줘야 한다. 참고 : 이일민, 『토비의 스프링 3.1』, 에이콘출판(2012)]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>토비의 스프링</tag>
        <tag>DispatcherServlet</tag>
        <tag>MVC</tag>
        <tag>Front Controller</tag>
        <tag>DispatcherServlet flow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] class, interface 상속]]></title>
    <url>%2Fjava%2Fclass-interface-%EC%83%81%EC%86%8D%2F</url>
    <content type="text"><![CDATA[class 기본적인 오버라이딩 123456789101112class Parent&#123; Integer test()&#123; return 0; &#125;&#125;class Child extends Parent&#123; Integer test()&#123; return 1; &#125;&#125;// child.test() == return 1; java는 기본적으로 이름이 같은데 리턴타입이 다른 메서드가 있을 수 없음. 뭘 호출하는지 알 수 없기 때문에. 12345678910class Parent&#123; Integer test()&#123; return 0; &#125;&#125;class Child extends Parent&#123; String test()&#123; // error return "1"; &#125;&#125; 필드의 경우 그냥 같은 이름은 무시됨 12345678class Parent&#123; Integer test = 0;&#125;class Child extends Parent&#123; String test = "1";&#125;// child.test == 1; interface 상수랑 추상메서드만 올 수 있음. 키워드는 자동으로 생략됨. 12345678interface Team&#123; // public static final int memberNumber 와 동일 // public static final은 생략해도 자동으로 붙음 int memberNumber = 11; // public abstract String printMembers() 와 동일 String printMembers();&#125; 상속은 기본적인 클래스 상속과 동일함. 특이점은 다중 상속이 된다는 것임. 12345678910111213141516171819interface A&#123; int a();&#125;interface B&#123; int b();&#125;// 2개 구현class Impl implements A, B&#123; @Override public int a()&#123; return 10; &#125; @Override public int b()&#123; return 20; &#125;&#125; 만약 메서드명이 겹치면 하나만 선언해도 됨 1234567891011121314interface A&#123; int a();&#125;interface B&#123; int a();&#125;// 1개 구현class Impl implements A, B&#123; @Override public int a()&#123; return 10; &#125;&#125; 같은 이름이지만 리턴타입이 다른 메서드를 가진 인터페이스를 다중 상속할 수 없음. 언급했듯이 자바는 기본적으로 동일한 이름에 다른 리턴타입의 메서드를 허용하지 않음. 123456789interface A&#123; int a();&#125;interface B&#123; String a();&#125;class Impl implements A, B&#123; // error&#125; 인터페이스 끼리 상속도 됨 12345678910111213141516171819interface A&#123; int a();&#125;interface B extends A&#123; int b();&#125;// 2개를 구현해야함class Impl implements B&#123; @Override public int a()&#123; return 10; &#125; @Override public int b()&#123; return 20; &#125;&#125; 인터페이스도 오버라이딩 되는데, default 메서드를 사용할때나 의미있지 평소에는 오버라이딩이 의미없음. 123456789101112interface A&#123; default int a()&#123; return 10; &#125;&#125;interface B extends A&#123; default int a()&#123; return 20; &#125;&#125;// impl.a() == 20 인터페이스 상속은 다중 상속을 지원함 123456789101112131415161718192021222324252627interface A&#123; int a();&#125;interface B&#123; int b();&#125;interface C extends A, B&#123; int c();&#125;// 3개를 구현해야함class Impl implements C&#123; @Override public int a()&#123; return 10; &#125; @Override public int b()&#123; return 20; &#125; @Override public int c()&#123; return 30; &#125;&#125; 필드는 구현체를 따라가지 않음. 기본적으로 static 이기 때문. 123456789101112131415161718interface A&#123; int a = 10;&#125;interface B extends A&#123; int a = 20;&#125;class Impl implements B&#123;&#125;public class Main&#123; @Test public void test()&#123; A a = new Impl(); System.out.println(a.a); // 10 B b = new Impl(); System.out.println(b.a); // 20 &#125;&#125; 이만하면 되지 않았을까…]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>class extends</tag>
        <tag>interface implements</tag>
        <tag>interface extends</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] database connection pool]]></title>
    <url>%2Fdb%2Fdatabase-connection-pool%2F</url>
    <content type="text"><![CDATA[database connection pool 정의 https://www.holaxprogramming.com/2013/01/10/devops-how-to-manage-dbcp/ 데이터베이스 작업에서 가장 비용이 많이 발생하는 부분은 커넥션을 생성하여 맺는 부분 db 작업이 일어날 때 마다 이렇게 커넥션을 생성하고 반납하면 매우 낭비가 크므로, 이러한 커넥션들을 미리 생성해서 풀에 담아두고 필요할 때 마다(db 요청이 있을 때 마다) 꺼내쓰도록 함 이를 커넥션 풀이라고 하고, Apache Commons DBCP를 사용 (정확히 어떻게 동작하는지 모르지만 LinkedList에 저장해놓고 적절히 반환하고 돌려받고 하는 듯 하다) 커넥션 풀 관련해서 여러가지 속성들(maxActive, maxIdle 등)이 있지만 이것보다 실제 커넥션 최대 개수가 성능상 가장 중요한 이슈이다 커넥션 개수를 설정하는 방법은 DBMS가 수용할 수 있는 커넥션 개수를 측정한 다음에, WAS 하나가 사용할 커넥션 풀 개수를 구하면 된다. 그리고 기본적으로 WAS의 Thread 수는 DBMS의 Connection 수보다 많아야 한다. 모든 작업이 DB를 필요로 하진 않기 때문이다. 쓰레드가 DBCP보다 한 10개 정도 많게 하면 적당하다. DB 커넥션 풀 100개 어플리케이션이 10개라면 각각 커넥션 풀 10개 어플리케이션 각각 쓰레드풀 20개]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>connection pool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] jdbc]]></title>
    <url>%2Fdb%2Fjdbc%2F</url>
    <content type="text"><![CDATA[JDBC java에서 mysql을 사용하기 위한 규약(?) JDBC는 그냥 껍데기(interface)일 뿐이고, DBMS 벤더들이 그 규약을 구현한 Driver 들을 제공한다. JDBC URL이 있다. 이것도 JDBC 규약에 있는 표준이다. jdbc:mysql:// 스트링은 고정이다. 요즘은 커넥션 풀을 사용하므로 아래의 코드가 익숙하지는 않곘지만… Class.forName(driverClass).newInstance(); 를 통해 드라이버를 JVM으로 로드하고, DriverManager.getConnection()으로 커넥션을 얻는다. 사용한 커넥션은 바로바로 반납해주는 것이 좋다. Statement VS PreparedStatement 기본적으로 JDBC 프로그래밍을 하면 Statement가 실행되는 과정은 아래와 같다. 1요청 -&gt; 쿼리 분석 -&gt; 최적화 -&gt; 권한요청 -&gt; 실행 쿼리 분석과 최적화에서 시간이 꽤 걸리는데, PreparedStatement는 이 과정을 미리 메모리에 저장해두고 다음 요청에서 바로 사용함으로써 시간을 많이 줄일 수 있게 된다. (PreparedStatement는 파라미터 바인딩을 지원하므로 완벽하게 똑같은 쿼리가 아니어도 상관없다.) 또한 PreparedStatement는 전달되는 파라미터 앞 뒤로 SQL injection에 문제될 문자들을 검사하고 escape 처리하기 때문에 개발자가 직접 처리해줘야 하는 번거로움이 없다.]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>java mysql</tag>
        <tag>preparedstatement</tag>
        <tag>sql injection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] 예외처리]]></title>
    <url>%2Fjava%2F%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[예외란 프로그램 실행 도중 발생하는 문제 상황을 얘기합니다. 따라서 컴파일 시 발생하는 문법적인 오류는 예외의 범주에 포함되지 않습니다. 예를 들면 아래와 같은 상황이 있습니다. 1234public static void main(String[] args)&#123; String str = null; System.out.println(str.length());&#125; 이 상황은 문법적으로는 문제가 없으므로, 컴파일 오류가 발생하지 않습니다. 그런데 실행하면 와 같이 Exception이 발생합니다. str변수에서 length 메서드를 호출한건데, str에는 현재 null이 들어가 있으므로(주소값이 없으므로) 비어있는 주소를 참조하여 메서드를 실행하려 했다… 뭐 이런의미로 NullPointerException이 발생한 겁니다. 근데 보다시피 해당 NullPointerException은 java.lang 패키지 내에 있는 클래스네요. 이렇듯 Exception라고 별 다른게 아니라 다 클래스들입니다. 예외적인 상황에 따라 대부분의 클래스가 정의되어 있고, 약속된 예외 상황에 따라 해당 예외 클래스가 발생합니다. 몇가지 예를 볼까요 NullPointerException : 참조변수가 null인 상황에서 메서드를 호출하는 상황 NegativeArraySizeException : 배열 선언 과정에서 배열의 크기를 음수로 지정하는 상황 ClassCastException : 허용되지 않는 형변환을 하는 상황 이렇듯 대부분 상황에 따라 정의되어 있습니다. 예외 클래스의 계층도 예외도 결국 클래스라고 했습니다. 그러므로 각 클래스간 계층이 존재하고, 이에 따라 예외의 종류도 나뉩니다. 보다시피 Throwable이 예외 클래스의 최상위 클래스입니다. 그리고 아래에 Error, Exception 2가지 클래스가 있는데, 이 2개의 클래스가 예외의 큰 범주입니다. Error 단순히 예외라고 하기에는 심각한 오류의 상황을 표현하는 예외입니다. 위의 그림에서 OutOfMemoryError, StackOverflowError 등이 보이시죠? 다들 많이 보셨을겁니다. 한 단계 올라가보면 VirtualMachineError로 자바 가상머신에 문제가 생겼음을 알려주고 있습니다. 한 단계 더 올라가보면 Error 클래스가 보이지요. 이처럼 심각한 오류들은 다 Error 예외의 하위 예외로 정의되어 있습니다. 이건 뭐… 저희가 할 수 있는 특별한 방법이 없으므로 그냥 프로그램이 종료되도록 놔두는 수밖에 없습니다. 종료 후에 원인을 찾고 해결하던가 해야합니다. 당장 애플리케이션 단에서는 저희가 조치를 취할 방법이 없습니다. Exception 일반적으로 우리가 마주하게 되는 예외들로, 저희가 직접 처리할 수 있는 대부분의 예외들을 말합니다. Exception 내에서도 또 종류가 2가지로 나뉩니다. 예외 처리 방법 예외처리의 대상이 되는 Exception 예외의 하위 예외를 대상으로 하며, 처리 방법에는 2가지가 있습니다. try ~ catch [ ~finally ] 사용자가 직접 예외를 처리하는 방법입니다. 123456789101112131415public static void main(String[] args)&#123; try&#123; String str1 = null; String str2 = "asd"; System.out.println(str1.length()); System.out.println(str2.length()); &#125; catch(NullPointerException e)&#123; System.out.println("참조변수의 값이 NULL 입니다."); &#125; catch(Exception e)&#123; System.out.println("다른 예외가 발생하였습니다."); &#125; finally&#123; System.out.println("마지막에 항상 실행되는 부분입니다."); &#125;&#125; 예외가 발생할 수 있는 부분을 try 구문으로 감싸고, 예외가 발생 시 발생 시점으로 부터 더 이상 try 부분의 코드는 진행되지 않고 catch 구문으로 들어가게 됩니다. catch의 파라미터는 try 구문에서 발생한 예외 클래스를 받게 됩니다. 다형성 가능합니다. catch 구문은 1개 이상 정의 가능하며(하나의 로직에서 발생할 수 있는 예외는 1개 이상이기 때문에), 위에서 부터 순차적으로 실행됩니다. 하나의 catch 구문에 들어갔을 경우 그 아래 catch구문은 건너뛰게 됩니다. switch case 처럼요. 그리고 마지막 finally 구문은 예외가 발생하든, 발생하지 않든 언제나 실행하는 부분으로써 선택적인 사항입니다. 결과는 아래와 같습니다. 먼저 System.out.println(str1.length()); 부분에서 null값 참조로 NullPointerException이 발생하게 됩니다. (예외가 발생했으므로 진행이 멈추게 되어 그 아래 System.out.println(str2.length()); 은 진행되지 않습니다.) 발생된 NullPointerException은 try 구문을 빠져나와 아래의 catch 구문에서 자기가 들어갈 곳을 찾게 됩니다. 위쪽부터 살펴보니 파라미터로 NullPointerException을 받는 catch 구문이 있네요. 구문에 진입하고, 구문 내의 행위를 실행합니다. catch 구문의 선택은 switch case 와 같다고 했었죠… NullPointerException 예외 처리 구문에 들어갔으니 아래의 Exception 예외 처리 구문에는 들어가지 않게 됩니다. 근데 여기서 유의하고 넘어가야 할 부분이 있습니다. catch 구문의 위치를 바꾸었다면 어떻게 될까요? 예를 들어 아래와 같이 예외를 처리했을 경우를 보시죠. 1234567catch(Exception e)&#123; // Exception 예외 처리&#125; catch(IOException e)&#123; // IOException 예외 처리&#125; catch(NullPointerException e)&#123; // NullPointerException 예외 처리&#125; Exception은 모든 예외의 상위 클래스이므로, 발생하는 예외를 모두 다 받을 수 있습니다. 상속관계도 가능하다고 했었죠. 즉, 이런식으로 예외처리 지정해버리면 백날 천날 예외 터져봐야 젤 위의 Exception 받는 부분에서 다 걸리므로 아래에 IOException, NullPointerException에 대한 예외 처리는 무용지물이 됩니다. 항상 유의해야 합니다. 마지막으로 finally 구문은 선택적인 부분입니다. 써도 그만 안 써도 그만. try 구문이 정상적으로 끝나든, 예외가 발생해서 catch 구문에 들어가고 끝나든 언제든 실행 될 부분을 적어주면 됩니다. 예를 들면 자원 반납, 로깅 처리 등이 있습니다. throws throws는 던지다 라는 의미를 갖고 있습니다. 그리고 예외처리에서의 throws도 동일한 의미로 사용됩니다. 발생된 예외를 자신을 호출한 쪽으로 던져버리는 것입니다. 처리를 위임한다고 표현하면 되곘네요. 사용 예는 아래와 같습니다. 1234567891011121314151617181920212223242526272829public static void main(String[] args)&#123; try&#123; System.out.println(calculator('+',1,2)); System.out.println(calculator('/',5,0)); System.out.println(calculator('*',3,3)); &#125; catch(ArithmeticException e)&#123; System.out.println("0으로 나눌 수 없습니다."); &#125;&#125;static int calculator(char sign, int num1, int num2) throws ArithmeticException&#123; int result = 0; switch(sign)&#123; case '+': result = num1 + num2; break; case '-': result = num1 - num2; break; case '*': result = num1 * num2; break; case '/': result = num1 / num2; // 0으로 나누는 경우가 발생할 수 있다 break; &#125; return result;&#125; 간단한 계산기 프로그램입니다. 보다시피 / 연산에서 0으로 나누는 예외인 ArithmeticException이 발생할 수 있지만 그에 대한 처리가 전혀 없습니다. 근데 잘 보시면 처리가 없는 대신 위에 throws 에서 ArithmeticException을 선언해주고 있네요. 위 구문의 의미는, ArithmeticException이 발생할 경우 자신을 호출한 메서드 쪽으로 그 처리를 던지겠다는 의미입니다. 위의 상황에서 calculator를 호출한 쪽은 main 메서드이므로, 보다시피 main 메서드에서 해당 예외를 잡아 처리하고 있음을 볼 수 있습니다. JVM의 예외처리 main 메서드는 프로그램의 시작점이지만, main 메서드도 예외를 받았을 경우 throws로 던져버릴 수 있습니다. 그러면 결국 main을 호출한 쪽으로 던져지게 됩니다. main을 호출한 영역은 가상머신(JVM) 이죠. 결과적으로 예외처리가 가상머신에 의해 이루어지게 되는 것입니다. 가상머신의 예외처리 방식 getMessage 메서드를 호출한다. printStackTrace 메서드를 호출해 예외상황이 발생해서 전달되는 과정을 출력해 준다. 프로그램을 종료한다. getMessage, printStackTrace 메서드는 예외의 최 상위 클래스인 Throwable에 정의된 메서드입니다. getMessage는 예외에 대해 정의된 간단한 메시지를 보여주며, printStackTrace는 예외 발생 과정을 상세하게 출력해줍니다. 위의 calculator에서 Throwable 제공 메서드들을 사용해보겠습니다. 12345678try&#123; System.out.println(calculator('+',1,2)); System.out.println(calculator('/',5,0)); System.out.println(calculator('*',3,3));&#125; catch(ArithmeticException e)&#123; System.out.println(e.getMessage()); e.printStackTrace();&#125; 결과는 아래와 같습니다. getMessage는 간단하게 예외의 내용을 출력해주고, printStackTrace는 예외 발생 과정을 상세하게 출력해줍니다. 특히나 printStackTrace는 예외 발생 시 원인을 찾는데 상당한 도움이 됩니다. 사용자 정의 예외 앞서 언급했던 NullPointerException, ArithmeticException 등의 경우, 실행 시 문제가 되므로 자바 가상머신에서 예외로 정의해놓았습니다. 미리 정의된 예외들이죠. 하지만 이 외에도 개발자가 직접 예외를 정의하는 방법도 있습니다. 예를 들어 비즈니스 로직에서의 예외가 있을 수 있습니다. 입/출금 관련 프로그램이 있다고 가정했을 때 잔고보다 많은 돈을 출금하려는 경우 자바 프로그램 상에서는 전혀 문제가 안되지만, 입/출금 관련 업무에서는 예외 상황이 됩니다. 현실에선 마이너스 잔고라는게 없기 때문입니다. (마이너스 통장 말고 일반 통장 기준입니다…ㅋㅋ) 이럴 경우는 개발자가 직접 예외를 정의해줘야 합니다. 위의 상황을 간단하게 정의해보겠습니다. 일단 사용자 정의 예외를 만들어줍니다. 예외 클래스가 되는 조건은 아래와 같습니다. Exception 클래스를 상속한다 Exception은 예외 클래스의 상위 클래스입니다. 따라서 이를 상속함으로써 해당 클래스는 예외 클래스가 되고, try ~ catch 구문에 활용이 가능한 예외 클래스가 됩니다. 먼저 위의 상황에 맞는 예외 클래스를 하나 만들어보겠습니다. 12345public class NoMoneyException extends Exception&#123; public NoMoneyException()&#123; super("돈이 없습니다.."); &#125;&#125; 별다른 처리 없이 간단히 문자열을 출력하게 하였습니다. Exception 클래스를 상속하였으므로, 모든 예외처리 문법에서 사용 가능합니다. 1234567891011static int totalMoney = 100000;static int withdraw(int money) throws NoMoneyException&#123; if(money &gt; totalMoney)&#123; // 예외 상황 발생시 throw new NoMoneyException(); // 예외 발생! &#125;else&#123; totalMoney -= money; &#125; return totalMoney;&#125; 전달받은 돈이 전체 돈보다 작을 경우는 위에서 우리가 언급한 예외 상황이 됩니다. 그리고 해당 예외 상황이 발생했을 경우, throw 구문을 사용하여 예외를 발생시킵니다. (new로 해당 예외를 생성하는 순간 해당 메서드에 예외가 발생된 것이기 때문에 해당 메서드에 예외 처리 구문이 필요합니다. 위와 같이 throws를 통해 호출한 곳으로 던져줘도 되고, 해당 메서드에서 직접 처리해도 됩니다.) main에서 예외상황을 발생시켜 보겠습니다. 123456789static int totalMoney = 100000;public static void main(String[] args)&#123; try&#123; System.out.println(withdraw(10000000)); &#125; catch(NoMoneyException e)&#123; e.printStackTrace(); &#125;&#125; 잔고는 10만원인데 1000만원 인출을 시도하고 있습니다… 대단한… 가슴 아픈 예외가 발생했네요… 보다시피 상황에 맞춰 정의했던 예외가 발생하고 있음을 보실 수 있습니다! 체크 예외, 언체크 예외 우리가 마주하는 일반적인 예외인 Exception 하위 예외들은 그 사이에서도 2가지로 종류가 나뉩니다. 체크 예외와 언체크 예외 라는 이름으로 나뉩니다. 이는 문법적으로도 차이가 있으니 알고 가시는게 좋습니다. 일단 해당 예외를 나누는 기준은, RuntimeException 이라는 예외의 상속 여부입니다. Exception의 하위 예외이면서 RumtimeException을 상속하지 않았을 경우 체크 예외, RumtimeException을 상속했을 경우 언체크 예외입니다. (RumtimeException은 Exception의 서브 클래스이므로, 상속하면 자동으로 Exception의 하위 예외가 됩니다.) 체크 예외 RumtimeException을 상속하지 않은 일반적인 예외들입니다. 반드시 try ~ catch나 throws를 통해 처리를 해줘야 하기 때문에 체크 예외라고 부릅니다. 항상 문법적으로 체크한다 라는 의미로 보시면 좋을 것 같습니다. 예를 들면 IOException, SQLException 등이 있습니다. 123456789public static void main(String[] args)&#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); try&#123; System.out.println(br.readLine()); &#125; catch(IOException e)&#123; System.out.println("IOException 발생"); &#125;&#125; 앞서 작성했던 예외처리와 별 다를것 없어 보이지만, 이를 직접 IDE에서 코딩해보시면 차이를 볼 수 있습니다. IOException은 체크 예외이므로 위와 같은 try ~ catch 구문이나, throws 구문이 없으면 컴파일 오류가 발생합니다. 프로그램상에서 반드시 이 예외를 처리하고 넘어가도록 강조하는 것입니다. 만약 throws를 통해 체크 예외를 던져줬다면, 호출하는 쪽에서도 해당 예외를 처리하는 구문을 필수로 입력해야 합니다. throw를 통해 체크 예외를 발생시킬 경우도 마찬가지입니다. 처리가 없을 경우 컴파일 오류가 발생합니다. 언체크 예외 RumtimeException을 상속한 예외입니다. 체크 예외처럼 해당 예외에 대한 처리를 문법적으로 강요하지 않기 때문에 언체크 예외라고 부릅니다. 개발자가 부주의할 경우 발생할 수 있는 예외들이라, 예상하지 못한 상황에 발생할 수 있는 그런 예외들이 아니므로 명시적인 처리를 강요하지 않은 것입니다. 예를 들면 NullPointerException, IllegalStatementException 등이 있습니다. 대부분의 예외가 런타임 예외입니다. 1234public static void main(String[] args)&#123; String str = null; System.out.println(str.length());&#125; 보다시피 해당 예외는 null값 참조로 인한 NullPointerException이 발생하는 코드입니다. 하지만 NullPointerException은 언체크 예외이므로 위의 코드는 컴파일 오류가 발생하지 않습니다. throws를 통해 예외를 던져줬을 경우, throw를 통해 예외를 발생시켰을 경우에도 마찬가지입니다. throws의 경우 호출하는 쪽에 해당 예외를 처리하는 코드가 없어도 되고, throw의 경우에도 발생하는 메서드에 해당 예외를 처리하는 코드가 없어도 됩니다. 해당 예외를 처리하는 것은 선택사항입니다.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java exception</tag>
        <tag>try catch</tag>
        <tag>checked exception</tag>
        <tag>unchecekd exception</tag>
        <tag>runtime exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] fetch, pull 차이]]></title>
    <url>%2Fgit%2Ffetch-pull-%EC%B0%A8%EC%9D%B4%2F</url>
    <content type="text"><![CDATA[fetch는 원격 저장소의 변경 내역을 가져오는 것이고 직접 로컬 branch에 반영하진 않는다 반면 pull은 fetch 한 내역을 로컬 branch에 merge까지 한다 그러므로 pull은 branch를 지정해야 가져올 수 있고, fetch는 branch를 지정해도 되고, 그냥 원격 저장소만 지정해도 된다 fetch로 가져온 내용은 checkout할 수 있다 12git checkout origin/developgit checkout another-origin/develop 위처럼 말고도 fetch_head 로도 checkout 할 수 있는데, 정확히 무슨 기준으로 checkout fetch_head가 결정되는지는 모르겠다(1개 이상의 branch를 fetch 했을 경우) fetch 한 상태에서 git merge 혹은 git pull 입력 시 기존의 git pull과 동일한 행위를 하게 된다. (branch를 입력하지 않을 경우 config를 참조하게 됨)]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>fetch</tag>
        <tag>pull</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[db] where과 on절 차이]]></title>
    <url>%2Fdb%2Fwhere%EA%B3%BC-on%EC%A0%88-%EC%B0%A8%EC%9D%B4%2F</url>
    <content type="text"><![CDATA[아래의 두 문장은 어떻게 다를까? 12345678910select count(*) from dept_emp deleft outer join departments don de.dept_no = d.dept_nowhere d.dept_name = 'Development';select count(*) from dept_emp deleft outer join departments don de.dept_no = d.dept_no and d.dept_name = 'Development'; select 절의 처리 순서를 찾아보면, where 이 on 보다 먼저 실행되는 것을 볼 수 있다 첫번째 문장의 경우 부서명이 Development 인 부서만을 들고온 상태에서 dept_emp 테이블과 조인하므로 부서명이 Development 가 아닌 부서는 결과에 포함되지 않는다 즉 필터 후 조인 이다 두번째 문장의 경우 모든 부서를 들고온 후 전달된 조건(부서번호가 같고 부서명이 Development)으로 조인하는 것이므로 dept_emp 의 로우가 departments 의 모든 로우를 돌면서 결과를 찾게 된다 이 상태에서 조인의 형태가 left join 이므로 dept_emp 의 모든 데이터가 다 남게된다 이 쿼리의 결과로는 부서명이 Development 가 아닌 부서도 결과에 포함된다 이렇게 둘 간의 처리 방식의 차이가 있기 때문에 left join 시 결과가 다르게 나오는 상황이 발생한다(inner join 시에는 같다) 아무래도 join 은 on 절에 명시된 조건으로 driving table 이 driven table을 모두 체크하는 구조이다 보니, 첫번째 쿼리처럼 where 로 driven 테이블의 로우를 줄여주고 시작하는 것이 좋다]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>where</tag>
        <tag>on</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[git] hexo와 github pages로 블로그 만들기]]></title>
    <url>%2Fgit%2Fhexo%EC%99%80-github-pages%EB%A1%9C-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0%2F</url>
    <content type="text"><![CDATA[이번엔 앞서 작성한 github-pages에 블로그 서비스를 하기 위해 정적 사이트 생성 도구인 hexo에 대해 알아보겠습니다. Hexo 블로그 형태의 정적사이트를 생성하는데 사용되는 도구입니다. hexo는 사용자가 작성한 포스트(markdown 등)을 읽어서, 정적파일 생성기를 통해 웹서버에 바로 서비스 할 수 있는 형태의 정적 웹사이트를 만들어냅니다. 대표적인 것으로 jekyll이 있지만 hexo가 좀 더 편해보이고 테마도 맘에 들어서 hexo를 사용하기로 했습니다 ㅎㅎ 설치 사전준비 : Node.js,npm,git 바로 설치하고 초기화 해보겠습니다. 12npm install -g hexo-clihexo init '폴더명' '폴더명’에 입력한 폴더를 만들고 그 폴더에 hexo 관련 파일을 초기화합니다. (폴더를 지정하지 않으면 현재 폴더에 초기화하는데, 현재 폴더가 비어있는 상태여야 합니다.) 아래는 초기화 후 폴더 모습입니다! 빨간색으로 표시해둔 _config.yml에서 블로그에 대한 대부분의 설정을 할 수 있습니다. 초기화가 완료되면 간단하게 로컬에서 테스트 해보도록 할까요 해당 폴더로 이동하여 1hexo server 라고 입력하면, INFO Start processing INFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 라는 메시지와 함께 http://localhost:4000 으로 접속 가능합니다. 기본 테마로 생성된 정적 블로그 페이지를 볼 수 있을 것입니다 ㅎㅎ 테마 적용 하지만 이대로 사용할 순 없으니 테마를 한번 적용해보도록 하죠. 적용방법은 매우 간단합니다. https://hexo.io/themes/index.html 위의 주소에 접속한 뒤, 마음에 드는 테마를 고르시면 됩니다. 각 테마의 github 페이지에 들어가면 테마 적용 방법에 대한 상세한 설명이 있으니 별로 어려움 없으실 겁니다 ㅎㅎ 제가 고른 테마는 Material Flow 라는 테마입니다. gitgub : https://github.com/stkevintan/hexo-theme-material-flow 보시다시피 매우 간단합니다. 소스를 clone받고 _config.yml에서 해당 테마로 지정해주기만 하면 됩니다. 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: material-flow 설정이 다 되었으면 12hexo cleanhexo generate # 정적 리소스 생성 와 같이 입력하여 정적 리소스를 생성해주면 됩니다. 간혹 제대로 되지 않는 경우도 있기 떄문에 clean도 한번 해줬습니다. 이제 다시 hexo server 입력 후 들어가보시면 테마가 잘 적용되어 있음을 보실 수 있습니다! 글을 써보자 블로그를 만들었으니 글을 써야겠네요. 123hexo new post [post_name]# ex) hexo new post 'first post'# ex) hexo new post first-post 과 같이 입력하면, 폴더에 아래와 같은 형태로 markdown 파일이 하나 생성됩니다.1234567```md---title: &apos;[git] first post&apos;date: 2017-09-23 10:51:08tags:--- 각종 폴더나 카테고리에 대한 설정도 _config.yml에서 할 수 있으니 각자 설정하시면 됩니다 ㅎㅎ 배포 이제 github에 배포해보도록 하겠습니다 ㅎㅎ 먼저 _config.yml에 deploy 관련 설정을 해 줍니다. 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/joont92/joont92.github.io.git branch: master 저장한 뒤 123456hexo cleanhexo generate # 정적파일 생성하고hexo deploy # 배포!# hexo deploy --generate 로도 가능 와 같이 해주면 끝입니다. 매우 간단하죠?? 배포시 아래와 같은 메시지와 함께 배포가 되지 않는 경우 1ERROR Deployer not found: git hexo-deployer-git 플러그인을 설치해주면 됩니다. 1npm install hexo-deloyer-git --save 여기까지입니다 ㅎㅎ 블로그에 markdown을 사용할 수 있고, git의 형상관리를 블로그에 사용할 수 있다니 매우 좋은것 같네요. 들려주셔서 감사합니다~~]]></content>
      <categories>
        <category>git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[markdown]]></title>
    <url>%2Fetc%2Fmarkdown%2F</url>
    <content type="text"><![CDATA[문단 쓰기, 줄바꿈 기본적으로 마크다운은 엔터 한번은 그냥 무시하고 한 문단으로 간주합니다. 띄워쓰기를 표시하려면 2번의 엔터를 쳐야 합니다. 그러나 몇몇 마크다운 에디터에서는 ‘엔터 한번’ == '개행’을 지원하기도 합니다. 1234개행 테스트개행!진짜 개행 개행 테스트 개행! 진짜 개행 html 직접 사용 가능 12&lt;hr /&gt;&lt;span style=&quot;color:red&quot;&gt;**red**&lt;/span&gt; &gt; 출력 **red** 특수문자 자동 적용됨 12&lt;div&gt; 태그를 사용하자&amp;lt;div&amp;gt; 태그를 사용하자 &gt; 출력 태그를 사용하자 &lt;div&gt; 태그를 사용하자 제목. 6개 크기 까지 지원한다 123456789101112131415# h1## h2### h3#### h4##### h5###### h6####### h7# h1## h2### h3#### h4##### h5###### h6####### h7 인용 1234&gt; 인용되는 글&gt; 길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게&gt;&gt; depth를 늘림&gt; 내부에서 **markdown** 사용 가능 &gt; 출력 인용되는 글 길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게길게 depth를 늘림, 변경할때는 아래와 같이 빈줄 하나 넣어야 함. 내부에서 markdown 사용 가능 리스트 12345678910111213141516171819202122232425262728293031숫자의 경우 자동으로 넘버링 된다.처음쓰는 리스트의 숫자부터 카운트 된다.1. 숫자2. 숫자3. 숫자순서를 바꿔도 1,2,3 으로 출력된다1. 숫자4. 숫자7. 숫자일반 리스트는 *, +, - 중 아무거나 쓰면 된다.* 일반 리스트- 일반 리스트+ 일반 리스트리스트 안에 두 문단이상 사용할 경우, tab을 사용한다.계속해서 이어가려면 아래와 왼쪽 탭 정렬선을 맞춰줘야 한다.- 첫번째 리스트이어지는 문단1이어지는 문단2- 두번째 리스트 &gt; 출력 숫자의 경우 자동으로 넘버링 된다. 처음쓰는 리스트의 숫자부터 카운트 된다. 숫자 숫자 숫자 순서를 바꿔도 1,2,3 으로 출력된다 숫자 숫자 숫자 일반 리스트는 *, +, - 중 아무거나 쓰면 된다. 일반 리스트 일반 리스트 일반 리스트 리스트 안에 두 문단이상 사용할 경우, tab을 사용한다. 계속해서 이어가려면 아래와 왼쪽 탭 정렬선을 맞춰줘야 한다. 첫번째 리스트 이어지는 문단1 이어지는 문단2 두번째 리스트 이스케이프 문자 태그로 변형되는 문자를 그냥 사용하고 싶을 때가 있습니다.(&gt;, - 등) 그냥 사용하면 md에서 해석해버리므로 앞에 \ 를 붙여줍니다. 123\&gt; : 원래는 인용으로 표시됨1998\.서울올림픽 : 원래는 1.서울올림픽으로 표시됨(자동 넘버링)\- : 원래는 리스트로 표시됨 &gt; : 원래는 인용으로 표시됨 1998.서울올림픽 : 원래는 1.서울올림픽으로 표시됨 - : 원래는 리스트로 표시됨 코드 블록 마크다운 내에서 코드를 표현 가능하다! 키보드 왼쪽 가장자리에 있는 ` 키를 사용하면 된다. ``` 옆에 사용하는 언어를 넣어주면 해당 문법에 맞게 해석한다 &lt;!–￼7–&gt; &gt; 출력 12var str = 'test str';console.log(str); 수평선 123456편한대로 골라쓰면 된다.**** * *---- - - &gt; 출력 편한대로 골라쓰면 된다. 강조하기 *나, _를 사용하여 표시한다. 12*이탤릭***볼드** &gt; 출력 이탤릭 볼드 취소선 1~~취소~~ &gt; 출력 취소 링크 걸기 링크 주소 자체에 링크를 거는 방법과, 문구를 사용하는 방법이 있다. 물론 상대경로도 가능하다! &lt;http://주소&gt; [링크걸 문구](http://주소) 12345&lt;http://google.com&gt;[구글](http://google.com)상대경로 : [1번 포스트](/1) &gt; 출력 http://google.com 구글 상대경로 : 1번 포스트 이미지 삽입 ![사진이름](사진경로) 1![테스트사진](http://leader.pubs.asha.org/data/Journals/ASHANL/934378/NIB1_web.png) &gt; 출력 테이블 | 와 - 로 구분하여 테이블을 그릴 수 있다. 1234Field1 | Field2 | Field3-|-|-Data1 | Data2 | Data3Data4 | Data5 | Data6 &gt; 출력 Field1 Field2 Field3 Data1 Data2 Data3 Data4 Data5 Data6 헤드부분, 즉 html의 th부분은 꼭 있어야 한다! (상단 2줄) emoji github은 emoji를 지원한다. https://www.webpagefx.com/tools/emoji-cheat-sheet/]]></content>
      <categories>
        <category>etc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[java] java se, ee 차이]]></title>
    <url>%2Fjava%2Fjava-se-ee-%EC%B0%A8%EC%9D%B4%2F</url>
    <content type="text"><![CDATA[토론글 https://okky.kr/article/278014 java SE, EE는 스펙이다(패키지는 SE는 java.xxxx, EE는 javax.XXXX 이다.) java EE 스펙은 SE 스펙을 포함한다. SE를 구현한 애들이 oracle, openJdk 등등인 것. java EE를 구현한 애들은 tomcat, jeus, jboss 등이다. 그래서 톰캣을 설치할 때 java를 미리 설치해둬야 하는 것.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java se</tag>
        <tag>java ee</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[linux] bash_profile과 bashrc]]></title>
    <url>%2Flinux%2Fbash-profile%EA%B3%BC-bashrc%2F</url>
    <content type="text"><![CDATA[리눅스에서 alias를 수정하거나 PATH를 변경할 떄 접하게 되는 대표적인 4가지의 파일들… 1/etc/profile, /etc/bashrc, ~\.bash_profile, ~\.bash_rc profile /etc/profile 파일의 경우 전역적인 파일로 모든 사용자가 로그인 시 실행되며, ~/.bash_profile 파일의 경우 지역적인 파일로 해당하는 사용자가 로그인 시만 실행된다. 또한 /etc/profile의 경우 어떠한 shell이든 상관없지만, ~/.bash_profile의 경우 bash shell일 경우에만 해당된다. bashrc profile과 달리 Login 과정이 없으므로 shell을 실행시키는 사용자로 구분한다. /etc/bashrc의 경우 모든 사용자가 shell을 실행시킬 때 마다 실행되며, ~/.bashrc의 경우 해당하는 사용자가 shell 실행시킬 때 실행된다. Login Shell, Non-Login Shell 두 파일의 차이는 로그인 여부인데, 쉘의 종류에도 Login Shell, Non-Login Shell이 있다 Login Shell Shell을 실행할 때 로그인이 필요한 경우를 말한다. ssh로 접속하거나, su 명령어로 다른계정을 들어갈 때 등이 해당된다. /etc/profile, ~/.bash_profile Non-Login Shell Shell을 실행할 떄 로그인이 필요하지 않은 경우를 말한다. 즉 Shell이 실행되는 모든 상황을 의마한다. GUI에서 터미널을 띄울때나, bash 명령어로 다시 bash를 실행하는 경우 등이 해당된다. /etc/bashrc, ~/.bashrc Non-Login Shell은 Login Shell을 포함한다. Login Shell이 실행될 때 profile과 bashrc 파일이 모두 실행되게 되고, Non-Login Shell이 실행될 때 bashrc 파일만 실행되게 된다. profile의 경우 대부분 환경 변수같은 것을 명시하고 bashrc의 경우 alias 같은 것을 명시한다…(?)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>bash_profile</tag>
        <tag>bashrc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[linux] apt 명렁어]]></title>
    <url>%2Flinux%2Fapt-%EB%AA%85%EB%A0%81%EC%96%B4%2F</url>
    <content type="text"><![CDATA[패키지란 커널 및 라이브러리 버전의 배포판 환경에 맞추어 빌드한 실행파일을 압축한 것. apt란 우분투에서 쓰이는 데비안 계열의 패키지를 관리하는데 쓰이는 도구이다. 패키지 저장소 리스트 : /etc/apt/sources.list 이 패키지 저장소 덕분에 일일히 홈페이지를 검색하며 들어가는 등의 수고를 할 필요가 없어지게 된다. 하지만 모든 프로그램이 우분투 공식 패키지 저장소에 들어갈 순 없다. 그래서 PPA(Personal Package Archivce)라는 개인 패키지 저장소를 이용한다. (공식 패키지 저장소가 느릴때 PPA를 사용하기도 한다. 공식 저장소에는 몇만개의 패키지가 있으므로…) 해당 패키지 저장소를 apt 패키지 저장소에 추가해주면 해당 패키지 저장소를 통해 패키지를 내려받을 수 있게 된다. 저장한 PPA 목록은 /etc/apt/sources.list.d 에서 확인할 수 있다. 12345678#추가sudo add-apt-repository '저장소이름'#삭제sudo add-apt-repository --remove '저장소이름'#이후작업sudo apt-get update # 저장된 패키지 저장소를 토대로 패키지 목록을 업데이트 한다sudo apt-get install '패키지명' # 패키지를 다운로드한다. 패키지 리스트 업데이트 1apt-get update &gt; 실제 패키지를 업그레이드 하는 것이 아니라 사용가능한 패키지 리스트의 정보를 업데이트 패키지 업데이트 1apt-get upgrade &gt; 실제 설치되어 있는 패키지들을 최신 버전으로 업그레이드 패키지 설치 1apt-get install [패키지명] 패키지 재설치 1apt-get --reinstall install [패키지명] 패키지 삭제 12apt-get remove [패키지명] # 설정파일은 지우지 않음apt-get purge [패키지명] # 설정파일까지 지움 패키지 검색 1apt-cache search [패키지명] 패키지 정보 1apt-cache show [패키지명] apt 명령어를 이용해 설치한 패키지는 /var/cache/apt/archives 에 설치된다.]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[linux] 프로세스]]></title>
    <url>%2Flinux%2F%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%2F</url>
    <content type="text"><![CDATA[작업 : 작성한 프로그램 + 프로그램 실행에 필요한 데이터 작업이 커널에 등록되어 커널의 관리하에 있게 되면 이를 프로세스라고 부른다. 일반적으로 실행중인 프로그램을 말한다. 종속된 자식 프로세스는 부모 프로세스의 환경을 물려받으나 역은 성립하지 않는다.]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[linux] 파일/디렉토리 관리]]></title>
    <url>%2Flinux%2F%ED%8C%8C%EC%9D%BC-%EB%94%94%EB%A0%89%ED%86%A0%EB%A6%AC-%EA%B4%80%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[와일드 카드 ? : 어떤 한 문자 : 어떤 문자도 없거나 그 이상 [] : [] 내에 지정된 각각의 문자 [a-d] 는 a,b,c,d를 의미, [acd]는 a,c,d를 의미한다. [abcd]* 는 a,b,c,d로 시작하는 파일 모두를 의미한다. [abcd]?, [a-d]? 등등의 예시가 많다. “rm [abc]*” 처럼 사용할 수 있다. 리다이렉션 표준 입출력을 키보드나 모니터가 아닌 파일로 받거나, 출력하는 것을 의미. 표준 출력의 변경 : &gt;, &gt;&gt; 결과를 파일로 보냄, &gt;&gt;는 기존 파일의 내용을 유지한 채 내용을 추가할 떄 사용한다. access.log 와 같이 사용하면 access.log 파일의 내용을 모두 지우고 크기를 0으로 만들어 버릴 수 있다. 아무것도 없는 내용을 access.log에 전달하기 때문 표준 입력의 변경 : &lt; cat &lt; testFile 과 같이 사용. 표준 출력은 터미널로 하고 표준 입력에 키보드가 아닌 testFile 이라는 파일을 사용한다 일반적으로 cat 파일명 &lt; 파일명 의 형태가 많이 사용된다. 표준 에러 : 2&gt;, 2&gt;&gt; 에러 내용을 파일로 저장하고자 할 때 사용한다. ls /asdfas 2&gt; errorFile 2인 경우는 파일 디스크립터의 순서가 입력 0, 출력 1, 에러 2 이기 때문 표준출력과 표준에러를 한꺼번에 저장하려면 &amp;&gt;를 사용한다. find / -name aa* &amp;&gt; errorLog 명령 &gt; 파일명 : 명령의 결과가 파일에 저장. 에러는 모니터로 출력 명령 2&gt; 파일명 : 에러 내용이 파일에 저장. 명령 결과는 모니터 출력 명령 &amp;&gt; 파일명 : 명령 결과, 에러 내용이 파일에 저장 파이프라인 한 명령의 표준 출력을 다른 명령의 표준 입력으로 보내는 방식 사용되는 기호는 | 이며, |로 구분 되면 한 개 이상의 명령이 연속되어 나올 수 있다 이전 명령의 표준 출력이 다음 명령의 표준 입력이 되는 방식이다. command1 | command2 | command3 | command4 … 1의 출력이 2의 입력, 2의 출력이 3의 입력, 3의 출력이 4의 입력… |&amp;을 사용하면 표준출력 + 표준에러를 다음 명령의 표준 입력으로 사용할 수 있다. $? 는 특수한 변수로써, 앞서 실행한 명령의 종료 상태값을 받는다. ※ 파이프라인에 자주 사용되는 grep 명령어는 패턴에 맞는 문자열을 찾아주는 명령어이다. grep 123 라고 입력하면, 사용자로부터 입력을 받아서 패턴에 맞을 경우 화면에 출력해준다. grep만 입력하면 키보드로 표준 입력을 받는다. 보통은 파이프라인을 사용하여 명령의 결과값을 grep에게 표준입력으로 전달한다. 기타(||, &amp;&amp;, ;, &amp;) ; : 한줄에 여러 명령어 입력. mkfs -t sfs -f /dev/sda5; mkdir /backup; mount /dev/sda5/backup 와 같이 사용 하나의 작업을 기다렸다가 다시 입력하고 하는 방식보다 위와 같이 세미콜론으로 연결해서 사용 가능하다. &amp;&amp;, || : 앞에서 실행한 명령의 결과에 따라 다음 명령어 실행. 각 연산자의 특징을 이용하여 사용된다. ; 으로 연결했을 경우 앞의 명령의 실패여부에 관계없이 뒤의 명령이 무조건 실행된다. 포멧이 실패했는데 마운트가 되는 경우가 발생할 수도 있는 셈이다. 그리하여 &amp;&amp;과 ||를 써서 적절히 성공여부에 따라 명령을 조절할 수 있다. &amp;&amp;의 경우 앞,뒤가 참이여야 하는 논리연산자이다. 그러므로 앞의 연산이 거짓일 경우, 뒤의 연산을 실행하지 않는다. -&gt; 이것이 중요하다. cat asddd &amp;&amp; echo ‘test’ 해보면, asddd 파일이 없을 경우 첫번째 연산결과가 거짓이므로 뒤의 echo는 실행되지 않는다. || 또한 마찬가지다. ||는 둘중 하나만 참이면 되는 논리연산자이다. 그리므로 앞의 연산이 참일 경우, 뒤의 연산을 실행하지 않는다. cat asddd || echo ‘test’ 했을 경우, 앞의 연산이 참이면 뒤의 연산이 실행되지 않는다. 즉, 위 두 연산자를 사용하려면 &amp;&amp;는 명령이 성공할 경우의 명령을 나열하면 되고, ||는 명령이 실패할 경우의 명령을 나열하면 되겠다. &amp; : 백그라운드 모드로 실행 명령어 마지막에 &amp;를 붙여주면 백그라운드 모드로 실행 가능하다. 디렉토리 구조 모든 디렉토리는 / 부터 시작한다. .는 현재 디렉토리 …는 부모 디렉토리 ~는 사용자의 홈 디렉토리를 의미한다. 파일 일반 파일 : 우리가 평상시 쓰는 파일 디렉토리 파일 : 파일을 포함하고 있는 파일. 일반 파일,디렉토리 파일, 특수 파일을 포함할 수 있다. 근데 실제로 들어가보면 파일을 포함하고 있는 것이 아니라, 리눅스가 파일을 엑세스 하는데 필요한 정보를 가지고 있을 뿐이다. 특수 파일 :]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[linux] 사용자/그룹 관리]]></title>
    <url>%2Flinux%2F%EC%82%AC%EC%9A%A9%EC%9E%90-%EA%B7%B8%EB%A3%B9-%EA%B4%80%EB%A6%AC%2F</url>
    <content type="text"><![CDATA[리눅스는 여러명의 사용자가 동시에 로그인하여 시스템을 사용할 수 있는 멀티유저 운영체제이다. 이러한 사용자들은 각각 계정을 가지고 시스템에 접근하게 된다. 그리고 이 사용자들을 묶어서 관리할 수 있는 그룹이라는 것이 있다. 모든 사용자는 하나의 그룹을 가진다. 사용자 관리 /etc/passwd : 사용자 정보가 담긴 파일 123root:x:0:0:root:/root:/bin/bash# 사용자명:패스워드:UID:GID:사용자정보:홈디렉토리:쉘 사용자명 : 로그인 시 사용하는 사용자명. 네이밍 규칙 : 영문자 대소문자, 숫자, “-”, “_”, “.” 사용 가능, 보통 1~32자 사용 패스워드 : /etc/shadow 파일에 암호화되어 저장되어 있다. UID : 사용자 고유 ID 일반 사용자는 1000~60000을 사용한다(/etc/login.defs에 명시) root 는 0으로 예약되어 있다. GID : 사용자가 속한 그룹 고유 ID 사용자정보 : 사용자에 대한 간단한 정보를 기입할 수 있다. 홈디렉토리 : 사용자 폴더. 로그인 시 최초 접속되는 폴더이기도 하다. 쉘 : 사용자가 사용하는 쉘 환경 사용자 추가 12345678910111213&gt; useradd [options] 사용자명 # options# -c [텍스트] : 사용자정보# -m : 홈디렉토리 생성# -M : 홈디렉토리 생성 안함# -d [폴더] : 홈디렉토리 지정# -N : 사용자 개인 그룹 생성하지 않음. default : 생성# -u [UID] : UID 직접 지정# -g [GID] : GID 직접 지정# -s [Shell] : shell 지정# 이 외에도 많은 옵션이 존재한다. useradd --help 를 활용한다. 생성 시 사용하는 default 값을 확인하고 바꿀 수 있다. 1234567891011121314151617181920&gt; useradd -DGROUP=100HOME=/homeINACTIVE=-1EXPIRE=SHELL=/bin/shSKEL=/etc/skelCREATE_MAIL_SPOOL=no&gt; useradd -D -b /test&gt; useradd -DGROUP=100HOME=/test #변경됨INACTIVE=-1EXPIRE=SHELL=/bin/shSKEL=/etc/skelCREATE_MAIL_SPOOL=no# 위에 대한 설정파일은 /etc/default/useradd 이다 사용자 추가 시 홈 디렉토리에 생성되는 파일 123456789101112&gt; useradd user1 -m&gt; cd /home/user1&gt; ls -al합계 32drwxr-xr-x 2 user1 user1 4096 10월 29 16:23 .drwxr-xr-x 4 root root 4096 10월 29 16:23 ..-rw-r--r-- 1 user1 user1 220 9월 1 2015 .bash_logout-rw-r--r-- 1 user1 user1 3771 9월 1 2015 .bashrc-rw-r--r-- 1 user1 user1 655 6월 25 2016 .profile-rw-r--r-- 1 user1 user1 8980 4월 20 2016 examples.desktop# /etc/skel에 있는 파일을 복사해 사용한다. 사용자 수정 12345678910111213141516171819&gt; usermod [options] 사용자명 # options# -c [텍스트] : 사용자정보 수정# -d [폴더] : 홈디렉토리 변경# -u [UID] : UID 변경# -s [Shell] : shell 지정# -L : 계정 락킹# -U : 계정 언락킹# -g [group] : 사용자 기본 그룹 변경 &gt; usermod -g user2 user1 # user1의 기본 그룹을 user2로 변경# -G [groups] : 사용자 그룹 추가,변경(제거). 기본 그룹은 영향을 받지 않는다. &gt; usermod -a -G group1,group2 user1 # user1에 group1,group2를 추가. -a 옵션은 기존그룹에 추가할지 안할지 여부이다. &gt; usermod -G group1 user1 # 그룹을 제거하는 방법. -a를 주지 않아 기존 그룹을 유지하지 않았다. 패스워드 변경 123456&gt; passwd [options] 사용자명# options# -d : 패스워드 삭제# -e : 패스워드 강제 만료# -l, -u : 패스워드 락킹/언락킹. usermod에 있는 -L, -U 옵션을 쓰는것이 더 좋다고 한다.. 사용자 삭제 12345userdel [options] 사용자명# options # -r : 사용자의 홈디렉토리, 메일박스, 임시디렉토리 까지 같이 삭제. 그룹은 기본적으로 속한 사용자가 없으면 자동 삭제된다 그룹 관리 /etc/group : 그룹 정보가 담긴 파일 123sudo:x:27:user1,user2# 그룹명:패스워드:GID:사용자리스트 그룹명 : 그룹의 이름이다 패스워드 : 그룹 패스워드 GID : 그룹 고유 ID 사용자리스트 : 해당 그룹에 속한 사용자들의 리스트. 생략되는 경우도 많기 때문에 비어있다고 그룹에 속한 사용자가 없다고 확신할 수 없다. 그룹 추가 1234&gt; groupadd [options] 그룹명# options# -g [GID] : GID 지정 그룹 수정 1234&gt; groupmod [options] 그룹명# options# -n [이름] : 그룹명 변경 그룹 삭제 1&gt; groupdel 그룹명]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[git] github push 403]]></title>
    <url>%2Fgit%2Fgithub-push-403%2F</url>
    <content type="text"><![CDATA[https://cheonjoosung.github.io/git/2017/07/10/it_git_permissionerror.html 1git remote set-url origin &apos;https://&lt;github-username&gt;@github.com/&lt;github-username&gt;/&lt;github-reppository-name&gt;&apos;]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>The requested URL returned error: 403</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[linux] 관리자 권한 전환]]></title>
    <url>%2Flinux%2F%EA%B4%80%EB%A6%AC%EC%9E%90-%EA%B6%8C%ED%95%9C-%EC%A0%84%ED%99%98%2F</url>
    <content type="text"><![CDATA[Windows에서 최고 관리자가 Administrator였다면, 리눅스에서는 root입니다. 보안상의 이유로 root로 직접 로그인하는 것은 권장되지 않고, 일반 사용자로 로그인 한 뒤, 필요시에 root 권한으로 작업하는 것을 권장합니다. sudo 일반 사용자가 root 권한으로 명령어를 실행하고자 할 떄 사용하는 명령어입니다. 1sudo cat /etc/shadow 당연히 누구나 사용할 순 없고, 허가된 사용자만이 사용 가능한 명령어입니다. 이를 확인하기 위해선 /etc/sudoers 파일을 확인해야 합니다. 현재 우분투를 사용하고 있으므로, 우분투를 기점으로 해당 파일을 열어봤을때 12# Allow members of group sudo to execute any command%sudo ALL=(ALL:ALL) ALL 부분을 발견할 수 있습니다. 보다시피 sudo 그룹에 해당하는 멤버들은 모든 명령어를 사용할 수 있다고 명시되어 있습니다. (기본적으로 관리자 계정으로 등록된 사용자는 sudo 그룹에 소속되어 있음) 그럼 유저를 하나 만들고, sudo 명령어를 사용 가능하도록 만들어보겠습니다. 12345678910111213141516171819202122232425# 먼저 사용자 추가를 위해 root 계정으로 변경합니다. (su 명령어는 아래서 설명)&gt; su -# user1 추가 및 패스워드 설정&gt; useradd -m user1&gt; passwd user1&gt; su - user1&gt; sudo cat /etc/shadow[sudo] password for user1: user1 is not in the sudoers file. This incident will be reported.# sudo 명령어를 실행할 수 없다.# user1에 sudo 그룹을 추가&gt; su - &gt; usermod -G sudo user1# 다시 실행&gt; su - user1&gt; sudo cat /etc/shadow[sudo] password for user1: root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin.....# 출력이 잘 된다! sudo 명령어를 통해 root 권한으로 명령어를 실행하고 나면 다시 일반 사용자 권한으로 돌아오게 됩니다. sudo 명령어 실행 시 입력한 암호는 5분 동안 유지됩니다! (5분간 암호를 재입력하지 않아도 됨) su sudo 처럼 잠시 관리자 권한을 빌려오는 것이 아니라, 아예 관리자 권한으로 전환할 때 사용합니다. 123&gt; su - 암호 :root@:~# - 옵션을 주어야 root 사용자의 환경변수까지 다 읽어오게 됩니다. 입력하지 않을 경우 환경변수가 없어 명령어를 찾지 못하는 현상이 발생할 수도 있습니다. su 뒤에 사용자명을 입력하여 root가 아닌 다른 사용자로 전환할 수도 있습니다. 123&gt; su - user2암호 : user2@:~$]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[db] 쿼리작성 및 최적화]]></title>
    <url>%2Fdb%2F%EC%BF%BC%EB%A6%AC%EC%9E%91%EC%84%B1-%EB%B0%8F-%EC%B5%9C%EC%A0%81%ED%99%94%2F</url>
    <content type="text"><![CDATA[시스템 설정 SQL 모드 MySql 서버에는 sql_mode 라는 시스템 설정이 있다. 이 설정을 통해 SQL의 작성, 결과 등에 영향을 주게 된다. STRICT_ALL_TABLES : 저장하려는 값의 길이가 컬럼의 길이보다 길 경우 오류 발생시킴. 원래는 초과된 부분만큼 자르고 저장하였음. STRICT_TRANS_TABLE : 컬럼의 타입과 값의 타입이 일치하지 않을 경우 오류 발생시킴. 원래는 비슷한 타입으로 최대한 변환시켜서 저장했음. ANSI_QUOTES : 문자열 표현에 오직 싱글쿼터만 사용가능. 더블쿼터는 컬럼명이나 테이블명만 사용 가능. 원래는 문자열에 싱글/더블 쿼터 모두 사용 가능했음. ONLY_FULL_GROUP_BY : GROUP BY 절에 포함되지 않은 컬럼을 출력하거나 HAVING에 사용할 수 없음. 원래는 가능했음… MySQL의 이상했던 점. PIPE_AS_CONCAT : 오라클 처럼 || 문자로 문자열 연결 가능. 원래는 불가능. 등등… 적용하는법은 my.cnf 파일에 1sql_mode=STRICT_TRANS_TABLES,ANSI 의 형태로 작성해주면 된다. ,로 구분하여 여러개 지정할 수 있다. sql_mode에서 알 수 있는 MySQL의 특징은? 영문 대소문자 MySql은 설치된 운영체제에 따라 테이블의 대소문자를 구분한다. ex) 윈도우 : 구분X, 리눅스 : 구분O DB나 테이블이 디스크의 디렉터리나 파일로 매핑되기 때문이다. 가능하면 대문자만 또는 소문자만으로 통일하는 것이 좋다. 예약어 일반적으로 테이블이나 컬럼 생성 시 예약어를 이름으로 사용하면 에러가 발생한다. 그러나 역따옴표(`)나 쌍따옴표(&quot;)로 감싸면 이를 피할 수 있다. 예를 들면 테이블 생성 시 역따옴표(`)로 둘러싸면 에러를 발생시키지 않는다. 하지만 역따옴표로 둘러싸서 생성하는 것을 권장하지는 않는다. 메뉴얼 읽는 법 대문자는 키워드를 의미한다. 이텔릭체는 사용자가 작성하는 이름 또는 표현식을 의미한다. 대괄호( [] )는 선택사항임을 의미한다. 파이프( | )는 연결된 것 중 단 하나만 선택할 수 있음을 의미한다. 중괄호( {} )는 괄호 내 선택사항 중 반드시 하나를 선택해야 함을 의미한다. …는 앞에 명시된 키워드나 표현이 반복될 수 있음을 의미한다. MySQL 연산자, 내장함수 리터럴 문자열 항상 홑따옴표(’)를 사용해서 표시한다. 홑따옴표 자체를 사용하고 싶을 때는 홑따옴표를 두번 연속 입력하면 된다. 쌍따옴표와 홑따옴표의 조합으로 위의 행위를 할 수 있지만 MySql만 지원되는 방식이다. 숫자 다른 시스템들과 동일하게 ’ 나 &quot; 없이 숫자 값을 입력하면 된다. ※ 자동 형 변환 MySql에서는 문자와 숫자를 비교할 때 자동으로 숫자값으로 형 변환해준다. (숫자 &gt; 문자) 1WHERE number_col = &apos;10001&apos;; 와 같은 경우는 문제가 되지 않지만, 1WHERE string_col = 10001; 의 경우에는 문제가 발생할 수 있다. 위와 같은 경우 오른쪽 숫자가 우선순위가 더 높으므로 왼쪽의 string_col이 숫자로 형 변환하는 상황이 발생하는데, 이로 인해 string_col은 인덱스를 사용하지 못하고, 만약 문자값이 포함된 데이터가 있다면 SQL 실행 오류로도 이어진다. 날짜 MySql에서 정해진 형태의 포멧으로 날짜를 입력하면(문자열로), MySql에서 자동으로 DATE나 DATETIME으로 변환하여 준다. 위의 숫자 리터럴과 동일하다. (날짜 &gt; 문자) 불리언 BOOLEAN이란 타입이 있긴 하나 실상 TINYINT에 대한 동의어일 뿐이다. (테이블 컬럼 타입을 BOOLEAN으로 생성한 뒤 확인해보면 TINYINT 이다.) BOOLEAN의 TRUE는 1을 의미하고, FALSE는 0을 의미한다. 0과 1만을 사용한다는 점을 주의해야 한다!! 연산자 동등 비교 =, &lt;=&gt; 기본적으로 NULL과의 연산은 결과값이 모두 NULL이다. 1SELECT NULL=1 FROM DUAL; &gt; NULL 그래서 NULL의 경우 IS NULL등의 연산자로 비교해야 하는데, &lt;=&gt; 의 경우에는 NULL 또한 하나의 값으로 인식하여 계산한다. 1SELECT NULL&lt;=&gt;NULL FROM DUAL; &gt; 1 부정 비교 &lt;&gt;, != 둘중 어느것을 사용하든 상관없지만 통일하는 것이 좋다. NOT 연산자 결과를 반대로 만든다. 부정의 결과를 정확히 예측할 수 없는 경우 사용하지 말자. ex) !‘test’ AND, OR 연산자 &amp;&amp;, ||로 대체할 수 있으나 혼란을 야기하니 사용하지 말자. 나누기, 나머지 연산자 123456SELECT 29/9 FROM DUAL;-- 3.222.. // 몫 + 나머지SELECT 29 DIV 9 FROM DUAL;-- 3 // 몫SELECT 29 MOD 9 FROM DUAL;-- 2 // 나머지 LIKE 연산자 상수 문자열이 있는지 없는지 정도만을 판단한다. 와일드 카드는 딱 2개만 사용가능하다. % : 0개 또는 1개 이상 일치 _ : 정확히 1개 일치 와일드카드 문자가 검색어의 뒤쪽에 있다면 인덱스 레인지 스캔을 사용할 수 있다! 와일드카드가 앞쪽에 올 경우 인덱스 풀 스캔 또는 테이블 풀 스캔을 하게 된다.(B-Tree 인덱스의 특징 때문) 와일드카드 문자를 검색에 사용하고 싶을 경우 ESCAPE 구문을 사용한다. 1SELECT 'a%' LIKE 'a/%' ESCAPE '/' FROM DUAL; BETWEEN 연산자 &gt;= 연산자와 &lt;= 연산자를 합친 기능을 수행한다. BETWEEN을 위의 연산자로 풀어도 성능상 차이가 없으니 크게 고려하지 않아도 된다. IN 연산자 동등비교를 여러번 하는 연산자이다. 즉 일반적으로 빠르게 처리된다. 연산자의 입력이 상수가 아니라 서브쿼리일 경우 상당히 느려질 수 있으니 주의해야 한다. NOT IN의 경우 부정형 비교여서 인덱스 풀 스캔을 사용한다. ※ BETWEEN 연산자와 IN 연산자 차이점 BETWEEN의 경우 범위만큼 인덱스를 다 읽어야 하는 반면, IN의 경우 위에서 언급했듯이 동등 연산자를 여러번 수행하는 것과 같다. 실행계획을 비교해보면 인덱스를 타고 있지만 rows에서 차이가 나는것을 확인할 수 있다. 내장함수 NULL 값 비교 IFNULL : 값이 NULL인지 비교하고 NULL이면 다른 값으로 대체한다. 첫번째 인자는 NULL인지 비교할 값, 두번째 인자는 NULL일시 대체값이다. 첫번째 인자가 NULL이 아닐경우 그대로 출력된다. ISNULL : 값이 NULL인지 아닌지 비교. NULL일 경우 TRUE(1), 아닐경우 FALSE(0) 현재 시각 조회 NOW : 시간 조회. 하나의 SQL내면 모두 같은 값을 가진다. SYSDATE : 시간 조회. 하나의 SQL내에서도 호출 시점에 따라 결과값이 달라진다. 1234SELECT NOW(), SLEEP(1), NOW() FROM DUAL;-- '2017-12-19 22:39:52' | '0' | '2017-12-19 22:39:52'SELECT SYSDATE(), SLEEP(1), SYSDATE() FROM DUAL;-- '2017-12-19 22:40:27' | '0' | '2017-12-19 22:40:28' 보다시피 SYSDATE의 경우 SLEEP(1)로 인해 1초가 차이난다. SYSDATE의 경우 호출때마다 다른 값을 반환하므로 상수가 아니다. 그래서 인덱스를 효율적으로 사용하지 못한다. (인덱스라기 보단 캐싱을 못한다는게 더 맞지 않을까?) 이러한 문제점이 있으니 SYSDATE는 가급적 사용하지 않는것이 좋다. CURRENT_TIMESTAMP, CURRNET_DATE, CURRENT_TIME은 NOW와 동일한데다 출력을 다양하게 활용할 수 있으니 이것을 사용해도 좋을 것 같다. 날짜 &lt;&gt; 문자 지정 문자 내용 %Y 4자리 연도 %y 2자리 연도 %m 월 %d 일자 %H 24h 시간(00 ~ 23) %h 12h 시간(01 ~ 12) %i 분 %s 초 DATE_FORMAT : 날짜를 문자열로 변환한다. ex) DATE_FORMAT(NOW(), %Y-%m-%d) &gt; 2017-12-19 STR_TO_DATE : 문자를 날짜로 변환한다. 여기서 주는 포멧은 기존 문자열의 포멧을 알려주는 것이다. ex) STR_TO_DATE(‘19920207’,’%Y%m^%d’) &gt; 1992-02-07 참고로 SQL 표준형태로 입력된 문자열은 STR_TO_DATE를 사용하지 않아도 자동으로 DATETIME으로 변환되어 처리된다. 날짜 연산 DATE_ADD : 첫번째 인자로 날짜, 두번째 인자로 더할 날짜를 주면 된다. 두번째 인자는 INTERVAL n {KEYWORD} 의 형태로 줘야 한다. DATE_SUB 함수도 있으나 두번째 인자로 음수값을 줄 수 있기 때문에 DATE_ADD만으로도 충분하다. ex) DATE_ADD(NOW(), INTERVAL 3 DAY) ※ KEYWORD : YEAR, MONTH, WEEK, DAY, HOUR … 문자열 처리 LPAD, RPAD : 좌측 혹은 우측에 특정한 문자를 지정한 바이트 만큼 채우는 함수이다. 첫번째 인자로 대상 문자열, 두번째 인자로 채울 바이트(기존 문자열 포함), 세번째 인자로 채울 문자를 넣는다. LTRIM, RTRIM, TRIM : 연속된 공백문자를 제거하는 함수이다. LTRIM은 왼쪽, RTRIM은 오른쪽, TRIM은 양쪽의 공백을 제거한다. 문자열 결합 CONCAT : 문자열을 결합해서 하나의 문자열로 합치는 함수로 인자 개수는 제한이 없다. ex) CONCAT('My name is ', 'JoonT. ', 'Nice to ', ‘meet you.’); &gt; My name is JoonT. Nice to meet you. CONCAT_WS : CONCAT과 같은데 첫번째 인자로 구분할 문자열을 넣어줄 수 있다. ex) CONCAT_WS(’-’, ‘a’, ‘b’, ‘c’); &gt; a-b-c GROUP_CONCAT : GROUP BY 로 그룹핑된 집합을 문자열로 연결시킬 수 있다. 단순 문자열 연결 외에도 구분자 추가, 정렬, 중복 제거 등을 할 수 있으므로 상당히 유용하다. 12345678SELECT GROUP_CONCAT(emp_no) FROM dept_emp GROUP BY dept_no;-- 1,2,3,4,5... : 기본적으로 ,로 연결됨SELECT GROUP_CONCAT(emp_no SEPARATOR '-') FROM dept_emp GROUP BY dept_no;-- 1-2-3-4-5... : 구분자를 줄 수 있음SELECT GROUP_CONCAT(emp_no ORDER BY from_date DESC) FROM dept_emp GROUP BY dept_no;-- dept_emp에 있는 컬럼으로 정렬 가능SELECT GROUP_CONCAT(DISTINCT emp_no) FROM dept_emp GROUP BY dept_no;-- 중복제거도 가능 값의 비교와 대체(CASE WHEN…THEN…END) CASE WHEN…THEN…END switch문과 비슷하다. CASE str WHEN ‘A’ THEN ‘A matched’ WHEN ‘B’ THEN ‘B matched’ ELSE ‘Nothing matched’ END 각각의 부분에 대해 동등(=) 연산을 한다. 아래처럼 따로 조건을 줄 수도 있다. CASE WHEN str &gt;= 65 AND str &lt;= 90 THEN ‘Upper Case’ WHEN str &gt;= 97 AND str &lt;= 122 THEN ‘Lower Case’ END THEN 부분에 쿼리를 실행해서 조건별 쿼리 실행도 가능하다. CASE sex WHEN ‘M’ THEN (SELECT salary from salaries WHERE ~~ LIMIT 1) WHEN ‘F’ THEN 0 END 남자일때는 쿼리를 실행하고, 여자일떄는 쿼리를 실행하지 않는다. 이로 인해 불필요한 쿼리가 실행되는 것을 막을 수 있다. 타입 변환 CAST : 명시적 형 변환을 하는 함수이다.변환할 값과 변환할 타입을 지정해주면 된다. 둘은 AS 로 연결한다. ex) CAST(‘1234’ AS INTEGER) CONVERT : CAST와 동일하게 형 변환을 하는 함수이다. 형 변환외에 문자집합 변환도 가능하다는 점이 차이점이다. ex) CONVERT(1234, UNSIGNED) ex) CONVERT(‘asd’ USING ‘utf8’) 이진값 &lt;&gt; 16진수 HEX : 이진값 -&gt; 16진수 UNHEX : 16진수 -&gt; 이진값 MD5, SHA 비대칭형 암호화 알고리즘에 사용된다. SHA는 SHA1 알고리즘을 써서 160비트(20바이트) 해시값을 반환하고, MD5는 메세지 다이제스트 알고리즘을 사용해서 128비트(16바이트) 해시값을 반환한다. 비밀번호 암호화 하는 과정에서 자주 사용되며, URL과 같은 긴 문자열에 인덱스를 주고싶을 때 해당 문자열을 해시값으로 변환해 인덱스를 주기도 한다. 해시값은 거의 진짜 왠만해선 중복되는 일이 없기 때문이다. 변환된 해시값은 16진수로 출력되기 때문에 값을 저장하려면 변환되는 해시값의 크기보다 2배 큰 값을 할당해줘야 하는데(1바이트 == 16진수 2개), 원래의 16바이트, 20바이트에 저장하고 싶다면 BINARY 타입의 컬럼에 저장하면 된다. BINARY로 변환은 위에서 사용한 HEX, UNHEX 함수를 사용하면 된다. 디버깅용 함수 SLEEP(초) : 실행되면 그 구간만큼 멈춘다. 디버깅 용도로 사용한다. BENCHMARK(쿼리문, 실행횟수) : 쿼리문이 실행되는데 몇초 걸리는가 테스트 해볼 수 있다. 실행횟수로 여러번 반복해볼 수도 있는데… 솔직히 별로 안쓸듯. 왜냐면 쿼리문은 하나의 컬럼, 레코드인 스칼라값을 반환해야 하고, 실행횟수도 실제로 10번 실행하는 것과 BENCHMARK로 10번 실행하는것이 다르기 떄문. 실제 쿼리를 10번 실행하는 서비스 환경에서는 10번의 네트워크 자원, 서버 자원등이 다 요청되지만 BENCHMARK의 경우 단 1번만 요청되기 떄문. IP 주소 변환 IP주소를 UNSIGNED INTEGER의 형태로 상호변환 가능하다. INET_ATON : IP 주소 -&gt; UNSIGNED INTEGER INET_NTOA : UNSIGNED INTEGER -&gt; IP 주소 ex) 1234SELECT name, INET_NTOA(ip_addr) FROM TEST WHERE ip_addr BETWEEN INET_ATON('192.168.0.1') AND INET_ATON('192.168.0.100'); 훨씬 효과적으로 저장 및 조회가 가능하다. MySQL 전용 암호화 MySQL내의 user를 생성할때 썼던 PASSWORD 함수를 말한다. 많은 사람들이 이 함수의 이름만 보고 실제 회원의 비밀번호를 이것으로 암호화하곤 했다. 하지만 이는 절대 암호화 함수가 아니다. MySQL 내부에서 그냥 사용하는 것이고, 내부 암호화 알고리즘도 명시된것이 없다. 바뀔수도 있다는 것이다. 실제로도 바뀌었다. MySQL 4.1 이전과 이후로 PASSWORD 함수를 입력하면 변경되는 값이 다름을 볼 수 있다. 이로인해 DB 버전 업데이트를 했을 경우 사용자가 로그인하지 못하는 심각한 일이 발생할 수 있다. 이럴 경우 기존의 PASSWORD함수를 OLD_PASSWORD로 바꿔서 임시로 막을 수 있다. 변경된 PASSWORD 함수에서는 암호화시에 *를 앞에 붙인다. 이로인해 어플리케이션에서 구분할수도 있다. 하지만… 기본적으로 이 함수를 절대 패스워드 암호화 하는데 쓰지 않도록 해야한다. VALUES 1234567INSERT INTO table_statistics(member_id, visit_count) SELECT member_id, COUNT(*) FROM tab_accesslog GROUP BY member_idON DUPLICATE KEY -- UPDATE visit_count = visit_count + COUNT(*) // 불가능UPDATE visit_count = visit_count + VALUES(visit_count) -- 인서트하려 했던 값 참조 가능 UPDATE 문은 서브쿼리의 일부가 아니기 때문에 COUNT(*)로 접근이 불가능하다. 하지만 위와 같이 VALUES 함수를 사용하면 위에 INSERT에서 추가하려고 했던 컬럼에 접근 가능하다. COUNT 출력된 레코드의 개수를 세거나, GROUP BY 함수와 같이 사용해 그룹화 된 요소들의 개수를 출력하는데 사용한다. COUNT() 라고 행 전체를 다 읽어오거나… 그런 의미가 아니라, 그냥 레코드 1개 자체를 의미한다. 즉 COUNT(), COUNT(‘A’), COUNT(1)은 아무 차이가 없다. 하지만 COUNT(컬럼명) 으로 했을 경우 NULL인 값은 제외되므로 이부분은 조심해서 사용해야 한다. 주석 1234567891011121314--- 한줄 주석(SQL 표준)/*여러줄 주석(SQL 표준)여러줄여러줄여러분*/ /*! 변형된 C언어 스타일의 주석인데, 이는 SQL에서 힌트로 사용할 수 있다. *//*! STRAIGHT_JOIN */# 한줄주석(비표준) SELECT 18, 204, 206, 211, 213, 215, 217, 232, 238]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>Real MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] 자료형]]></title>
    <url>%2Fjava%2F%EC%9E%90%EB%A3%8C%ED%98%95%2F</url>
    <content type="text"><![CDATA[기본형(Primitive Type) 자료형 표현 데이터 크기 범위 boolean 참/거짓 1byte true, false char 문자 2byte 모든 유니코드 문자 byte 정수 1byte -128 ~ 127 short 정수 2byte -32768 ~ 32767 int 정수 4byte -2147483648 ~ 2147483647 long 정수 8byte -9223372036854775808 ~ 9223372036854775807 float 실수 4byte 1.4E-45 ~ 3.4028235E38 double 실수 8byte 4.9E-324 ~ 1.7976931348623157E308 범위에 대한 기본 개념 1byte를 예로 들어보자 1byte는 8bit 이므로, 0000 0000 ~ 1111 1111 까지 표현이 가능하다 이 값은 0 ~ 255 까지의 값인데, java는 unsigned 형이 없으므로 범위에 음수를 포함하여야 한다 하지만 보다시피 현재 상태로는 음수 값을 표현할 수 없다 그래서 가장 왼쪽에 있는 비트를 부호로 사용하며, 이를 MSB 라고 한다 MSB가 0이면 양수, 1이면 음수인 것이다 그러므로 1byte 자료형이 표현할 수 있는 범위는 1111 1111 ~ 0111 1111 까지가 되는 것이다 음수값을 표현할때는 2의 보수(1의 보수를 구한후 + 1)를 사용한다 12ex1) 0111 1111(127) : 1000 0001(-127) ex2) 0000 1111(15) : 1111 0001(-15) 여기까지만 보면 1byte의 범위는 -127 ~ 127 까지가 되어야할 것 같은데, 왜 -128까지 일까? 아래는 추측이다 하지만 첫번째 비트를 MSB로 사용했기 때문에, 우리는 0을 2가지 방법으로 표현할 수 있는 특징을 가지게 된다(0000 0000, 1000 0000) 여기서 1000 0000 값을 버리지 않고 1000 0001(-127) 에서 1 빠진 값으로 -128을 사용하는 것으로 보인다 리터럴 정수값은 기본이 int 리터럴이다. 실수값은 기본이 double 리터럴이다. 12int i = 10; // int형double d = 10.10; // double형 long, float을 사용하고 싶으면 추가적인 문자를 사용해야 한다. 12long l = 3000000000L; // 30억은 int 범위를 넘어가므로 long 리터럴을 사용하지 않으면 표현할 수 없다.float f = 10.10F; short, byte는 리터럴이 없다. 그냥 정수값을 넣으면 자동으로 변환된다. 12byte b = 10; // byte형이 됨short s = 10; // short형이 됨 형변환 명시적 형 변환 캐스팅 연산자를 써서 명시적으로 형을 변환하는 방법이다. 12double d = (double)10; // int를 double로 형 변환float f = (float)10.10; // double을 float으로 형 변환 크기에 따른 자동 형 변환 byte &lt; short &lt; int &lt; long &lt; float &lt; double 왼쪽으로 오른쪽 순서로 타입의 크기이다. 큰 자료형에 작은 자료형을 저장할 경우 자동으로 형 변환이 일어난다. 12long l = 10; // long형이 됨float f = 10L; // float이 더 크므로 long을 저장 가능하다 산술연산에 의한 자동 형 변환 연산을 하는 숫자들 중 가장 큰 자료형으로 나머지 숫자의 자료형이 결정된다. 12345int i1 = 10L + 10; // long형으로 변환되므로 컴파일 에러int i2 = 3 + 3.5; // double형으로 변환되므로 컴파일 에러double d1 = 3 / 2; // 둘다 int 형이므로 소수점 이하가 잘린 1이 저장된다.double d2 = (double)3 / 2; // 3을 double로 강제 형 변환 하였으므로 연산의 결과는 double이 된다. byte형이나 short형은 연산 시 모두 int형으로 변환된다. 1234short s1 = 1; // 여기선 자동 변환된다short s2 = 2; // 여기선 자동 변환된다short s3 = s1 + s2; // int로 변환되었기 때문에 컴파일 에러short s4 = (short)s1 + s2; // short로 변환해야 함 char형은 문자형이지만 연산할 수 있다. 123char c = 'a';int i = 100;System.out.println(c + i); // 문자 a는 아스키코드값(97)로 변환되어 197이 반환된다. 참조 : https://hit-it-sum.tistory.com/1]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java 형 변환</tag>
        <tag>primitive type</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[java] 입출력]]></title>
    <url>%2Fjava%2F%EC%9E%85%EC%B6%9C%EB%A0%A5%2F</url>
    <content type="text"><![CDATA[I/O는 Input/Output의 약자로, 입력/출력을 말한다. 입출력은 컴퓨터 내/외부 장치와 프로그램간의 데이터를 주고받는 것을 말한다. 스트림 데이터를 주고받으려면 두 대상을 연결하고 데이터를 전송할 수 있는 연결통로 같은것이 필요한데, 이를 스트림이라고 한다. 스트림은 단방향으로만 가능하므로 하나의 스트림으로 입,출력을 동시에 진행할 수 없다. 그러므로 입력 스트림, 출력 스트림이 따로 존재하고, 입력과 출력을 동시에 수행하려면 2개의 스트림이 필요하다. 스트림은 FIFO 구조로 되어있다. 자바에서 제공하는 스트림은 크게 2가지가 있다. 바이트 단위로 데이터를 전송하는 바이트기반 스트림과 해당 스트림의 기능을 보완하기 위한 필터 스트림 문자 단위로 데이터를 전송하는 문자기반 스트림과 해당 스트림의 기능을 보완하기 위한 필터 스트림 바이트 기반 스트림 바이트 단위로 데이터를 전송하는 스트림이다. 최상위 클래스는 InputStream과 OutputStream이고, 입출력 대상에 따라 이를 상속한 FileInputStream, ByteArrayInputStream, PipedInputStream 등이 있다. InputStream, OutputStream InputStream 입력 소스마다 입력 방식이 다르므로, abstract 메서드로 read()를 제공하고 자식쪽에서 이를 구현하도록 했다. 구현 클래스에서는 입력 소스로부터 1byte를 읽어오는 내용을 구현하면 된다 이 같은 추상화로 인해 입출력의 방식이 달라져도 일관된 방법으로 입출력이 가능하다. read()는 입력 소스로부터 가져온 바이트를 반환하는데, 바이트를 받음에도 불구하고 byte가 아닌 int를 사용하고 있음을 볼 수 있다. 1byte의 데이터를 받으러면 0~255 까지의 데이터를 담아야하는데, java는 unsigned 형이 없어서 java의 byte로는 양수로 127까지 밖에 담지 못한다 그리고 만약 java에 unsigned 형이 있어서 255까지 받을 수 있다고 하더라도, EOF 값(-1)을 받지 못하기 때문에(unsigned 로는 음수를 표현할 수 없으니까) 어찌됐든 byte 형태는 사용할 수 없다 그렇다면 short를 사용하면 되지, 왜 int를 사용하는가? 이는 대부분의 연산의 기본 정수형이 int 타입이기 때문이다(JVM 기본 정수형도 int 타입이다) 그러므로 사실상 int를 사용하는 것이 가장 연산이 빠르고(추가적으로 형변환을 하지 않아도 되기 때문에?), 대부분의 정수 관련 연산은 int 타입을 사용하므로 int를 택한것으로 보인다 참고 : https://stackoverflow.com/questions/21062744/why-does-inputstream-read-return-an-int-and-not-a-short read(byte[] b), read(byte[] b, int off, int len) 1 byte씩 데이터를 가져오면 너무 느리므로, byte 배열을 전달하고 이 배열에 데이터를 담아오도록 한다 하나씩 전달하는 것 보다 바구니에 담아서 전달하는 것이 더 빠른것을 생각하면 된다. 반환되는 int 값은 읽은 바이트의 개수이다. read(byte[] b): 배열 b의 크기만큼 데이터를 읽어와서 b에 저장한다. read(byte[] b, int off, int len) : len의 크기만큼 데이터를 읽어와서 배열 b의 off 위치부터 저장한다. OutputStream abstract 메서드로 write(int b) 를 제공한다. 인자 b는 출력소스로 보낼 데이터이다 (flush의 경우 버퍼가 있는 출력 스트림의 경우에만 의미가 있다.) write(byte[] b), write(byte[] b, int off, int len) write(byte[] b) : 배열 b에 저장된 모든 내용을 출력소스에 쓴다 write(byte[] b, int off, int len) : 배열 b에 저장된 내용을 off 위치부터 len개 만큼 출력소스에 쓴다. FileInputStream, FileOutputStream 파일에 입출력을 하기 위한 스트림으로, 실제 프로그래밍에서 많이 사용된다. FileInputStream FileOutputStream 아래는 file 복사의 간단한 예제이다. 12345678910111213141516171819@Testpublic void fileCopy()&#123; try( FileInputStream fileInputStream = new FileInputStream("/Users/home/test.mov"); FileOutputStream fileOutputStream = new FileOutputStream("/Users/home/test_copy.mov") )&#123; byte[] temp = new byte[8192]; long start = System.currentTimeMillis(); while(fileInputStream.read(temp) &gt; 0)&#123; fileOutputStream.write(temp); &#125; long end = System.currentTimeMillis(); System.out.println((end - start) / 1000.0); &#125; catch(IOException e)&#123; e.printStackTrace(); &#125;&#125; ByteArrayInputStream, ByteArrayOutputStream 바이트 배열에 입출력 하기 위한 스트림이다. 입출력 대상이 메모리(배열은 java heap에 저장되므로) 이므로 close를 하지 않아줘도 된다는 특징이 있다. 자주 사용되진 않지만 read(byte[] b) 사용 시 발생할 수 있는 실수를 알아보기 위해 첨부하였음. 12345678910111213141516171819202122@Testpublic void readFromByte() &#123; byte[] src = &#123;1,2,3,4,5,6,7,8,9,10&#125;; byte[] dst; byte[] temp = new byte[4]; ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(src); ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); try&#123; while(byteArrayInputStream.read(temp) &gt; 0)&#123; byteArrayOutputStream.write(temp); &#125; &#125; catch (IOException e)&#123; e.printStackTrace(); &#125; dst = byteArrayOutputStream.toByteArray(); System.out.println(Arrays.toString(dst));&#125; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 8] 이 출력된다. 미자막 루프에서 9, 10 2byte만 읽었으므로 temp의 앞 2byte만 교체되는데, write(temp)에서 temp의 모든 내용을 출력하도록 했기 때문이다. 12345678int len;try&#123; while((len = byteArrayInputStream.read(temp)) &gt; 0)&#123; byteArrayOutputStream.write(temp, 0, len); &#125;&#125; catch (IOException e)&#123; e.printStackTrace();&#125; 이처럼 변경해줘야 한다. 바이트 기반 보조 스트림 바이트 기반 스트림의 기능을 보완(성능향상 및 기능추가)하기 위함. 자체적인 입출력 기능은 없다. 입출력 기능은 기반 스트림에 위임한다. 그러므로 생성자에서 항상 기반 스트림을 받는다. 부모 클래스는 FilterInputStream, FilterOutputStream이고, 자식으로는 BufferedInputStream/BufferedOutputStream, DataInputStream/DataOutputStream, SequenceInputStream, PrintStream 등이 있다. 보조 스트림을 close 하면 기반 스트림도 같이 close 된다. FilterInputStream, FilterOutputStream FilterInputStream FilterOutputStream 위 두 클래스는 생성자가 protected이므로 직접 생성이 불가능하고, 상속을 통해 오버라이딩 되어야 한다. 데코레이터 패턴을 기반으로 디자인 된 클래스 구조이기 때문에, 여러 Filter 클래스들을 겹쳐서 사용할 수 있다! 1DataInputStream dis = new DataInputStream(new BufferedInputStream(System.in)); BufferedInputStream, BufferedOutputStream BufferedInputStream BufferedOutputStream 생성할 때 지정한 버퍼의 크기만큼 입력소스로부터 읽고, 내부 버퍼에 저장해놓는다 예를 들어 2048의 크기로 BufferedInputStream 을 생성하고 read() 메서드를 호출하게 되면, 입력소스로부터 2048 byte 만큼 읽어와서 내부 배열에 저장하고, read() 메서드의 결과로는 2048 byte 중 1 byte만을 돌려주게 된다 read() 메서드로 내부 버퍼의 내용을 다 읽게 될 때 까지 입력소스에 추가적으로 접근하지 않게되고, 내부 버퍼의 내용을 다 읽으면 다시 입력소스에 접근해서 생성시 지정한 버퍼의 크기만큼 데이터를 읽어온다 BufferedInputStream 을 2048의 크기로 생성하고, read(byte[] b)에 100 크기의 배열을 전달하게 되면, 처음 read(byte[] b)를 호출하는 순간 입력소스로부터 2048 바이트를 읽어온 뒤 BufferedInputStream 에 있는 내부 배열에 저장해놓고, 거기서 100 byte만을 읽어서 반환해주게 된다 즉, read가 200번 호출될 떄 까지는 원본 소스에 직접 접근하는 일은 없을 것이다 BufferedOutputStream 또한 출력버퍼로 바로 전송하지 않고, 내부 버퍼에 데이터를 쌓아두었다가 버퍼의 내용이 가득차면 출력소스로 보낸다. 버퍼가 가득 차지 못해서 출력되지 못할 수도 있으니 항상 마지막에 flush()나 close()를 호출해서 버퍼를 비우도록 해야한다. 표준 입출력 표준 입출력이란 사용자가 별다른 입출 자바에서는 System 클래스의 in, out, err 를 통해 표준 입출력에 접근 가능하다 자바 어플리케이션의 실행과 동시에 자동적으로 생성되며, 내부적으로 BufferedInputStream, BufferedOutputStream 을 사용한다 아래는 표준 입출력을 이용해 문자를 입력받고 출력하는 소스이다 12345int input = 0;while((input = System.in.read()) != -1) &#123; System.out.println((char)input);&#125; 표준 입출력의 끝을 나타내고 싶을 때는 Enter 키를 입력하거나, ^Z(맥에서는 ^D) 를 입력하면 된다 근데 여기서 조금 문제가 되는게, Enter 키를 입력할 경우 두 개의 특수문자 ₩r, ₩n 이 입력된것으로 간주된다는 점이다 ₩r은 커서를 라인의 첫번째로 이동, ₩n은 커서를 다음 줄로 이동을 의미한다 이 때문에 Scanner의 nextInt() 같은 명령을 수행하게 되면 입력 버퍼에 ₩r₩n이 그대로 남아있게 되어, 이 후 문자열 입력 명령이 수행되어 버리는 문제점이 있다 그러므로 이를 방지하기 위해서 Scanner의 nextLine() 명령을 사용해주는 것이 좋다 nextLine()으로 받은 문자열에 Integer.parseInt 같은 명령을 사용하면 개행문자는 삭제되기 때문이다 문자 기반 스트림 java는 UTF16 인코딩을 사용하기 때문에 문자형을 2byte로 처리한다. 그러므로 바이트 기반 스트림으로 문자를 처리하기에는 어려움이 많다. 그래서 탄생한 것이 Reader, Writer이다. 이 또한 가장 최상위 클래스이다. 자식은 바이트 기반 스트림 자식에서 이름만 InputStream -&gt; Reader, OutputStream -&gt; Writer로 바꿔주면 된다.(FileReader/FileWriter, PipedReader/PipedWriter 등) Reader, Writer Reader Writer byte배열 대신 char배열을 사용한다는 것과, 추상메서드가 read(char[] cb, inf off, int len) 으로 달라졌다는게 차이점이다. (프로그래밍 관점에서 이 추상메서드를 사용하는게 좀 더 바람직하기 때문이다) FileReader, FileWriter File로 부터 문자열을 입출력할 때 사용한다. FileReader FileWriter 12345678910111213141516171819202122@Testpublic void InputStream와Reader의문자처리() &#123; try( FileInputStream fileInputStream = new FileInputStream("/Users/home/Desktop/대충 정리.md"); FileReader fileReader = new FileReader("/Users/home/Desktop/대충 정리.md") )&#123; int a; while((a = fileInputStream.read()) != -1)&#123; System.out.print((char)a); &#125; System.out.println(); int b; while((b = fileReader.read()) != -1)&#123; System.out.print((char)b); &#125; &#125; catch(Exception e)&#123; e.printStackTrace(); &#125;&#125; InputStream으로 읽었을 경우, 1byte만 읽어오므로 latin 인코딩이 아니면 문자가 깨진다. 문자 기반 보조 스트림 바이트 기반 보조 스트림과 용도는 동일하다. FilterInputStream/FilterOutputStream 처럼 기반이 되는 클래스는 따로 없다. BufferedReader, BufferedWriter BufferedReader BufferedWriter 버퍼를 이용해 입출력의 성능을 높여준다. BufferedReader는 라인 단위로 읽어올 수 있는 readLine() 메서드를 제공하고, BufferedWriter는 개행을 출력할 수 있는 newLine() 메서드를 제공한다. InputStreamReader, OutputStreamWriter InputStreamReader OutputStreamWriter 이름에서 알 수 있듯이 바이트 기반 스트림을 문자 기반 스트림으로 변환해주는 역할을 한다. 인코딩을 직접 지정할 수도 있다. 인코딩을 지정하지 않으면 OS에서 기본으로 사용하는 인코딩을 사용할 것이다. 콘솔 입력을 받을 때 주로 이용한다.(이 외에도 많겠지?) 12345InputStreamReader in = new InputStreamReader(System.in);BufferedReader br = new BufferedReader(in);// do something...br.readLine(); 참고 : 남궁성, 『Java의 정석 2nd Edition』, 도우출판(2010) 표준 입출력 : https://shoark7.github.io/programming/knowledge/what-is-standard-stream]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>OutputStream</tag>
        <tag>Reader</tag>
        <tag>Writer</tag>
        <tag>java I/O</tag>
        <tag>표준 입출력</tag>
        <tag>InputSream</tag>
        <tag>FiliterInputStream</tag>
        <tag>FilterOutputStream</tag>
      </tags>
  </entry>
</search>
